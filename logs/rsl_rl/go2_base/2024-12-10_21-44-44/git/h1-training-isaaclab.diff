--- git status ---
On branch release
Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   README.md
	deleted:    isaaclab/extension/omni.isaac.viplanner/config/extension.toml
	deleted:    isaaclab/extension/omni.isaac.viplanner/data/mpcat40_to_vip_sem.yml
	deleted:    isaaclab/extension/omni.isaac.viplanner/data/town01/area_filter_cfg.yaml
	deleted:    isaaclab/extension/omni.isaac.viplanner/data/town01/cw_multiply_cfg.yml
	deleted:    isaaclab/extension/omni.isaac.viplanner/data/town01/keyword_mapping.yml
	deleted:    isaaclab/extension/omni.isaac.viplanner/data/town01/people_cfg.yml
	deleted:    isaaclab/extension/omni.isaac.viplanner/data/town01/vehicle_cfg.yml
	deleted:    isaaclab/extension/omni.isaac.viplanner/data/town02/cw_multiply_cfg.yml
	deleted:    isaaclab/extension/omni.isaac.viplanner/data/town02/keyword_mapping.yml
	deleted:    isaaclab/extension/omni.isaac.viplanner/data/town02/vehicle_cfg.yml
	deleted:    isaaclab/extension/omni.isaac.viplanner/data/warehouse/keyword_mapping.yml
	deleted:    isaaclab/extension/omni.isaac.viplanner/data/warehouse/people_cfg.yml
	deleted:    isaaclab/extension/omni.isaac.viplanner/intern/carla_exploration/config.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/intern/carla_exploration/explorer.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/__init__.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/eval_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/evaluator.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/vip_algo.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/vip_anymal.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/vip_config.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/visual_imperative_planner.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/walking_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/intern/matterport_exploration/__init__.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/intern/matterport_exploration/random_exploration.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/intern/matterport_exploration/sampler_config.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/intern/run_scripts/anymal_run_carla.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/intern/run_scripts/anymal_run_warehouse.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/__init__.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/base_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/carla_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/custom_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/custom_objnav_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/g1/__init__.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/g1/g1_low_base_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/g1/g1_low_vision_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go1/__init__.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go1/go1_low_base_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go1/go1_low_vision_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/__init__.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_low_base_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_low_base_nav_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_low_vision_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_matterport_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/__init__.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_2dof_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_base_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_vision_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_matterport_deploy_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_matterport_deploy_gpt_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/test_h1_low_terrain_generator_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/test_h1_matterport_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/train_h1_matterport_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/matterport_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/test_obj_nav_matterport_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/train_obj_nav_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/train_obj_nav_low_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/warehouse_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/warehouse_objnav_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/__init__.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/inference_seem_pano.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/matterport_raycast_camera.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/unreal_importer.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/unreal_importer_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/viplanner_matterport_raycast_camera.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/visualizer.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/wrappers.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/__init__.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/__init__.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/__init__.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/navigation_actions.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/vlm_navigation_actions.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/vlm_navigation_actions_gpt.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/__init__.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/goal_command_generator.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/goal_command_generator_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/lowlevel_command_generator.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/lowlevel_command_generator_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/midlevel_command_generator.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/midlevel_command_generator_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator_gpt.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator_gpt_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/rl_command_generator.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/rl_command_generator_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/robot_vel_command_generator.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/robot_vel_command_generator_cfg.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/curriculums.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/events.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/observations.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/rewards/__init__.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/rewards/objnav_rewards.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/viplanner_algo.py
	deleted:    isaaclab/extension/omni.isaac.viplanner/setup.py
	deleted:    isaaclab/extension/omni.waypoints/config/extension.toml
	deleted:    isaaclab/extension/omni.waypoints/omni/isaac/waypoints/recorder/__init__.py
	deleted:    isaaclab/extension/omni.waypoints/omni/isaac/waypoints/recorder/recorder.py
	deleted:    isaaclab/extension/omni.waypoints/omni/isaac/waypoints/scripts/__init__.py
	deleted:    isaaclab/extension/omni.waypoints/omni/isaac/waypoints/scripts/recorder_ui.py
	deleted:    isaaclab/extension/omni.waypoints/setup.py
	modified:   isaaclab/standalone/play.py
	modified:   isaaclab/standalone/play_low_matterport_keyboard.py
	modified:   isaaclab/standalone/train.py
	deleted:    pyproject.toml
	deleted:    tests/test_history_obs.py
	deleted:    viplanner/__init__.py
	deleted:    viplanner/config/__init__.py
	deleted:    viplanner/config/coco_sem_meta.py
	deleted:    viplanner/config/costmap_cfg.py
	deleted:    viplanner/config/learning_cfg.py
	deleted:    viplanner/config/viplanner_sem_meta.py
	deleted:    viplanner/cost_builder.py
	deleted:    viplanner/cost_maps/__init__.py
	deleted:    viplanner/cost_maps/cost_to_pcd.py
	deleted:    viplanner/cost_maps/sem_cost_map.py
	deleted:    viplanner/cost_maps/tsdf_cost_map.py
	deleted:    viplanner/depth_reconstruct.py
	deleted:    viplanner/plannernet/PlannerNet.py
	deleted:    viplanner/plannernet/__init__.py
	deleted:    viplanner/plannernet/autoencoder.py
	deleted:    viplanner/plannernet/rgb_encoder.py
	deleted:    viplanner/train.py
	deleted:    viplanner/traj_cost_opt/__init__.py
	deleted:    viplanner/traj_cost_opt/traj_cost.py
	deleted:    viplanner/traj_cost_opt/traj_opt.py
	deleted:    viplanner/traj_cost_opt/traj_viz.py
	deleted:    viplanner/utils/__init__.py
	deleted:    viplanner/utils/dataset.py
	deleted:    viplanner/utils/eval_utils.py
	deleted:    viplanner/utils/torchutil.py
	deleted:    viplanner/utils/trainer.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	isaaclab/extension/omni.isaac.leggednav/
	isaaclab/standalone/collect_data_loco.py
	isaaclab/standalone/collect_data_matterport.py
	isaaclab/standalone/collected_data/
	isaaclab/standalone/go2_matterport_data_20hz.zip
	isaaclab/standalone/run_data_collection.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/README.md b/README.md
index 8ab1f88..07d7238 100644
--- a/README.md
+++ b/README.md
@@ -13,7 +13,7 @@ conda activate isaaclab
 3. Install PyTorch and Isaac Sim packages.
 ```shell
 pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121
-pip install isaacsim-rl isaacsim-replicator isaacsim-extscache-physics isaacsim-extscache-kit-sdk isaacsim-extscache-kit isaacsim-app --extra-index-url https://pypi.nvidia.com
+pip install isaacsim-rl==4.1.0 isaacsim-replicator==4.1.0 isaacsim-extscache-physics==4.1.0 isaacsim-extscache-kit-sdk==4.1.0 isaacsim-extscache-kit==4.1.0 isaacsim-app==4.1.0 --extra-index-url https://pypi.nvidia.com
 ```
 
 2. Clone the Isaac Lab repository, and link extensions.
@@ -23,14 +23,13 @@ cd IsaacLab
 git checkout 3f96602eef
 cd source/extensions
 ln -s {THIS_REPO_DIR}/isaaclab/extension/omni.isaac.matterport .
-ln -s {THIS_REPO_DIR}/isaaclab/extension/omni.isaac.viplanner .
+ln -s {THIS_REPO_DIR}/isaaclab/extension/omni.isaac.leggednav .
 ```
 
 3. Run the Isaac Lab installer script and additionally install RSL RL and ViPlanner
 ```shell
 ./isaaclab.sh -i
 cd ../h1-training-isaaclab
-pip install -e .
 cd rsl_rl && pip install -e .
 ```
 
diff --git a/isaaclab/extension/omni.isaac.viplanner/config/extension.toml b/isaaclab/extension/omni.isaac.viplanner/config/extension.toml
deleted file mode 100644
index 538e824..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/config/extension.toml
+++ /dev/null
@@ -1,19 +0,0 @@
-[package]
-version = "0.0.1"
-title = "ViPlanner extension"
-description="Extension containing the novel ViPlanner: Visual Semantic Imperative Learning for Local Navigation"
-authors =["Pascal Roth", "Julian Nubert", "Fan Yang", "Mayank Mittal", "Marco Hutter"]
-repository = "https://github.com/leggedrobotics/viplanner"
-category = "robotics"
-keywords = ["kit", "robotics"]
-readme  = "docs/README.md"
-
-[dependencies]
-"omni.kit.uiapp" = {}
-"omni.isaac.ui" = {}
-"omni.isaac.core" = {}
-"omni.isaac.lab" = {}
-
-# Main python module this extension provides.
-[[python.module]]
-name = "omni.isaac.viplanner"
diff --git a/isaaclab/extension/omni.isaac.viplanner/data/mpcat40_to_vip_sem.yml b/isaaclab/extension/omni.isaac.viplanner/data/mpcat40_to_vip_sem.yml
deleted file mode 100644
index a7f7936..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/data/mpcat40_to_vip_sem.yml
+++ /dev/null
@@ -1,48 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-void:              static
-wall:              wall
-floor:             floor
-chair:             furniture
-door:              door
-table:             furniture
-picture:           wall
-cabinet:           furniture
-cushion:           furniture
-window:            wall
-sofa:              furniture
-bed:               furniture
-curtain:           wall
-chest_of_drawers:  furniture
-plant:             vegetation
-sink:              furniture
-stairs:            stairs
-ceiling:           ceiling
-toilet:            furniture
-stool:             furniture
-towel:             indoor_soft
-mirror:            wall
-tv_monitor:        wall
-shower:            furniture
-column:            wall
-bathtub:           furniture
-counter:           furniture
-fireplace:         furniture
-lighting:          static
-beam:              furniture
-railing:           wall
-shelving:          wall
-blinds:            wall
-gym_equipment:     furniture
-seating:           furniture
-board_panel:       wall
-furniture:         furniture
-appliances:        dynamic
-clothes:           indoor_soft
-objects:           static
-misc:              static
-unlabeled:         static
diff --git a/isaaclab/extension/omni.isaac.viplanner/data/town01/area_filter_cfg.yaml b/isaaclab/extension/omni.isaac.viplanner/data/town01/area_filter_cfg.yaml
deleted file mode 100644
index 9ed1fc3..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/data/town01/area_filter_cfg.yaml
+++ /dev/null
@@ -1,27 +0,0 @@
-# Definition of which areas should not be explored and used to sample points
-# Adjusted for: TOWN01
-
-# each entry has the following format:
-# name:
-#   x_low:      [float]           low number of the x axis
-#   x_high:     [float]           high number of the x axis
-#   y_low:      [float]           low number of the y axis
-#   y_high:     [float]           high number of the y axis
-
-area_1:
-  x_low:  208.9
-  x_high: 317.8
-  y_low:  100.5
-  y_high: 325.5
-
-area_2:
-  x_low:  190.3
-  x_high: 315.8
-  y_low:  12.7
-  y_high: 80.6
-
-area_3:
-  x_low:  123.56
-  x_high: 139.37
-  y_low:  10
-  y_high: 80.0
diff --git a/isaaclab/extension/omni.isaac.viplanner/data/town01/cw_multiply_cfg.yml b/isaaclab/extension/omni.isaac.viplanner/data/town01/cw_multiply_cfg.yml
deleted file mode 100644
index c6da529..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/data/town01/cw_multiply_cfg.yml
+++ /dev/null
@@ -1,178 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Definition of which crosswalks should be repeated how often along which axis
-# Adjusted for: TOWN01
-
-# each entry has the following format:
-# name:
-#   cw_prim:      [str]           prim of the crosswalk in the loaded town file
-#   factor:       [int]           number how often the crosswalk should be repeated
-#   translation:  [float, float]  vector along which the crosswalk should be repeated, defines the position of the first
-#                                 repeated crosswalk, every following crosswalk will be placed at the position of the
-#                                 previous one plus the translation vector
-#   suffix:       [str]           optional, str will be added to the copied prim of the new crosswalk
-
-# NOTE: rotations and scales applied to the mesh are not applied to the translations given here, i.e. they have to be
-#       in the original dataformat of the town file, i.e. y-up and in cm
-
-town_prim: "Town01_Opt/Town01_Opt"
-
-cw_2:
-  cw_prim: "Road_Crosswalk_Town01_2"
-  factor: 2
-  translation: [0, 0, -1500]
-
-cw_3_pos:
-  cw_prim: "Road_Crosswalk_Town01_3"
-  factor: 6
-  translation: [1500, 0, 0]
-
-cw_3_neg:
-  cw_prim: "Road_Crosswalk_Town01_3"
-  factor: 1
-  translation: [-1500, 0, 0]
-  suffix: "_neg"
-
-cw_4:
-  cw_prim: "Road_Crosswalk_Town01_4"
-  factor: 1
-  translation: [1500, 0, 0]
-
-cw_5:
-  cw_prim: "Road_Crosswalk_Town01_5"
-  factor: 3
-  translation: [1500, 0, 0]
-
-cw_6:
-  cw_prim: "Road_Crosswalk_Town01_6"
-  factor: 3
-  translation: [0, 0, -1500]
-
-cw_9:
-  cw_prim: "Road_Crosswalk_Town01_9"
-  factor: 2
-  translation: [0, 0, -1500]
-
-cw_10:
-  cw_prim: "Road_Crosswalk_Town01_10"
-  factor: 1
-  translation: [0, 0, 1500]
-
-cw_11:
-  cw_prim: "Road_Crosswalk_Town01_11"
-  factor: 1
-  translation: [0, 0, 1500]
-
-cw_14:
-  cw_prim: "Road_Crosswalk_Town01_14"
-  factor: 1
-  translation: [0, 0, 1500]
-
-cw_15:
-  cw_prim: "Road_Crosswalk_Town01_15"
-  factor: 2
-  translation: [0, 0, -1500]
-
-cw_18:
-  cw_prim: "Road_Crosswalk_Town01_18"
-  factor: 5
-  translation: [1500, 0, 0]
-
-cw_19:
-  cw_prim: "Road_Crosswalk_Town01_19"
-  factor: 2
-  translation: [1500, 0, 0]
-
-cw_21:
-  cw_prim: "Road_Crosswalk_Town01_21"
-  factor: 3
-  translation: [1500, 0, 0]
-
-cw_22:
-  cw_prim: "Road_Crosswalk_Town01_22"
-  factor: 5
-  translation: [1500, 0, 0]
-
-cw_24:
-  cw_prim: "Road_Crosswalk_Town01_24"
-  factor: 3
-  translation: [-1500, 0, 0]
-
-cw_26_pos:
-  cw_prim: "Road_Crosswalk_Town01_26"
-  factor: 5
-  translation: [1500, 0, 0]
-
-cw_26_neg:
-  cw_prim: "Road_Crosswalk_Town01_26"
-  factor: 3
-  translation: [-1500, 0, 0]
-  suffix: "_neg"
-
-cw_28:
-  cw_prim: "Road_Crosswalk_Town01_28"
-  factor: 4
-  translation: [0, 0, 1500]
-
-cw_29:
-  cw_prim: "Road_Crosswalk_Town01_29"
-  factor: 4
-  translation: [0, 0, 1500]
-
-cw_30:
-  cw_prim: "Road_Crosswalk_Town01_30"
-  factor: 4
-  translation: [0, 0, 1500]
-
-cw_31:
-  cw_prim: "Road_Crosswalk_Town01_31"
-  factor: 2
-  translation: [0, 0, 1500]
-
-cw_32:
-  cw_prim: "Road_Crosswalk_Town01_32"
-  factor: 6
-  translation: [0, 0, -1500]
-
-cw_33_pos:
-  cw_prim: "Road_Crosswalk_Town01_33"
-  factor: 4
-  translation: [1500, 0, 0]
-
-cw_33_neg:
-  cw_prim: "Road_Crosswalk_Town01_33"
-  factor: 3
-  translation: [-2500, 0, 0]
-  suffix: "_neg"
-
-cw_34:
-  cw_prim: "Road_Crosswalk_Town01_34"
-  factor: 7
-  translation: [1500, 0, 0]
-
-cw_35:
-  cw_prim: "Road_Crosswalk_Town01_35"
-  factor: 1
-  translation: [1500, 0, 0]
-
-cw_36_pos:
-  cw_prim: "Road_Crosswalk_Town01_36"
-  factor: 1
-  translation: [0, 0, 1500]
-
-cw_36_neg:
-  cw_prim: "Road_Crosswalk_Town01_36"
-  factor: 5
-  translation: [0, 0, -1500]
-  suffix: "_neg"
-
-cw_40:
-  cw_prim: "Road_Crosswalk_Town01_40"
-  factor: 4
-  translation: [1500, 0, 0]
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/data/town01/keyword_mapping.yml b/isaaclab/extension/omni.isaac.viplanner/data/town01/keyword_mapping.yml
deleted file mode 100644
index c51d095..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/data/town01/keyword_mapping.yml
+++ /dev/null
@@ -1,137 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-road:
-  - Road_Road
-  - Road_Marking
-  - ManholeCover
-  - roadunique
-sidewalk:
-  - Road_Sidewalk
-  - SideWalkCube
-  - Road_Grass  # pedestrian terrain (between building, squares, ...)
-crosswalk:
-  - Road_Crosswalk
-floor:
-  - Pathwalk  # way to the door of a building
-  - PathWay  # wat to the door of a building
-  - curb
-  - iron_plank
-  - Cube
-vehicle:
-  - Van
-  - Vehicle
-  - Car
-building:
-  - NewBlueprint  # roofs, windows, other parts of buildings
-  - CityBuilding
-  - Suburb
-  - House
-  - MergingBuilding
-  - BuildingWall
-  - garage
-  - airConditioner
-  - Office
-  - Block
-  - Apartment
-  - ConstructBuilding
-  - snacksStand
-  - doghouse
-  - streetCounter
-  - fountain
-  - container
-  - pergola
-  - GuardShelter
-  - atm
-  - awning
-  - bus_stop
-  - NewsStand
-  - ironplank
-  - kiosk
-wall:
-  - GardenWall
-  - Wall
-  - RepSpline  # fences or walls to limit residential areas
-  - RepeatedMeshesAlongSpline  # should make the spline go around the building --> not working in isaac
-fence:
-  - urbanFence
-  - chain_barrier
-  - picketFence
-  - fence
-pole:
-  - bollard
-  - Lamppost
-  - Parklight
-  - CityLamp
-  - Traffic_Light_Base
-traffic_sign:
-  - streetBillboard
-  - RoundSign
-  - roadsigns
-traffic_light:
-  - TLights
-  - TL_BotCover
-bench:
-  - bench
-vegetation:
-  - tree
-  - Stone
-  - Cypress
-  - PlantPot
-  - TreePot
-  - Maple
-  - Beech
-  - FanPalm
-  - Sassafras
-  - Pine_Bush
-  - Hedge
-  - Bush
-  - palm
-  - acer
-terrain:
-  - dirtDebris  # roughness in the terrain, street or sidewalk (traversable but more difficult)
-  - GrassLeaf
-  - Grass
-  - LandscapeComponent
-  - Ash
-water_surface:
-  - TileLake
-sky:
-  - terrain2
-  - sky
-dynamic:
-  - Trashbag
-  - advertise
-  - creased_box
-  - garbage
-  - trashcan
-  - clothes_line
-  - barbecue
-  - ConstructionCone
-  - box
-  - droppingasset
-  - barrel
-static:
-  - firehydrant
-  - Gnome
-  - metroMap
-  - Bikeparking
-  - StaticMesh  # gate barrier
-  - trampoline
-  - wheelbarrow
-  - NewspaperBox
-  - swing
-  - bin
-  - big_plane
-  - slide
-  - instancedfoliageactor
-  - roadbillboard
-  - prophitreacting_child  # vending machines
-furniture:
-  - Campingtable
-  - swingcouch
-  - table
-  - chair
diff --git a/isaaclab/extension/omni.isaac.viplanner/data/town01/people_cfg.yml b/isaaclab/extension/omni.isaac.viplanner/data/town01/people_cfg.yml
deleted file mode 100644
index 0204db6..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/data/town01/people_cfg.yml
+++ /dev/null
@@ -1,559 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Definition of where people should be added and where to which position they should walk
-# Adjusted for: TOWN01
-
-# each entry has the following format:
-# name:
-#   prim_name:    [str]                   final prim part of the person defined as /World/People/prim_name
-#   translation:  [float, float, float]   initial position of the person
-#   target:       [float, float, float]   optional, target position where the person should walk to
-#   usd_path:     [str]                   optional, path to the usd file that should be loaded for the person
-#                                         if None, "People/Characters/F_Business_02/F_Business_02.usd" will be loaded
-
-person_1:
-  prim_name: "Person_1"
-  translation: [3740.228313568127, 586.8435707315994, 0.0]
-  target: [0, 0, 0]
-  usd_path: People/Characters/male_adult_police_04/male_adult_police_04.usd
-
-person_2:
-  prim_name: "Person_2"
-  translation: [5246.217283123889, 586.843570731599, 0.0]
-  target: [0, 0, 0]
-
-person_3:
-  prim_name: "Person_3"
-  translation: [5415.460881607675, 639.5094053915548, 0.6]
-  target: [0, 0, 0]
-
-person_4:
-  prim_name: "Person_4"
-  translation: [5714.735140961697, 550.4730007356956, 0.7]
-  target: [0, 0, 0]
-
-person_5:
-  prim_name: "Person_5"
-  translation: [7078.016845824465, -568.2695538281929, 0.0]
-  target: [0, 0, 0]
-
-person_6:
-  prim_name: "Person_6"
-  translation: [8120.543989818335, -568.2695538281931, 0.0]
-  target: [0, 0, 0]
-
-person_7:
-  prim_name: "Person_7"
-  translation: [669.3401063595267, 27118.130697694694, 0.7]
-  target: [0, 0, 0]
-
-person_8:
-  prim_name: "Person_8"
-  translation: [12246.122318180935, -545.4147267796062, 0.0]
-  target: [0, 0, 0]
-
-person_9:
-  prim_name: "Person_9"
-  translation: [12454.398076588071, -165.1673637204202, 0.0]
-  target: [0, 0, 0]
-
-person_10:
-  prim_name: "Person_10"
-  translation: [12613.586036894338, 907.0320886276056, 0.]
-  target: [0, 0, 0]
-
-person_11:
-  prim_name: "Person_11"
-  translation: [13368.88728944269, 1408.606986049066, 0.9]
-  target: [0, 0, 0]
-
-person_12:
-  prim_name: "Person_12"
-  translation: [12861.687394639228, 1873.3963671243043, 0.9]
-  target: [0, 0, 0]
-
-person_13:
-  prim_name: "Person_13"
-  translation: [13352.418407603123, 2262.531599879499, 0.]
-  target: [0, 0, 0]
-
-person_14:
-  prim_name: "Person_14"
-  translation: [12589.522051888143, 2629.8556750820458, 0.2]
-  target: [0, 0, 0]
-
-person_15:
-  prim_name: "Person_15"
-  translation: [12848.62562034568, 3750.556652440331, 0.6]
-  target: [0, 0, 0]
-
-person_16:
-  prim_name: "Person_16"
-  translation: [13217.034580605141, 4308.035384643077, 0.5]
-  target: [0, 0, 0]
-
-person_17:
-  prim_name: "Person_17"
-  translation: [13792.626413019212, 5007.6197554025275, 0.9]
-  target: [0, 0, 0]
-
-person_18:
-  prim_name: "Person_18"
-  translation: [13413.62053388464, 5163.241895655993, 0.5]
-  target: [0, 0, 0]
-
-person_19:
-  prim_name: "Person_19"
-  translation: [12674.490992287037, 5588.466017703104, 0.6]
-  target: [0, 0, 0]
-
-person_20:
-  prim_name: "Person_20"
-  translation: [13022.107716879374, 6431.315030491002, 0.8]
-  target: [0, 0, 0]
-
-person_21:
-  prim_name: "Person_21"
-  translation: [13325.604194516001, 6821.7461451991085, 0.8]
-  target: [0, 0, 0]
-
-person_22:
-  prim_name: "Person_22"
-  translation: [12867.694754653112, 7059.330001453522, 0.4]
-  target: [0, 0, 0]
-
-person_23:
-  prim_name: "Person_23"
-  translation: [12645.85107322485, 7315.704436075569, 0.8]
-  target: [0, 0, 0]
-
-person_24:
-  prim_name: "Person_24"
-  translation: [12985.924188935845, 7564.827497346252, 0.2]
-  target: [0, 0, 0]
-
-person_25:
-  prim_name: "Person_25"
-  translation: [12545.791293968454, 8388.067141863512, 0.4]
-  target: [0, 0, 0]
-
-person_26:
-  prim_name: "Person_26"
-  translation: [11970.334229880405, 10064.216006417662, 0.7]
-  target: [0, 0, 0]
-
-person_27:
-  prim_name: "Person_27"
-  translation: [16455.25980024711, 8426.573650093294, 0.2]
-  target: [0, 0, 0]
-
-person_28:
-  prim_name: "Person_28"
-  translation: [19493.479542700436, 8426.573650093298, 0.2]
-  target: [0, 0, 0]
-
-person_29:
-  prim_name: "Person_29"
-  translation: [20526.827464454396, 7798.145489012485, 0.7]
-  target: [0, 0, 0]
-
-person_30:
-  prim_name: "Person_30"
-  translation: [21466.110178370192, 8248.379807773212, 0.7]
-  target: [0, 0, 0]
-
-person_31:
-  prim_name: "Person_31"
-  translation: [20492.699328867577, 10354.416282635397, 0.]
-  target: [0, 0, 0]
-
-person_32:
-  prim_name: "Person_32"
-  translation: [20586.597906936375, 11779.56415544446, 0.9]
-  target: [0, 0, 0]
-
-person_33:
-  prim_name: "Person_33"
-  translation: [20758.345724032693, 13507.94105377282, 0.]
-  target: [0, 0, 0]
-
-person_34:
-  prim_name: "Person_34"
-  translation: [18844.11145852961, 13802.294017785467, 0.3]
-  target: [0, 0, 0]
-
-person_35:
-  prim_name: "Person_35"
-  translation: [20857.845387803143, 15851.986625184061, 0.8]
-  target: [0, 0, 0]
-
-person_36:
-  prim_name: "Person_36"
-  translation: [20629.472766729676, 17882.921568668447, 0.4]
-  target: [0, 0, 0]
-
-person_37:
-  prim_name: "Person_37"
-  translation: [19005.51336634387, 18210.348243135773, 0.5]
-  target: [0, 0, 0]
-
-person_38:
-  prim_name: "Person_38"
-  translation: [23374.868579878137, 16438.840644955762, 0.5]
-  target: [0, 0, 0]
-
-person_39:
-  prim_name: "Person_39"
-  translation: [26968.844230071987, 15807.94332246256, 0.9]
-  target: [0, 0, 0]
-
-person_40:
-  prim_name: "Person_40"
-  translation: [29283.786757962414, 13816.015970719725, 0.3]
-  target: [0, 0, 0]
-
-person_41:
-  prim_name: "Person_41"
-  translation: [29283.786757962414, 17726.67919711671, 0.6]
-  target: [0, 0, 0]
-
-person_42:
-  prim_name: "Person_42"
-  translation: [29184.925736051493, 16185.80334517331, 0.]
-  target: [0, 0, 0]
-
-person_43:
-  prim_name: "Person_43"
-  translation: [27930.09977489682, 16185.803345173312, 0.]
-  target: [0, 0, 0]
-
-person_44:
-  prim_name: "Person_44"
-  translation: [27020.43586667152, 17164.04879540286, 0.9]
-  target: [0, 0, 0]
-
-person_45:
-  prim_name: "Person_45"
-  translation: [26806.54440321764, 18968.794397555444, 0.]
-  target: [0, 0, 0]
-
-person_46:
-  prim_name: "Person_46"
-  translation: [27677.42708073639, 20201.849744256044, 0.1]
-  target: [0, 0, 0]
-
-person_47:
-  prim_name: "Person_47"
-  translation: [29163.542299117173, 20201.849744256047, 0.1]
-  target: [0, 0, 0]
-
-person_48:
-  prim_name: "Person_48"
-  translation: [25767.63098586499, 21065.76414542053, 0.2]
-  target: [0, 0, 0]
-
-person_49:
-  prim_name: "Person_49"
-  translation: [25034.203438218083, 22863.653092037155, 0.3]
-  target: [0, 0, 0]
-
-person_50:
-  prim_name: "Person_50"
-  translation: [25439.504130263675, 24418.94870769878, 0.7]
-  target: [0, 0, 0]
-
-person_51:
-  prim_name: "Person_51"
-  translation: [27348.159291431577, 25579.124584965954, 0.2]
-  target: [0, 0, 0]
-
-person_52:
-  prim_name: "Person_52"
-  translation: [28959.387772557588, 24705.031440789277, 0.1]
-  target: [0, 0, 0]
-
-person_53:
-  prim_name: "Person_53"
-  translation: [31643.27629427197, 24705.031440789277, 0.1]
-  target: [0, 0, 0]
-
-person_54:
-  prim_name: "Person_54"
-  translation: [32234.44677345883, 22249.75349995971, 0.5]
-  target: [0, 0, 0]
-
-person_55:
-  prim_name: "Person_55"
-  translation: [32234.446773458832, 25423.02995967793, 0.6]
-  target: [0, 0, 0]
-
-person_56:
-  prim_name: "Person_56"
-  translation: [33435.151982953554, 27223.591260808997, 0.6]
-  target: [0, 0, 0]
-
-person_57:
-  prim_name: "Person_57"
-  translation: [33435.151982953554, 23616.846603557944, 0.6]
-  target: [0, 0, 0]
-
-person_58:
-  prim_name: "Person_58"
-  translation: [33435.151982953554, 17508.29317521096, 0.]
-  target: [0, 0, 0]
-
-person_59:
-  prim_name: "Person_59"
-  translation: [32386.4715869386, 19199.430950091366, 0.4]
-  target: [0, 0, 0]
-
-person_60:
-  prim_name: "Person_60"
-  translation: [27397.8098531043, 27598.61567839424, 0.8]
-  target: [0, 0, 0]
-
-person_61:
-  prim_name: "Person_61"
-  translation: [27215.64740847075, 28887.31213064116, 0.9]
-  target: [0, 0, 0]
-
-person_62:
-  prim_name: "Person_62"
-  translation: [27914.59251018619, 29575.446673311068, 0.3]
-  target: [0, 0, 0]
-
-person_63:
-  prim_name: "Person_63"
-  translation: [28559.611754879934, 29946.203008516477, 0.5]
-  target: [0, 0, 0]
-
-person_64:
-  prim_name: "Person_64"
-  translation: [27159.84826758545, 30664.6871971984, 0.]
-  target: [0, 0, 0]
-
-person_65:
-  prim_name: "Person_65"
-  translation: [27364.006370124185, 31488.878763581935, 0.0]
-  target: [0, 0, 0]
-
-person_66:
-  prim_name: "Person_66"
-  translation: [27364.006370124185, 33000.39972613324, 0.7]
-  target: [0, 0, 0]
-
-person_67:
-  prim_name: "Person_67"
-  translation: [30318.853757701392, 32704.242178232933, 0.4]
-  target: [0, 0, 0]
-
-person_68:
-  prim_name: "Person_68"
-  translation: [23912.230649149726, 32704.242178232944, 0.4]
-  target: [0, 0, 0]
-
-person_69:
-  prim_name: "Person_69"
-  translation: [21185.295446489567, 33118.704270787915, 0.5]
-  target: [0, 0, 0]
-
-person_70:
-  prim_name: "Person_70"
-  translation: [20958.25256311863, 30360.79199306313, 0.6]
-  target: [0, 0, 0]
-
-person_71:
-  prim_name: "Person_71"
-  translation: [20415.40938302089, 23865.487076097994, 0.]
-  target: [0, 0, 0]
-
-person_72:
-  prim_name: "Person_72"
-  translation: [20768.258224735568, 24457.860121168047, 0.7]
-  target: [0, 0, 0]
-
-person_73:
-  prim_name: "Person_73"
-  translation: [20276.9034763617, 26857.79211892222, 0.3]
-  target: [0, 0, 0]
-
-person_74:
-  prim_name: "Person_74"
-  translation: [18771.30782588506, 26857.79211892222, 0.3]
-  target: [0, 0, 0]
-
-person_75:
-  prim_name: "Person_75"
-  translation: [18771.30782588506, 29451.612371087005, 0.6]
-  target: [0, 0, 0]
-
-person_76:
-  prim_name: "Person_76"
-  translation: [19215.142194720564, 32349.983087295055, 0.2]
-  target: [0, 0, 0]
-
-person_77:
-  prim_name: "Person_77"
-  translation: [13693.71462220456, 32349.983087295055, 0.2]
-  target: [0, 0, 0]
-
-person_78:
-  prim_name: "Person_78"
-  translation: [13839.638955652783, 29943.90189726095, 0.5]
-  target: [0, 0, 0]
-
-person_79:
-  prim_name: "Person_79"
-  translation: [13839.638955652781, 26402.9290316626, 0.3]
-  target: [0, 0, 0]
-
-person_80:
-  prim_name: "Person_80"
-  translation: [12528.438232933597, 26402.929031662607, 0.3]
-  target: [0, 0, 0]
-
-person_81:
-  prim_name: "Person_81"
-  translation: [12528.438232933597, 23693.389492662598, 0.4]
-  target: [0, 0, 0]
-
-person_82:
-  prim_name: "Person_82"
-  translation: [14632.706837288715, 22818.729972244953, 0.3]
-  target: [0, 0, 0]
-
-person_83:
-  prim_name: "Person_83"
-  translation: [15522.927373719667, 22704.509900589725, 0.6]
-  target: [0, 0, 0]
-
-person_84:
-  prim_name: "Person_84"
-  translation: [16811.678329347546, 22313.725497932173, 0.4]
-  target: [0, 0, 0]
-
-person_85:
-  prim_name: "Person_85"
-  translation: [17594.423783999147, 22750.35284842414, 0.6]
-  target: [0, 0, 0]
-
-person_86:
-  prim_name: "Person_86"
-  translation: [12507.983441768796, 18737.82969853814, 0.4]
-  target: [0, 0, 0]
-
-person_87:
-  prim_name: "Person_87"
-  translation: [13702.468187382483, 16232.793598129021, 0.9]
-  target: [0, 0, 0]
-
-person_88:
-  prim_name: "Person_88"
-  translation: [13702.468187382483, 12467.78105864283, 0.3]
-  target: [0, 0, 0]
-
-person_89:
-  prim_name: "Person_89"
-  translation: [12405.19284595733, 13889.846778529223, 0.2]
-  target: [0, 0, 0]
-
-person_90:
-  prim_name: "Person_90"
-  translation: [9340.771938544503, 9756.78115441167, 0.4]
-  target: [0, 0, 0]
-
-person_91:
-  prim_name: "Person_91"
-  translation: [10686.07873701808, 8501.262543001658, 0.1]
-  target: [0, 0, 0]
-
-person_92:
-  prim_name: "Person_92"
-  translation: [5829.459928215348, 8031.938804987546, 0.2]
-  target: [0, 0, 0]
-
-person_93:
-  prim_name: "Person_93"
-  translation: [3527.334094492171, 8460.536849529562, 0.2]
-  target: [0, 0, 0]
-
-person_94:
-  prim_name: "Person_94"
-  translation: [5097.116671532964, 10872.918851535747, 0.8]
-  target: [0, 0, 0]
-
-person_95:
-  prim_name: "Person_95"
-  translation: [5217.790807492532, 13373.640330412796, 0.3]
-  target: [0, 0, 0]
-
-person_96:
-  prim_name: "Person_96"
-  translation: [6363.9988395402015, 13373.640330412796, 0.3]
-  target: [0, 0, 0]
-
-person_97:
-  prim_name: "Person_97"
-  translation: [6363.998839540203, 11613.608571085351, 0.3]
-  target: [0, 0, 0]
-
-person_98:
-  prim_name: "Person_98"
-  translation: [6363.998839540205, 20397.9692173452, 0.7]
-  target: [0, 0, 0]
-
-person_99:
-  prim_name: "Person_99"
-  translation: [5044.1103127644465, 20914.39894895205, 0.5]
-  target: [0, 0, 0]
-
-person_100:
-  prim_name: "Person_100"
-  translation: [5044.110312764447, 24788.278668184572, 0.]
-  target: [0, 0, 0]
-
-person_101:
-  prim_name: "Person_101"
-  translation: [6431.492469622843, 26239.47428713576, 0.7]
-  target: [0, 0, 0]
-
-person_102:
-  prim_name: "Person_102"
-  translation: [5124.4324559690385, 27614.946714461115, 0.8]
-  target: [0, 0, 0]
-
-person_103:
-  prim_name: "Person_103"
-  translation: [6887.443716248569, 30328.582141414503, 0.6]
-  target: [0, 0, 0]
-
-person_104:
-  prim_name: "Person_104"
-  translation: [6887.443716248573, 29338.22343903503, 0.9]
-  target: [0, 0, 0]
-
-person_105:
-  prim_name: "Person_105"
-  translation: [6887.443716248571, 18930.575403546387, 0.]
-  target: [0, 0, 0]
-
-person_106:
-  prim_name: "Person_106"
-  translation: [1511.2858517975667, 31457.703052385103, 0.1]
-  target: [0, 0, 0]
-
-person_107:
-  prim_name: "Person_107"
-  translation: [669.3401063595277, 31837.942704536705, 0.4]
-  target: [0, 0, 0]
-
-person_108:
-  prim_name: "Person_108"
-  translation: [20331.6293020427, 32278.622047147244, 0.4]
-  target: [0, 0, 0]
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/data/town01/vehicle_cfg.yml b/isaaclab/extension/omni.isaac.viplanner/data/town01/vehicle_cfg.yml
deleted file mode 100644
index 355ab23..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/data/town01/vehicle_cfg.yml
+++ /dev/null
@@ -1,75 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Definition of where additional vehicles should be added
-# Adjusted for: TOWN01
-
-# each entry has the following format:
-# name:
-#   prim_part:    [str]                         part of the prim of the vehicle that should be multiplied (every prim containing this string will be multiplied)
-#   translation:  [[float, float, float]]       list of translations of the vehicle
-
-# NOTE: rotations and scales applied to the mesh are not applied to the translations given here, i.e. they have to be
-#       in the original dataformat of the town file, i.e. y-up and in cm
-
-# NOTE: for Town01, take "ChevroletImpala_High_V4" for vehicles along the x axis and "JeepWranglerRubicon_36"
-#       for vehicles along the y axis
-
-town_prim: "Town01_Opt"
-
-vehicle_1:
-  prim_part: "ChevroletImpala_High_V4"
-  translation:
-    - [-15300, 0, -4000]
-    - [-15300, 0, 0]
-    - [-15300, 0, 15000]
-    - [-15600, 0, 21000]
-    - [9000, 0, 20500]
-    - [9400, 0, 15000]
-    - [9400, 0, 9000]
-    - [9400, 0, 7000]
-    - [9000, 0, 6000]
-    - [9000, 0, 500]
-    - [9000, 0, -4000]
-
-vehicle_2:
-  prim_part: "JeepWranglerRubicon_36"
-  translation:
-    - [0, 0, -1500]
-    - [3500, 0, -1500]
-    - [5300, 0, -1900]
-    - [9000, 0, -1900]
-    - [16500, 0, -1500]
-    - [22500, 0, -1900]
-    - [25000, 0, 3800]
-    - [20000, 0, 4200]
-    - [17000, 0, 4200]
-    - [12000, 0, 3800]
-    - [7000, 0, 3800]
-    - [7000, 0, 11100]
-    - [11000, 0, 11500]
-    - [16000, 0, 11100]
-    - [20000, 0, 11100]
-    - [26000, 0, 11500]
-    - [26000, 0, 17800]
-    - [23000, 0, 18200]
-    - [18000, 0, 18200]
-    - [14000, 0, 17800]
-    - [13500, 0, 18200]
-    - [10000, 0, 18200]
-    - [9500, 0, 17800]
-    - [4000, 0, 17800]
-    - [2000, 0, 30800]
-    - [-1000, 0, 31300]
-    - [6000, 0, 31300]
-    - [12000, 0, 30800]
-    - [15000, 0, 30800]
-    - [15600, 0, 30800]
-    - [16400, 0, 30800]
-    - [21000, 0, 31300]
-    - [25000, 0, 31300]
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/data/town02/cw_multiply_cfg.yml b/isaaclab/extension/omni.isaac.viplanner/data/town02/cw_multiply_cfg.yml
deleted file mode 100644
index 9d681e7..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/data/town02/cw_multiply_cfg.yml
+++ /dev/null
@@ -1,140 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Definition of which crosswalks should be repeated how often along which axis
-# Adjusted for: TOWN02
-
-# each entry has the following format:
-# name:
-#   cw_prim:      [str]           prim of the crosswalk in the loaded town file
-#   factor:       [int]           number how often the crosswalk should be repeated
-#   translation:  [float, float]  vector along which the crosswalk should be repeated, defines the position of the first
-#                                 repeated crosswalk, every following crosswalk will be placed at the position of the
-#                                 previous one plus the translation vector
-#   suffix:       [str]           optional, str will be added to the copied prim of the new crosswalk
-
-# NOTE: rotations and scales applied to the mesh are not applied to the translations given here, i.e. they have to be
-#       in the original dataformat of the town file, i.e. y-up and in cm
-
-town_prim: "Town02"
-
-cw_2:
-  cw_prim: "Road_Crosswalk_Town02_8"
-  factor: 4
-  translation: [+1500, 0, 0]
-
-cw_3:
-  cw_prim: "Road_Crosswalk_Town02_10"
-  factor: 2
-  translation: [-1500, 0, 0]
-
-cw_4:
-  cw_prim: "Road_Crosswalk_Town02_9"
-  factor: 4
-  translation: [+1500, 0, 0]
-  suffix: "_neg"
-
-cw_5:
-  cw_prim: "Road_Crosswalk_Town02_11"
-  factor: 4
-  translation: [1500, 0, 0]
-
-cw_6_pos:
-  cw_prim: "Road_Crosswalk_Town02_12"
-  factor: 1
-  translation: [0, 0, 1500]
-
-cw_6_neg:
-  cw_prim: "Road_Crosswalk_Town02_12"
-  factor: 2
-  translation: [0, 0, -1500]
-
-cw_7_neg:
-  cw_prim: "Road_Crosswalk_Town02_7"
-  factor: 1
-  translation: [-1500, 0, 0]
-
-cw_7_pos:
-  cw_prim: "Road_Crosswalk_Town02_7"
-  factor: 1
-  translation: [1500, 0, 0]
-
-cw_8:
-  cw_prim: "Road_Crosswalk_Town02_4"
-  factor: 2
-  translation: [1500, 0, 0]
-
-cw_9:
-  cw_prim: "Road_Crosswalk_Town02_3"
-  factor: 4
-  translation: [1500, 0, 0]
-
-cw_10:
-  cw_prim: "Road_Crosswalk_Town02_6"
-  factor: 2
-  translation: [-1500, 0, 0]
-
-cw_11_neg:
-  cw_prim: "Road_Crosswalk_Town02_1"
-  factor: 4
-  translation: [-1500, 0, 0]
-
-cw_11_pos:
-  cw_prim: "Road_Crosswalk_Town02_1"
-  factor: 2
-  translation: [+1500, 0, 0]
-
-cw_12:
-  cw_prim: "Road_Crosswalk_Town02_2"
-  factor: 4
-  translation: [-1500, 0, 0]
-
-cw_13:
-  cw_prim: "Road_Crosswalk_Town02_13"
-  factor: 2
-  translation: [0, 0, +1500]
-
-cw_14_pos:
-  cw_prim: "Road_Crosswalk_Town02_15"
-  factor: 2
-  translation: [0, 0, +1500]
-
-cw_14_neg:
-  cw_prim: "Road_Crosswalk_Town02_15"
-  factor: 1
-  translation: [0, 0, -1500]
-
-cw_15:
-  cw_prim: "Road_Crosswalk_Town02_16"
-  factor: 2
-  translation: [0, 0, -1500]
-
-cw_16_neg:
-  cw_prim: "Road_Crosswalk_Town02_17"
-  factor: 2
-  translation: [0, 0, -1500]
-
-cw_16_pos:
-  cw_prim: "Road_Crosswalk_Town02_17"
-  factor: 4
-  translation: [0, 0, +1500]
-
-cw_17_neg:
-  cw_prim: "Road_Crosswalk_Town02_19"
-  factor: 4
-  translation: [0, 0, -1500]
-
-cw_17_pos:
-  cw_prim: "Road_Crosswalk_Town02_19"
-  factor: 1
-  translation: [0, 0, +1500]
-
-cw_18:
-  cw_prim: "Road_Crosswalk_Town02_20"
-  factor: 3
-  translation: [0, 0, +1500]
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/data/town02/keyword_mapping.yml b/isaaclab/extension/omni.isaac.viplanner/data/town02/keyword_mapping.yml
deleted file mode 100644
index c6eee18..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/data/town02/keyword_mapping.yml
+++ /dev/null
@@ -1,147 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-road:
-  - Road_Road
-  - Road_Marking
-  - ManholeCover
-  - roadunique
-sidewalk:
-  - Road_Sidewalk
-  - SideWalkCube
-  - Road_Grass  # pedestrian terrain (between building, squares, ...)
-crosswalk:
-  - Road_Crosswalk
-floor:
-  - Pathwalk  # way to the door of a building
-  - PathWay  # wat to the door of a building
-  - curb
-  - iron_plank
-  - Cube
-  - Floor
-vehicle:
-  - Van
-  - Vehicle
-  - Car
-building:
-  - NewBlueprint  # roofs, windows, other parts of buildings
-  - CityBuilding
-  - Suburb
-  - House
-  - MergingBuilding
-  - BuildingWall
-  - garage
-  - airConditioner
-  - Office
-  - Block
-  - Apartment
-  - ConstructBuilding
-  - snacksStand
-  - doghouse
-  - streetCounter
-  - fountain
-  - container
-  - pergola
-  - GuardShelter
-  - atm
-  - awning
-  - bus_stop
-  - NewsStand
-  - ironplank
-  - kiosk
-  - TownHall
-wall:
-  - GardenWall
-  - Wall
-  - RepSpline  # fences or walls to limit residential areas
-  - RepeatedMeshesAlongSpline  # should make the spline go around the building --> not working in isaac
-fence:
-  - urbanFence
-  - chain_barrier
-  - picketFence
-  - fence
-pole:
-  - bollard
-  - Lamppost
-  - Parklight
-  - CityLamp
-  - Traffic_Light_Base
-  - ElectricPole
-  - PoleCylinder
-traffic_sign:
-  - streetBillboard
-  - RoundSign
-  - roadsigns
-traffic_light:
-  - TLights
-  - TL_BotCover
-  - SM_Charger
-  - SM_FreewayLights
-bench:
-  - bench
-vegetation:
-  - tree
-  - Stone
-  - Cypress
-  - PlantPot
-  - TreePot
-  - Maple
-  - Beech
-  - FanPalm
-  - Sassafras
-  - Pine_Bush
-  - Hedge
-  - Bush
-  - palm
-  - acer
-  - plant_pit
-  - arbusto_pine
-terrain:
-  - dirtDebris  # roughness in the terrain, street or sidewalk (traversable but more difficult)
-  - GrassLeaf
-  - Grass
-  - LandscapeComponent
-  - Ash
-water_surface:
-  - TileLake
-sky:
-  - terrain2
-  - sky
-dynamic:
-  - Trashbag
-  - advertise
-  - creased_box
-  - garbage
-  - trashcan
-  - clothes_line
-  - barbecue
-  - ConstructionCone
-  - box
-  - droppingasset
-  - barrel
-static:
-  - firehydrant
-  - Gnome
-  - metroMap
-  - Bikeparking
-  - StaticMesh  # gate barrier
-  - trampoline
-  - wheelbarrow
-  - NewspaperBox
-  - swing
-  - bin
-  - big_plane
-  - plane
-  - slide
-  - instancedfoliageactor
-  - roadbillboard
-  - prophitreacting_child  # vending machines
-  - prop_wateringcan
-furniture:
-  - Campingtable
-  - swingcouch
-  - table
-  - chair
diff --git a/isaaclab/extension/omni.isaac.viplanner/data/town02/vehicle_cfg.yml b/isaaclab/extension/omni.isaac.viplanner/data/town02/vehicle_cfg.yml
deleted file mode 100644
index 0a33aa7..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/data/town02/vehicle_cfg.yml
+++ /dev/null
@@ -1,53 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Definition of where additional vehicles should be added
-# Adjusted for: TOWN02
-
-# each entry has the following format:
-# name:
-#   prim_part:    [str]                         part of the prim of the vehicle that should be multiplied (every prim containing this string will be multiplied)
-#   translation:  [[float, float, float]]       list of translations of the vehicle
-
-# NOTE: rotations and scales applied to the mesh are not applied to the translations given here, i.e. they have to be
-#       in the original dataformat of the town file, i.e. y-up and in cm
-
-# NOTE: for Town02, take "Vh_Car_SeatLeon_54" for vehicles along the x axis
-
-town_prim: "Town02"
-
-vehicle_1:
-  prim_part: "Vh_Car_SeatLeon_54"
-  translation:
-  # horizontal road low
-    - [3900,   0,    600]
-    - [3900,   0,   3000]
-    - [3900,   0,   3500]
-    - [3900,   0,   4000]
-    - [3900,   0,   6000]
-    - [3900,   0,  -1500]
-    - [3900,   0,  -4000]
-    - [3900,   0,  -7500]
-    - [3900,   0,  -8000]
-    - [3500,   0, -10000]
-    - [3500,   0,  -7500]
-    - [3500,   0,  -3000]
-    - [3500,   0,   1000]
-    - [3500,   0,   5000]
-    # horizontal road middle
-    - [-10800, 0,   1000]
-    - [-10800, 0,   5000]
-    - [-10800, 0,  -2500]
-    # horizontal road high
-    - [-15800, 0,   2000]
-    - [-15800, 0,   4700]
-    - [-16200, 0,   3400]
-    - [-16200, 0,      0]
-    - [-16200, 0,  -3000]
-    - [-16200, 0,  -6000]
-    - [-16200, 0,  -9000]
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/data/warehouse/keyword_mapping.yml b/isaaclab/extension/omni.isaac.viplanner/data/warehouse/keyword_mapping.yml
deleted file mode 100644
index d340d84..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/data/warehouse/keyword_mapping.yml
+++ /dev/null
@@ -1,45 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-floor:
-  - SM_Floor1
-  - SM_Floor2
-  - SM_Floor3
-  - SM_Floor4
-  - SM_Floor5
-  - SM_Floor6
-  - groundplane
-
-wall:
-  - FuseBox
-  - SM_PillarA
-  - SM_Sign
-  - SM_Wall
-  - S_Barcode
-
-bench:
-  - Bench
-
-ceiling:
-  - SM_Ceiling
-  - PillarPartA
-  - SM_Beam
-  - SM_Bracket
-
-static:
-  - LampCeiling
-  - SM_FloorDecal
-  - SM_FireExtinguisher
-
-furniture:
-  - SM_Rack
-  - SM_SignCVer
-  - S_AisleSign
-  - SM_Palette
-  - SM_CardBox
-  - SmallKLT
-  - SM_PushCarta
-  - SM_CratePlastic
diff --git a/isaaclab/extension/omni.isaac.viplanner/data/warehouse/people_cfg.yml b/isaaclab/extension/omni.isaac.viplanner/data/warehouse/people_cfg.yml
deleted file mode 100644
index ee049f8..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/data/warehouse/people_cfg.yml
+++ /dev/null
@@ -1,65 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-person_1:
-  prim_name: "Person_1"
-  translation: [4.23985, -2.42198, 0.0]
-  target: [0, 0, 0]
-  usd_path: People/Characters/male_adult_construction_01_new/male_adult_construction_01_new.usd
-
-person_2:
-  prim_name: "Person_2"
-  translation: [2.51653, 7.80822, 0.0]
-  target: [0, 0, 0]
-  usd_path: People/Characters/male_adult_construction_03/male_adult_construction_03.usd
-
-person_3:
-  prim_name: "Person_3"
-  translation: [5.07179, 3.8561, 0.0]
-  target: [0, 0, 0]
-  usd_path: People/Characters/male_adult_construction_05_new/male_adult_construction_05_new.usd
-
-person_4:
-  prim_name: "Person_4"
-  translation: [-3.2015, 11.79695, 0.0]
-  target: [0, 0, 0]
-  usd_path: People/Characters/original_male_adult_construction_01/male_adult_construction_01.usd
-
-person_5:
-  prim_name: "Person_5"
-  translation: [-6.70566, 7.58019, 0.0]
-  target: [0, 0, 0]
-  usd_path: People/Characters/original_male_adult_construction_02/male_adult_construction_02.usd
-
-person_6:
-  prim_name: "Person_6"
-  translation: [-5.12784, 2.43409, 0.0]
-  target: [0, 0, 0]
-  usd_path: People/Characters/original_male_adult_construction_05/male_adult_construction_05.usd
-
-person_7:
-  prim_name: "Person_7"
-  translation: [-6.98476, -9.47249, 0.0]
-  target: [0, 0, 0]
-  usd_path: People/Characters/male_adult_construction_01_new/male_adult_construction_01_new.usd
-
-person_8:
-  prim_name: "Person_8"
-  translation: [-1.63744, -3.43285, 0.0]
-  target: [0, 0, 0]
-  usd_path: People/Characters/male_adult_construction_01_new/male_adult_construction_01_new.usd
-
-person_9:
-  prim_name: "Person_9"
-  translation: [6.15617, -8.3114, 0.0]
-  target: [0, 0, 0]
-  usd_path: People/Characters/original_male_adult_construction_05/male_adult_construction_05.usd
-
-person_10:
-  prim_name: "Person_10"
-  translation: [5.34416, -7.47814, 0.0]
-  target: [0, 0, 0]
-  usd_path: People/Characters/male_adult_construction_05_new/male_adult_construction_05_new.usd
diff --git a/isaaclab/extension/omni.isaac.viplanner/intern/carla_exploration/config.py b/isaaclab/extension/omni.isaac.viplanner/intern/carla_exploration/config.py
deleted file mode 100644
index c7cd380..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/intern/carla_exploration/config.py
+++ /dev/null
@@ -1,100 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# python
-import os
-from dataclasses import dataclass
-from typing import Optional, Tuple
-
-from omni.isaac.carla.config import DATA_DIR
-
-# isaac-lab
-from omni.isaac.lab.sensors.camera import PinholeCameraCfg
-
-
-@dataclass
-class CarlaExplorerConfig:
-    """Configuration for the CarlaMap class."""
-
-    # coverage parameters
-    points_per_m2: float = 0.5
-    obs_loss_threshold: float = 0.8
-    max_cam_recordings: Optional[int] = 10000  # if None, not limitation is applied
-    # indoor filter (for outdoor maps filter inside of buildings as traversable, for inside maps set to False)
-    indoor_filter: bool = True
-    carla_filter: Optional[str] = os.path.join(DATA_DIR, "town01", "area_filter_cfg.yml")
-    # nomoko model
-    nomoko_model: bool = False
-    # are limiter --> only select area within the defined prim names (e.g. "Road_SideWalk")
-    space_limiter: Optional[str] = "Road_Sidewalk"  # carla: "Road_Sidewalk"  nomoko None  park: MergedRoad05
-    # robot height
-    robot_height = 0.7  # m
-    # depth camera
-    camera_cfg_depth: PinholeCameraCfg = PinholeCameraCfg(
-        sensor_tick=0,
-        height=480,
-        width=848,
-        data_types=["distance_to_image_plane"],
-        usd_params=PinholeCameraCfg.UsdCameraCfg(
-            focal_length=1.93, clipping_range=(0.01, 1.0e5), horizontal_aperture=3.8
-        ),
-    )
-    camera_intrinsics_depth: Optional[Tuple[float]] = None
-    # ANYmal D/C realsense:         (423.54608, 0.0, 427.69815, 0.0, 423.54608, 240.17773, 0.0, 0.0, 1.0)
-    # RealSense D455:               (430.31607, 0.0, 428.28408, 0.0, 430.31607, 244.00695, 0.0, 0.0, 1.0)
-    # ANYmal D wide_angle_camera: 1.0 <-> ANYmal C realsense: 1.93 <-> RealSense D455: 1.93
-    camera_prim_depth: str = "/World/CameraSensor_depth"
-    # semantic camera
-    camera_cfg_sem: PinholeCameraCfg = PinholeCameraCfg(
-        sensor_tick=0,
-        height=720,  # 480,  # 1080
-        width=1280,  # 848,  # 1440
-        data_types=["rgb", "semantic_segmentation"],
-        usd_params=PinholeCameraCfg.UsdCameraCfg(
-            focal_length=1.93, clipping_range=(0.01, 1.0e5), horizontal_aperture=3.8
-        ),
-    )
-    # ANYmal D wide_angle_camera: (1440, 1080)  <-> ANYmal C realsense (848, 480) <-> RealSense D455 (1280, 720)
-    # ANYmal D wide_angle_camera: 1.93 <-> ANYmal C realsense: 1.93 <-> RealSense D455: 1.93
-    camera_intrinsics_sem: Optional[Tuple[float]] = None
-    # ANYmal D wide_angle_camera:   (575.60504, 0.0, 745.73121, 0.0, 578.56484, 519.52070, 0.0, 0.0, 1.0)
-    # ANYmal C realsense:           (423.54608, 0.0, 427.69815, 0.0, 423.54608, 240.17773, 0.0, 0.0, 1.0)
-    # RealSense D455:               (644.15496, 0.0, 639.53125, 0.0, 643.49212, 366.30880, 0.0, 0.0, 1.0)
-    camera_prim_sem: str = "/World/CameraSensor_sem"
-    x_angle_range: Tuple[float, float] = (-5, 5)  # downtilt angle of the camera in degree
-    y_angle_range: Tuple[float, float] = (
-        -2,
-        5,
-    )  # downtilt angle of the camera in degree  --> isaac convention, positive is downwards
-    # image suffix
-    depth_suffix = "_cam0"
-    sem_suffix = "_cam1"
-    # transformation from depth (src) to semantic camera (target)
-    tf_pos: tuple = (0.0, 0.0, 0.0)  # (translation in depth frame)
-    # ANYmal D: (-0.002, 0.025, 0.042)  <-> ANYmal C and RealSense D455: (0.0, 0.0, 0.0)
-    tf_quat: tuple = (0.0, 0.0, 0.0, 1.0)  # xyzw quaternion format (rotation in depth frame)
-    # ANYmal D: (0.001, 0.137, -0.000, 0.991)  <-> ANYmal C and RealSense D455: (0.0, 0.0, 0.0, 1.0)
-    tf_quat_convention: str = "roll-pitch-yaw"  # or "isaac"
-    # NOTE: if the quat follows the roll-pitch-yaw convention, i.e. x-forward, y-right, z-down, will be converted to the isaac convention
-    # high resolution depth for reconstruction (in city environment can otherwise lead to artifacts)
-    # will now also take the depth image of the rgb camera and use its depth images for reconstruction
-    high_res_depth: bool = False
-    # output_dir
-    output_root: Optional[str] = None  # if None, output dir is stored under root_dir
-    output_dir_name: str = "town01"
-    ros_p_mat: bool = True  # save intrinsic matrix in ros P-matrix format
-    depth_scale: float = 1000.0  # scale depth values before saving s.t. mm resolution can be achieved
-
-    # add more people to the scene
-    nb_more_people: Optional[int] = 1200  # if None, no people are added
-    random_seed: Optional[int] = 42  # if None, no seed is set
-
-    @property
-    def output_dir(self) -> str:
-        if self.output_root is not None:
-            return os.path.join(self.output_root, self.output_dir_name)
-        else:
-            return os.path.join(self.root_path, self.output_dir_name)
diff --git a/isaaclab/extension/omni.isaac.viplanner/intern/carla_exploration/explorer.py b/isaaclab/extension/omni.isaac.viplanner/intern/carla_exploration/explorer.py
deleted file mode 100644
index 40fb9e7..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/intern/carla_exploration/explorer.py
+++ /dev/null
@@ -1,692 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-import random
-import time
-from typing import Tuple
-
-# omniverse
-import carb
-import cv2
-
-# python
-import numpy as np
-import omni.isaac.debug_draw._debug_draw as omni_debug_draw
-import scipy.spatial.transform as tf
-import yaml
-
-# isaac-carla
-from omni.isaac.carla.configs import CarlaExplorerConfig, CarlaLoaderConfig
-from omni.isaac.core.objects import VisualCuboid
-
-# isaac-core
-from omni.isaac.core.simulation_context import SimulationContext
-
-# isaac-lab
-from omni.isaac.lab.sensors.camera import Camera
-from omni.isaac.lab.utils.math import convert_quat
-from omni.physx import get_physx_scene_query_interface
-from pxr import Gf, Usd, UsdGeom
-from scipy.spatial import KDTree
-from scipy.stats import qmc
-
-# isaac-anymal
-from viplanner.config.viplanner_sem_meta import VIPlannerSemMetaHandler
-
-from .loader import CarlaLoader
-
-
-class CarlaExplorer:
-    debug: bool = False
-
-    def __init__(self, cfg: CarlaExplorerConfig, cfg_load: CarlaLoaderConfig) -> None:
-        self._cfg = cfg
-        self._cfg_load = cfg_load
-
-        # check simulation context
-        if SimulationContext.instance():
-            self.sim: SimulationContext = SimulationContext.instance()
-        else:
-            carb.log_error("CarlaExplorer can only be loaded in a running simulationcontext!\nRun CarlaLoader!")
-
-        # Acquire draw interface
-        self.draw_interface = omni_debug_draw.acquire_debug_draw_interface()
-
-        # VIPlanner Semantic Meta Handler and mesh to sem class mapping
-        if self._cfg_load.sem_mesh_to_class_map is not None:
-            self.vip_sem_meta: VIPlannerSemMetaHandler = VIPlannerSemMetaHandler()
-            with open(self._cfg_load.sem_mesh_to_class_map) as f:
-                self.class_keywords = yaml.safe_load(f)
-
-        # init buffers
-        self.camera_positions: np.ndarray = np.array([])
-        self.cam_angles: np.ndarray = np.array([])
-        self.nbr_points: int = 0
-
-        # get camera
-        self.camera_depth: Camera = None
-        self.camera_semantic: Camera = None
-
-        return
-
-    def explore(self) -> None:
-        # init camera
-        self._camera_init()
-
-        # define camera positions and targets
-        self._get_cam_position()
-        self._get_cam_target()
-
-        # record rgb, depth, semantic segmentation at the camera posiitions
-        self._domain_recorder()
-
-        return
-
-    """ Exploration Helper Functions """
-
-    def _raycast_check(self, ray_origins: np.ndarray, ray_directions: np.ndarray, max_distance: float):
-        """
-        Check which object is hit by the raycast and give back the position, loss and class name of the hit object
-        """
-
-        start = time.time()
-        hits = [
-            get_physx_scene_query_interface().raycast_closest(
-                carb.Float3(ray_single), carb.Float3(ray_dir), max_distance
-            )
-            for ray_single, ray_dir in zip(ray_origins, ray_directions)
-        ]
-        end = time.time()
-        print("took ", end - start, "s for raycast the possible camera points")
-
-        # if point achieved a hit, get the hit point and the hit object
-        hit_pt_obj = [
-            (np.array(single_hit["position"]), single_hit["collision"].lower())
-            for single_hit in hits
-            if single_hit["hit"]
-        ]
-        hit_idx = [idx for idx, single_hit in enumerate(hits) if single_hit["hit"]]
-
-        # get offset
-        offset = np.array([0.0, 0.0, self._cfg.robot_height])
-
-        # get semantic class for each points and the corresponding cost
-        hit_class_name = np.zeros(len(hit_pt_obj), dtype=str)
-        hit_loss = np.zeros(len(hit_pt_obj))
-        hit_position = np.zeros((len(hit_pt_obj), 3))
-
-        if self._cfg_load.sem_mesh_to_class_map is not None:
-            for idx, single_hit in enumerate(hit_pt_obj):
-                success = False
-                for class_name, keywords in self.class_keywords.items():
-                    if any([keyword.lower() in single_hit[1] for keyword in keywords]):
-                        hit_class_name[idx] = class_name
-                        hit_loss[idx] = self.vip_sem_meta.class_loss[class_name]
-                        hit_position[idx] = single_hit[0] + offset  # add offset to get the center of the point
-                        success = True
-                        break
-                assert success, f"No class found for hit object: {single_hit}"
-        else:
-            hit_position = np.array([single_hit[0] + offset for single_hit in hit_pt_obj])
-
-        return hit_position, hit_loss, hit_class_name, hit_idx
-
-    def _get_cam_position(self) -> None:
-        """
-        Get suitable robot positions for exploration of the map. Robot positions are are dense cover of the map
-
-
-        Args:
-            points_per_m2 (float, optional): points per m^2. Defaults to 0.1.
-            obs_loss_threshold (float, optional): loss threshold for point to be suitable as robot position. Defaults to 0.6.   # choose s.t. no points on the terrain  TODO: change at some point
-            debug (bool, optional): debug mode. Defaults to True.
-        """
-        # get x-y-z coordinates limits where the explortion of all the mesh should take place
-        # for Carla, Town01_Opt is the explored map equal to the city surrounded by the road
-        # --> get min und max over the maximum extent of the Road_Sidewalk meshes
-        # IMPORTANT: y-up!!!
-        mesh_prims, mesh_prims_name = CarlaLoader.get_mesh_prims(self._cfg_load.prim_path + self._cfg_load.suffix)
-
-        if self._cfg.space_limiter:
-            # if space limiter is given, only consider the meshes with the space limiter in the name
-            mesh_idx = [
-                idx
-                for idx, prim_name in enumerate(mesh_prims_name)
-                if self._cfg.space_limiter.lower() in prim_name.lower()
-            ]
-        else:
-            # remove ground plane since has infinite extent
-            mesh_idx = [idx for idx, prim_name in enumerate(mesh_prims_name) if "groundplane" not in prim_name.lower()]
-        mesh_prims = [mesh_prims[idx] for idx in mesh_idx]
-
-        bbox_cache = UsdGeom.BBoxCache(Usd.TimeCode.Default(), ["default", "render"])
-        bbox = [self.compute_bbox_with_cache(bbox_cache, curr_prim) for curr_prim in mesh_prims]
-        prim_max = np.vstack([list(prim_range.GetMax()) for prim_range in bbox])
-        prim_min = np.vstack([list(prim_range.GetMin()) for prim_range in bbox])
-        x_min, y_min, z_min = np.min(prim_min, axis=0)
-        x_max, y_max, z_max = np.max(prim_max, axis=0)
-
-        max_area = (x_max - x_min) * (y_max - y_min)
-        max_distance = (z_max - z_min) + 10  # 10m extra
-        print("Exploration area: ", round(max_area / (1000) ** 2, 3), "km^2 or ", max_area, "m^2")
-
-        # init sampler as qmc
-        sampler = qmc.Halton(d=2, scramble=False)
-        # determine number of samples to dram
-        nbr_points = int(max_area * self._cfg.points_per_m2)
-        # get raw samples origins
-        points = sampler.random(nbr_points)
-        if self._cfg.nomoko_model:
-            points = qmc.scale(points, [y_min, x_min], [y_max, x_max])
-        else:
-            points = qmc.scale(points, [x_min, y_min], [x_max, y_max])
-
-        if self._cfg.indoor_filter:
-            heights = np.ones((nbr_points, 1)) * (z_max + 2 * self._cfg.robot_height)  # above the map highest point
-        else:
-            heights = np.ones((nbr_points, 1)) * (z_min + 2 * self._cfg.robot_height)  # above the map lowest point
-        ray_origins = np.hstack((points, heights))
-
-        # get ray directions in negative z direction
-        ray_directions = np.zeros((nbr_points, 3))
-        ray_directions[:, 2] = -1.0
-
-        # perform raycast check
-        hit_position, hit_loss, _, _ = self._raycast_check(ray_origins, ray_directions, max_distance)
-
-        # filter all indexes which are not in traversable terrain
-        camera_positions = hit_position[hit_loss < self._cfg.obs_loss_threshold]
-
-        # indoor filter
-        if self._cfg.indoor_filter:
-            # check on all 4 sites can only be performed with semantics
-            if self._cfg_load.sem_mesh_to_class_map is not None:
-                # filter all points within buildings by checking if hit above the point and if yes, if hit on all 4 sites of it
-                # rays always send from both sides since mesh only one-sided
-                # check if hit above the point
-                camera_positions_elevated = camera_positions + np.array([0.0, 0.0, 100])
-                _, hit_loss_low, _, hit_idx_low = self._raycast_check(
-                    camera_positions_elevated, ray_directions, max_distance=200
-                )
-                ray_directions[:, 2] = 1.0
-                _, hit_loss_high, _, hit_idx_high = self._raycast_check(
-                    camera_positions, ray_directions, max_distance=200
-                )
-
-                hit_idx_low = np.array(hit_idx_low)[hit_loss_low >= self._cfg.obs_loss_threshold]
-                hit_idx_high = np.array(hit_idx_high)[hit_loss_high >= self._cfg.obs_loss_threshold]
-                hit_idx = np.unique(np.hstack([hit_idx_low, hit_idx_high]))
-
-                if len(hit_idx) > 0:
-                    # check hit on sites of the point
-                    ray_directions[:, 2] = 0.0  # reset ray direction
-
-                    ray_directions[:, 0] = 1.0
-                    _, hit_loss, _, hit_idx_front = self._raycast_check(
-                        camera_positions[hit_idx], ray_directions[hit_idx], max_distance=10
-                    )
-                    traversable_front_pos = np.ones((len(hit_idx), 1), dtype=bool)
-                    traversable_front_pos[hit_idx_front, 0] = hit_loss < self._cfg.obs_loss_threshold
-                    ray_directions[:, 0] = -1.0
-                    _, hit_loss, _, hit_idx_front = self._raycast_check(
-                        camera_positions[hit_idx] + np.array([10, 0.0, 0.0]), ray_directions[hit_idx], max_distance=10
-                    )
-                    traversable_front_neg = np.ones((len(hit_idx), 1), dtype=bool)
-                    traversable_front_neg[hit_idx_front, 0] = hit_loss < self._cfg.obs_loss_threshold
-                    traversable_front = np.all(np.hstack([traversable_front_pos, traversable_front_neg]), axis=1)
-
-                    ray_directions[:, 0] = -1.0
-                    _, hit_loss, _, hit_idx_back = self._raycast_check(
-                        camera_positions[hit_idx], ray_directions[hit_idx], max_distance=10
-                    )
-                    traversable_back_neg = np.ones((len(hit_idx), 1), dtype=bool)
-                    traversable_back_neg[hit_idx_back, 0] = hit_loss < self._cfg.obs_loss_threshold
-                    ray_directions[:, 0] = 1.0
-                    _, hit_loss, _, hit_idx_back = self._raycast_check(
-                        camera_positions[hit_idx] - np.array([10, 0.0, 0.0]), ray_directions[hit_idx], max_distance=10
-                    )
-                    traversable_back_pos = np.ones((len(hit_idx), 1), dtype=bool)
-                    traversable_back_pos[hit_idx_back, 0] = hit_loss < self._cfg.obs_loss_threshold
-                    traversable_back = np.all(np.hstack([traversable_back_pos, traversable_back_neg]), axis=1)
-
-                    ray_directions[:, 0] = 0.0  # reset ray direction
-
-                    ray_directions[:, 1] = 1.0
-                    _, hit_loss, _, hit_idx_right = self._raycast_check(
-                        camera_positions[hit_idx], ray_directions[hit_idx], max_distance=10
-                    )
-                    traversable_right_pos = np.ones((len(hit_idx), 1), dtype=bool)
-                    traversable_right_pos[hit_idx_right, 0] = hit_loss < self._cfg.obs_loss_threshold
-                    ray_directions[:, 1] = -1.0
-                    _, hit_loss, _, hit_idx_right = self._raycast_check(
-                        camera_positions[hit_idx] + np.array([0.0, 10, 0.0]), ray_directions[hit_idx], max_distance=10
-                    )
-                    traversable_right_neg = np.ones((len(hit_idx), 1), dtype=bool)
-                    traversable_right_neg[hit_idx_right, 0] = hit_loss < self._cfg.obs_loss_threshold
-                    traversable_right = np.all(np.hstack([traversable_right_pos, traversable_right_neg]), axis=1)
-
-                    ray_directions[:, 1] = -1.0
-                    _, hit_loss, _, hit_idx_left = self._raycast_check(
-                        camera_positions[hit_idx], ray_directions[hit_idx], max_distance=10
-                    )
-                    traversable_left_neg = np.ones((len(hit_idx), 1), dtype=bool)
-                    traversable_left_neg[hit_idx_left, 0] = hit_loss < self._cfg.obs_loss_threshold
-                    ray_directions[:, 1] = -1.0
-                    _, hit_loss, _, hit_idx_left = self._raycast_check(
-                        camera_positions[hit_idx] - np.array([0.0, 10, 0.0]), ray_directions[hit_idx], max_distance=10
-                    )
-                    traversable_left_pos = np.ones((len(hit_idx), 1), dtype=bool)
-                    traversable_left_pos[hit_idx_left, 0] = hit_loss < self._cfg.obs_loss_threshold
-                    traversable_left = np.all(np.hstack([traversable_left_neg, traversable_left_pos]), axis=1)
-
-                    # filter points
-                    traversable_all = np.vstack(
-                        [traversable_front, traversable_back, traversable_right, traversable_left]
-                    ).all(axis=0)
-                    hit_idx_non_traverable = np.array(hit_idx)[~traversable_all]
-                else:
-                    hit_idx_non_traverable = []
-            else:
-                # semantics not available -> check compared to mean height
-                hit_idx_non_traverable = np.where(camera_positions[:, 2] > np.mean(camera_positions[:, 2]))[0]
-        else:
-            hit_idx_non_traverable = []
-
-        # update camera positions and nbr of points
-        if len(hit_idx_non_traverable) > 0:
-            self.camera_positions = np.delete(camera_positions, hit_idx_non_traverable, axis=0)
-        else:
-            self.camera_positions = camera_positions
-
-        # add more people in the scene
-        if self._cfg.nb_more_people is not None:
-            random.seed(self._cfg.random_seed)
-            pts_idx = random.sample(range(len(self.camera_positions)), self._cfg.nb_more_people)
-
-            if self._cfg_load.scale == 1.0:
-                scale_people = 100
-            else:
-                scale_people = 1
-
-            # add people and remove previous added offset
-            offset = np.array([0.0, 0.0, self._cfg.robot_height])
-            for idx in pts_idx:
-                CarlaLoader.insert_single_person(f"random_{idx}", self.camera_positions[idx] - offset, scale_people)
-
-            self.camera_positions = np.delete(self.camera_positions, pts_idx, axis=0)
-
-        if self._cfg.carla_filter:
-            # for CARLA filter large open spaces
-            # Extract the x and y coordinates from the odom poses
-            x_coords = self.camera_positions[:, 0]
-            y_coords = self.camera_positions[:, 1]
-
-            # load file
-
-            # Filter the point cloud based on the square coordinates
-            mask_area_1 = (y_coords >= 100.5) & (y_coords <= 325.5) & (x_coords >= 208.9) & (x_coords <= 317.8)
-            mask_area_2 = (y_coords >= 12.7) & (y_coords <= 80.6) & (x_coords >= 190.3) & (x_coords <= 315.8)
-            mask_area_3 = (y_coords >= 10.0) & (y_coords <= 80.0) & (x_coords >= 123.56) & (x_coords <= 139.37)
-
-            combined_mask = mask_area_1 | mask_area_2 | mask_area_3
-            points_free_space = ~combined_mask
-            self.camera_positions = self.camera_positions[points_free_space]
-
-        self.nbr_points = len(self.camera_positions)
-
-        # plot dense cover of the mesh
-        if self.debug:
-            self.sim.play()
-            self.draw_interface.draw_points(
-                self.camera_positions, [(1, 1, 1, 1)] * len(self.camera_positions), [5] * len(self.camera_positions)
-            )
-            self.draw_interface.draw_points(
-                camera_positions[hit_idx_non_traverable],
-                [(1.0, 0.5, 0, 1)] * len(camera_positions[hit_idx_non_traverable]),
-                [5] * len(camera_positions[hit_idx_non_traverable]),
-            )
-            for count in range(100000):
-                self.sim.step()
-            self.sim.pause()
-
-        return
-
-    def _construct_kdtree(self, num_neighbors: int = 50) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
-        # construct kdtree to find nearest neighbors of points
-        camera_position_unified_height = np.copy(self.camera_positions)
-        camera_position_unified_height[:, 2] = np.max(self.camera_positions[:, 2])
-
-        kdtree = KDTree(camera_position_unified_height)
-        _, nearest_neighbors_idx = kdtree.query(camera_position_unified_height, k=num_neighbors + 1, workers=-1)
-        # remove first neighbor as it is the point itself
-        nearest_neighbors_idx = nearest_neighbors_idx[:, 1:]
-
-        # define origin and neighbor points
-        origin_point = np.repeat(camera_position_unified_height, repeats=num_neighbors, axis=0)
-        neighbor_points = camera_position_unified_height[nearest_neighbors_idx, :].reshape(-1, 3)
-        distance = np.linalg.norm(origin_point - neighbor_points, axis=1)
-
-        # check for collision with raycasting
-        hit_position, _, _, hit_idx = self._raycast_check(
-            origin_point, neighbor_points - origin_point, np.max(distance)
-        )
-
-        # filter connections that collide with the environment
-        collision = np.zeros(len(origin_point), dtype=bool)
-        collision[hit_idx] = np.linalg.norm(hit_position - origin_point[hit_idx], axis=1) < distance[hit_idx]
-        collision = collision.reshape(-1, num_neighbors)
-        return nearest_neighbors_idx, collision, distance
-
-    def _get_cam_target(self) -> None:
-        """Camera orientation variation (similar to matterport variation)"""
-        # the variation around the up axis (z-axis) has to be picked in order to avoid that the camera faces a wall
-        # done by construction a graph of all sample points and pick the z angle in order to point at one of the neighbors of the node
-
-        # get nearest neighbors and check for collision
-        nearest_neighbors_idx, collision, _ = self._construct_kdtree()
-
-        # get nodes where all neighbors are in collision
-        all_collision_idx = np.all(collision, axis=1)
-
-        # select random neighbor that is not in collision
-        direction_neighbor_idx = np.hstack(
-            [
-                (collision_single_node is False).nonzero()[0][-1]
-                for collision_single_node in collision[~all_collision_idx, :]
-            ]
-        )
-        direction_neighbor_idx = np.vstack(
-            (np.arange(nearest_neighbors_idx.shape[0])[~all_collision_idx], direction_neighbor_idx)
-        ).T
-        selected_neighbor_idx = nearest_neighbors_idx[direction_neighbor_idx[:, 0], direction_neighbor_idx[:, 1]]
-
-        # get the z angle of the neighbor that is closest to the origin point
-        neighbor_direction = (
-            self.camera_positions[~all_collision_idx, :] - self.camera_positions[selected_neighbor_idx, :]
-        )
-        z_angles = np.rad2deg(np.arctan2(neighbor_direction[:, 1], neighbor_direction[:, 0]))
-
-        # filter points that have no neighbors that are not in collision and update number of points
-        self.camera_positions = self.camera_positions[~all_collision_idx, :]
-        self.nbr_points = self.camera_positions.shape[0]
-
-        # vary the rotation of the forward and horizontal axis (in camera frame) as a uniform distribution within the limits
-        x_angles = np.random.uniform(self._cfg.x_angle_range[0], self._cfg.x_angle_range[1], self.nbr_points)
-        y_angles = np.random.uniform(self._cfg.y_angle_range[0], self._cfg.y_angle_range[1], self.nbr_points)
-
-        self.cam_angles = np.hstack((x_angles.reshape(-1, 1), y_angles.reshape(-1, 1), z_angles.reshape(-1, 1)))
-        return
-
-    """ Camera and Image Creator """
-
-    def _camera_init(self) -> None:
-        # Setup camera sensor
-        self.camera_depth = Camera(cfg=self._cfg.camera_cfg_depth, device="cpu")
-        self.camera_depth.spawn(self._cfg.camera_prim_depth)
-        if self._cfg.camera_intrinsics_depth:
-            intrinsic_matrix = np.array(self._cfg.camera_intrinsics_depth).reshape(3, 3)
-            self.camera_depth.set_intrinsic_matrix(intrinsic_matrix)
-        self.camera_depth.initialize()
-
-        if self._cfg.high_res_depth:
-            self._cfg.camera_cfg_sem.data_types += ["distance_to_image_plane"]
-
-        self.camera_semantic = Camera(cfg=self._cfg.camera_cfg_sem, device="cpu")
-        self.camera_semantic.spawn(self._cfg.camera_prim_sem)
-        if self._cfg.camera_intrinsics_sem:
-            intrinsic_matrix = np.array(self._cfg.camera_intrinsics_sem).reshape(3, 3)
-            self.camera_semantic.set_intrinsic_matrix(intrinsic_matrix)
-        self.camera_semantic.initialize()
-        return
-
-    def _domain_recorder(self) -> None:
-        """
-        Will iterate over all camera positions and orientations while recording the resulting images in the different
-        domains (rgb, depth, semantic). The resulting images will be saved in the following folder structure:
-        NOTE: depth images are saved as png and the corresponding depth arrays are saved as npy files because the large
-        depths in CARLA exceed the int16 range of png images and lead to wrong depth values -> png only for visualization
-
-        - self._cfg.output_dir
-            - camera_extrinsic{depth_suffix}.txt  (format: x y z qx qy qz qw for depth camera)
-            - camera_extrinsic{sem_suffix}.txt  (format: x y z qx qy qz qw for semantic camera)
-            - intrinsics.txt (expects ROS CameraInfo format --> P-Matrix, both cameras)
-            - rgb
-                - xxxx{sem_suffix}.png  (images should be named with 4 digits, e.g. 0000.png, 0001.png, etc.)
-            - depth
-                - xxxx{depth_suffix}.png  (images should be named with 4 digits, e.g. 0000.png, 0001.png, etc.)
-                - xxxx{depth_suffix}.npy  (arrays should be named with 4 digits, e.g. 0000.npy, 0001.npy, etc.)
-            - semantics
-                - xxxx{sem_suffix}.png  (images should be named with 4 digits, e.g. 0000.png, 0001.png, etc.)
-
-        The depth and semantic suffix are for example "_depth" and "_sem" and can be set in the config file. They are
-        necessary to differentiate between the two cameras and their extrinsics. The suffix in the image naming is for
-        for compatibility with the matterport3d explorer.
-
-        If high resolution depth images are enabled, the following additional folder is added:
-            - depth_high_res
-                - xxxx{depth_suffix}.png  (images should be named with 4 digits, e.g. 0000.png, 0001.png, etc.)
-                - xxxx{depth_suffix}.npy  (arrays should be named with 4 digits, e.g. 0000.npy, 0001.npy, etc.)
-
-        """
-
-        # create save dirs for domains
-        os.makedirs(os.path.join(self._cfg.output_dir, "rgb"), exist_ok=True)
-        os.makedirs(os.path.join(self._cfg.output_dir, "depth"), exist_ok=True)
-        os.makedirs(os.path.join(self._cfg.output_dir, "semantics"), exist_ok=True)
-
-        if self._cfg.high_res_depth:
-            os.makedirs(os.path.join(self._cfg.output_dir, "depth_high_res"), exist_ok=True)
-
-        # save intrinsics
-        intrinsics = []
-        depth_intrinsics = (
-            np.array(self._cfg.camera_intrinsics_depth).reshape(3, 3)
-            if self._cfg.camera_intrinsics_depth
-            else self.camera_depth.data.intrinsic_matrix
-        )
-        sem_intrinsics = (
-            np.array(self._cfg.camera_intrinsics_sem).reshape(3, 3)
-            if self._cfg.camera_intrinsics_sem
-            else self.camera_semantic.data.intrinsic_matrix
-        )
-        for intrinsics_single in [depth_intrinsics, sem_intrinsics]:
-            if self._cfg.ros_p_mat:
-                p_mat = np.zeros((3, 4))
-                p_mat[:3, :3] = intrinsics_single
-                intrinsics.append(p_mat.flatten())
-            else:
-                intrinsics.append(intrinsics_single.flatten())
-        np.savetxt(os.path.join(self._cfg.output_dir, "intrinsics.txt"), np.vstack(intrinsics), delimiter=",")
-
-        # init pose buffers
-        sem_poses = np.zeros((self._cfg.max_cam_recordings, 7))
-        depth_poses = np.zeros((self._cfg.max_cam_recordings, 7))
-
-        # Play simulator
-        self.sim.play()
-
-        # Simulate for a few steps
-        # FIXME: This is a workaround to ensure that the textures are loaded.
-        #   Check "Known Issues" section in the documentation for more details.
-        for _ in range(14):
-            self.sim.render()
-
-        # matrix to transform opengl to isaac coordinate system
-        isaac_to_opengl_mat = tf.Rotation.from_euler("XYZ", [90, -90, 0], degrees=True).as_matrix()
-
-        for idx, sem_pos in enumerate(self.camera_positions):
-            start = time.time()
-
-            # Set semantic camera pose
-            sem_rot = self.cam_angles[idx].copy()
-            sem_rot = sem_rot.astype(np.float64)  # convert to double precision
-            sem_rot_mat = tf.Rotation.from_euler("xyz", sem_rot, degrees=True).as_matrix()
-            rot = sem_rot_mat @ isaac_to_opengl_mat
-            rot_quad = tf.Rotation.from_matrix(rot).as_quat()
-            self.camera_semantic._sensor_xform.set_world_pose(sem_pos, convert_quat(rot_quad, "wxyz"))
-
-            # get correct rotation from depth camera to semantic camera
-            if self._cfg.tf_quat_convention == "isaac":
-                cam_rot_sem_from_depth = tf.Rotation.from_quat(self._cfg.tf_quat).as_matrix()
-            elif self._cfg.tf_quat_convention == "roll-pitch-yaw":
-                cam_rot_sem_from_depth = tf.Rotation.from_quat(self._cfg.tf_quat).as_euler("XYZ", degrees=True)
-                cam_rot_sem_from_depth[[1, 2]] *= -1
-                cam_rot_sem_from_depth = tf.Rotation.from_euler("XYZ", cam_rot_sem_from_depth, degrees=True).as_matrix()
-            else:
-                raise ValueError(f"tf_quat_convention {self._cfg.tf_quat_convention} not supported")
-
-            # set depth camera pose
-            cam_rot_sem_from_depth = cam_rot_sem_from_depth.astype(np.float64)  # convert to double precision
-            depth_rot_mat = sem_rot_mat @ cam_rot_sem_from_depth.T  # get depth rotation in odom frame
-            depth_pos = sem_pos - depth_rot_mat @ self._cfg.tf_pos
-            rot = depth_rot_mat @ isaac_to_opengl_mat
-            rot_quad = tf.Rotation.from_matrix(rot).as_quat()
-            # set depth camera pose
-            self.camera_depth._sensor_xform.set_world_pose(depth_pos, convert_quat(rot_quad, "wxyz"))
-
-            # FIXME: This is a workaround to ensure that the textures are loaded.
-            #   Check "Known Issues" section in the documentation for more details.
-            for _ in range(5):
-                self.sim.render()
-
-            # Update camera data
-            self.camera_depth.update(dt=0.0)
-            self.camera_semantic.update(dt=0.0)
-
-            # save poses in Isaac convention and extrinsic format (xyz)
-            sem_poses[idx, :3] = sem_pos
-            sem_poses[idx, 3:] = tf.Rotation.from_matrix(sem_rot_mat).as_quat()
-            depth_poses[idx, :3] = depth_pos
-            depth_poses[idx, 3:] = tf.Rotation.from_matrix(depth_rot_mat).as_quat()
-
-            # Save images
-            # RGB
-            if "rgb" in self.camera_semantic.data.output:
-                cv2.imwrite(
-                    os.path.join(self._cfg.output_dir, "rgb", f"{idx}".zfill(4) + self._cfg.sem_suffix + ".png"),
-                    cv2.cvtColor(self.camera_semantic.data.output["rgb"], cv2.COLOR_RGB2BGR),
-                )
-            # DEPTH
-            np.save(
-                os.path.join(self._cfg.output_dir, "depth", f"{idx}".zfill(4) + self._cfg.depth_suffix + ".npy"),
-                self.camera_depth.data.output["distance_to_image_plane"] * self._cfg.depth_scale,
-            )
-            cv2.imwrite(
-                os.path.join(self._cfg.output_dir, "depth", f"{idx}".zfill(4) + self._cfg.depth_suffix + ".png"),
-                (self.camera_depth.data.output["distance_to_image_plane"] * self._cfg.depth_scale).astype(
-                    np.uint16
-                ),  # convert to meters
-            )
-            # High Resolution Depth
-            if self._cfg.high_res_depth:
-                np.save(
-                    os.path.join(
-                        self._cfg.output_dir, "depth_high_res", f"{idx}".zfill(4) + self._cfg.depth_suffix + ".npy"
-                    ),
-                    self.camera_semantic.data.output["distance_to_image_plane"] * self._cfg.depth_scale,
-                )
-                cv2.imwrite(
-                    os.path.join(
-                        self._cfg.output_dir, "depth_high_res", f"{idx}".zfill(4) + self._cfg.depth_suffix + ".png"
-                    ),
-                    (self.camera_semantic.data.output["distance_to_image_plane"] * self._cfg.depth_scale).astype(
-                        np.uint16
-                    ),  # convert to meters
-                )
-
-            # SEMANTICS
-            if self._cfg_load.sem_mesh_to_class_map:
-                class_color_with_unlabelled = self.vip_sem_meta.class_color
-                class_color_with_unlabelled["unlabelled"] = [0, 0, 0]
-
-                idToColor = np.array(
-                    [
-                        [
-                            int(k),
-                            self.vip_sem_meta.class_color[v["class"].lower()][0],
-                            self.vip_sem_meta.class_color[v["class"].lower()][1],
-                            self.vip_sem_meta.class_color[v["class"].lower()][2],
-                        ]
-                        for k, v in self.camera_semantic.data.output["semantic_segmentation"]["info"][
-                            "idToLabels"
-                        ].items()
-                    ]
-                )
-                idToColorArray = np.zeros((idToColor.max(axis=0)[0] + 1, 3))
-                idToColorArray[idToColor[:, 0]] = idToColor[:, 1:]
-                sem_img = idToColorArray[
-                    self.camera_semantic.data.output["semantic_segmentation"]["data"].reshape(-1)
-                ].reshape(self.camera_semantic.data.output["semantic_segmentation"]["data"].shape + (3,))
-                cv2.imwrite(
-                    os.path.join(self._cfg.output_dir, "semantics", f"{idx}".zfill(4) + self._cfg.sem_suffix + ".png"),
-                    cv2.cvtColor(sem_img.astype(np.uint8), cv2.COLOR_RGB2BGR),
-                )
-
-            # Print Info
-            duration = time.time() - start
-            print(f"Recording {idx + 1}/{self.nbr_points} ({(idx + 1) / self.nbr_points * 100:.2f}%) in {duration:.4f}")
-
-            # stop condition
-            if self._cfg.max_cam_recordings is not None and idx >= self._cfg.max_cam_recordings - 1:
-                break
-
-            if self.debug:
-                VisualCuboid(
-                    prim_path="/cube_example",  # The prim path of the cube in the USD stage
-                    name="waypoint",  # The unique name used to retrieve the object from the scene later on
-                    position=sem_pos
-                    + (
-                        tf.Rotation.from_euler("xyz", sem_rot, degrees=True).as_matrix()
-                        @ np.array([100, 0, 0]).reshape(-1, 1)
-                    ).reshape(
-                        -1
-                    ),  # Using the current stage units which is in meters by default.
-                    scale=np.array([15, 15, 15]),  # most arguments accept mainly numpy arrays.
-                    size=1.0,
-                    color=np.array([255, 0, 0]),  # RGB channels, going from 0-1
-                )
-
-                import matplotlib.pyplot as plt
-
-                _, axs = plt.subplots(1, 3, figsize=(15, 5))
-                axs[0].imshow(self.camera_semantic.data.output["rgb"])
-                axs[1].imshow(self.camera_depth.data.output["distance_to_image_plane"])
-                axs[2].imshow(self.camera_semantic.data.output["semantic_segmentation"]["data"])
-                plt.show()
-
-        np.savetxt(
-            os.path.join(self._cfg.output_dir, f"camera_extrinsic{self._cfg.sem_suffix}.txt"),
-            sem_poses[:idx],
-            delimiter=",",
-        )
-        np.savetxt(
-            os.path.join(self._cfg.output_dir, f"camera_extrinsic{self._cfg.depth_suffix}.txt"),
-            depth_poses[:idx],
-            delimiter=",",
-        )
-
-        return
-
-    @staticmethod
-    def compute_bbox_with_cache(cache: UsdGeom.BBoxCache, prim: Usd.Prim) -> Gf.Range3d:
-        """
-        Compute Bounding Box using ComputeWorldBound at UsdGeom.BBoxCache. More efficient if used multiple times.
-        See https://graphics.pixar.com/usd/dev/api/class_usd_geom_b_box_cache.html
-
-        Args:
-            cache: A cached, i.e. `UsdGeom.BBoxCache(Usd.TimeCode.Default(), ['default', 'render'])`
-            prim: A prim to compute the bounding box.
-        Returns:
-            A range (i.e. bounding box), see more at: https://graphics.pixar.com/usd/release/api/class_gf_range3d.html
-
-        """
-        bound = cache.ComputeWorldBound(prim)
-        bound_range = bound.ComputeAlignedBox()
-        return bound_range
-
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/__init__.py b/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/__init__.py
deleted file mode 100644
index 686f0ba..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/__init__.py
+++ /dev/null
@@ -1,40 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from .controller_cfg import LocomotionRlControllerCfg
-from .eval_cfg import ANYmalEvaluatorConfig
-from .ros_cfg import ROSPublisherCfg
-from .sensor_cfg import (
-    ANYMAL_C_CAMERA_SENSORS,
-    ANYMAL_C_LIDAR_SENSORS,
-    ANYMAL_D_CAMERA_SENSORS,
-    ANYMAL_D_LIDAR_SENSORS,
-    ANYMAL_FOLLOW,
-)
-from .vip_config import TwistControllerCfg, VIPlannerCfg
-from .walking_cfg import ANYmalCfg, SensorCfg, SimCfg, TerrainCfg, ViewerCfg
-
-__all__ = [
-    # configs
-    "ANYmalCfg",
-    "SimCfg",
-    "ViewerCfg",
-    "TerrainCfg",
-    "SensorCfg",
-    "LocomotionRlControllerCfg",
-    "ROSPublisherCfg",
-    "VIPlannerCfg",
-    "TwistControllerCfg",
-    "ANYmalEvaluatorConfig",
-    # Perception Sensor Settings
-    "ANYMAL_D_CAMERA_SENSORS",
-    "ANYMAL_D_LIDAR_SENSORS",
-    "ANYMAL_C_CAMERA_SENSORS",
-    "ANYMAL_C_LIDAR_SENSORS",
-    "ANYMAL_FOLLOW",
-]
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/eval_cfg.py b/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/eval_cfg.py
deleted file mode 100644
index 855b3d3..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/eval_cfg.py
+++ /dev/null
@@ -1,57 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# python
-import os
-from dataclasses import dataclass, field
-from typing import List, Optional
-
-# lab-assets
-from omni.isaac.assets import ASSETS_RESOURCES_DIR
-
-
-@dataclass
-class ANYmalEvaluatorConfig:
-    # termination conditions
-    max_time = 60  # seconds
-    max_remain_time = 5  # seconds
-
-    # cost map file
-    cost_map_dir: Optional[str] = None  # "/home/pascal/viplanner/imperative_learning/data/cost_maps"
-    cost_map_name: Optional[str] = None  # "2n8kARJN3HM_cost_map_long_2"
-
-    # use previous results or previous generated waypoints
-    use_prev_results: bool = True
-    use_existing_explored_waypoints: bool = True
-    handcrafted_waypoint_file: Optional[str] = None  # "2n8kARJN3HM_waypoints_long"  # _2
-    waypoint_dir: Optional[str] = "/home/pascal/viplanner/imperative_learning/data/waypoints"
-    # NOTE: can either load waypoint generated by the waypoint extension (define file under handcrafted_waypoint_file)
-    #       or load previously explored waypoints (define file under handcrafted_waypoint_file), if neither env will
-    #       be explored
-    repeat_waypoints: Optional[int] = 50  # number of times to repeat the waypoints, create distribution of results
-
-    # waypoint exploration parameters
-    num_pairs: int = 500  # number of start-goal pairs to explore
-    min_goal_dist: int = 5  # meters
-    max_goal_dist: int = 15  # meters
-    num_connections: int = 3  # number of connections when building the graph of all samples
-    seed: int = 1  # random seed for collection of start-goal points
-
-    # multi model
-    multi_model: bool = False
-    models: List[str] = field(default_factory=list)
-    save_dir: str = os.path.join(ASSETS_RESOURCES_DIR, "vip_models")
-
-    # intermediate result saving period
-    save_period: int = 10  # waypoints
-
-    @property
-    def waypoint_file(self):
-        assert all([self.waypoint_dir, self.handcrafted_waypoint_file]), "Waypoint file not specified"
-        return os.path.join(self.waypoint_dir, f"{self.handcrafted_waypoint_file}.json")
-
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/evaluator.py b/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/evaluator.py
deleted file mode 100644
index a20cd41..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/evaluator.py
+++ /dev/null
@@ -1,817 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# python
-import asyncio
-import datetime
-import json
-import os
-import pickle
-import random
-import shutil
-from abc import abstractmethod
-from typing import List, Tuple
-
-# omni
-import carb
-import cv2
-import networkx as nx
-import numpy as np
-
-# isaac-debug
-import omni.isaac.debug_draw._debug_draw as omni_debug_draw
-import scipy.spatial.transform as tf
-import torch
-
-# isaac-anymal
-from omni.isaac.anymal.config import (
-    ANYMAL_FOLLOW,
-    ANYmalCfg,
-    ANYmalEvaluatorConfig,
-    VIPlannerCfg,
-)
-from omni.isaac.anymal.robot import ANYmal
-from omni.isaac.anymal.tasks import VIPlannerANYmal
-from omni.isaac.anymal.utils.camera_utils import get_cam_pose
-from omni.isaac.anymal.utils.gif_utils import create_gif
-
-# isaac-core
-from omni.isaac.core.simulation_context import SimulationContext
-
-# isaac-lab
-from omni.isaac.lab.utils.math import convert_quat, quat_mul
-from pxr import Usd
-
-# viplanner
-from viplanner.utils.eval_utils import BaseEvaluator
-
-
-class ANYmallabEvaluator(BaseEvaluator):
-    def __init__(
-        self,
-        cfg: ANYmalEvaluatorConfig,
-        cfg_anymal: ANYmalCfg,
-        cfg_planner: VIPlannerCfg,
-    ) -> None:
-        # get args
-        self._cfg = cfg
-        self._cfg_anymal = cfg_anymal
-        self._cfg_planner = cfg_planner
-        # change flag
-        if self._cfg_anymal.viewer.debug_vis:
-            print(
-                "WARNING: Debug visualization will be switched off since markers do not have semantic label and lead to errors."
-            )
-            self._cfg_anymal.viewer.debug_vis = False
-
-        # super init
-        super().__init__(
-            distance_tolerance=self._cfg_planner.conv_dist,
-            obs_loss_threshold=self._cfg_planner.obs_loss_threshold,
-            cost_map_dir=self._cfg.cost_map_dir,
-            cost_map_name=self._cfg.cost_map_name,
-        )
-
-        # Acquire draw interface
-        self.draw_interface = omni_debug_draw.acquire_debug_draw_interface()
-
-        # init ANYmal with corresponding agent
-        self._anymal: ANYmal = None
-        self._agent: VIPlannerANYmal = None
-
-        # get simulation context
-        self.sim: SimulationContext = None
-
-        # flags
-        self.use_waypoint_file: bool = True if self._cfg.waypoint_dir and self._cfg.handcrafted_waypoint_file else False
-
-        return
-
-    @abstractmethod
-    def load_scene(self) -> None:
-        """Load scene."""
-        raise NotImplementedError
-
-    @abstractmethod
-    def explore_env(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
-        """Setup explorer."""
-        raise NotImplementedError
-
-    @abstractmethod
-    def post_setup(self) -> None:
-        """Post step."""
-        pass
-
-    @abstractmethod
-    def get_env_name(self) -> str:
-        """Get environment name."""
-        raise NotImplementedError
-
-    ##
-    # Buffers
-    ##
-
-    def create_buffers(self) -> None:
-        # create standard buffers
-        super().create_buffers()
-        # add additional buffers
-        self.goal_reached: np.ndarray = np.zeros(self._nbr_paths, dtype=bool)
-        self.goal_within_fov: np.ndarray = np.ones(self._nbr_paths, dtype=bool)
-        self.base_collision: np.ndarray = np.zeros(self._nbr_paths, dtype=bool)
-        self.knee_collision: np.ndarray = np.zeros(self._nbr_paths, dtype=bool)
-        self.walking_time: np.ndarray = np.ones(self._nbr_paths) * self._cfg.max_time
-        self.skip_waypoint: np.ndarray = np.zeros(self._nbr_paths, dtype=bool)
-
-    ##
-    # Run Simulation
-    ##
-
-    def run_single(self, show_plot: bool = True, repeat_idx: str = "") -> None:
-        """RUN SINGLE MODEL"""
-        eval_dir = os.path.join(self._cfg_planner.model_dir, f"eval_{self.get_env_name()}", repeat_idx)
-        os.makedirs(eval_dir, exist_ok=True)
-
-        # check if results already exist
-        if self._cfg.use_prev_results:
-            print(f"[INFO] Using previous results from {eval_dir}!")
-            success_use_prev_results, start_idx = self.load_eval_arrays(eval_dir=eval_dir)
-        else:
-            success_use_prev_results = False
-            start_idx = 0
-
-        if not success_use_prev_results:
-            self.run(eval_dir, start_idx)
-            # create eval stats and plots
-            self.save_eval_arrays(eval_dir=eval_dir)
-            self._filter_statistics()
-            self.eval_statistics()
-            self.save_eval_results()
-
-            self.plt_single_model(eval_dir=eval_dir, show=show_plot)
-        else:
-            self.eval_statistics()
-
-        return
-
-    def run_repeat(self) -> None:
-        # adjust configs
-        self._cfg_anymal.rec_path = True
-        self._cfg_anymal.rec_sensor = False
-        self._cfg_anymal.follow_camera = True
-
-        start_point = self.waypoints["start"]
-
-        assert (
-            self._cfg.repeat_waypoints
-        ), "To repeat waypoints, please specify the repeat_waypoints flag in the config!"
-
-        if self._cfg.use_prev_results:
-            repeat_indexes = []
-            for file in os.listdir(os.path.join(self._cfg_planner.model_dir, f"eval_{self.get_env_name()}")):
-                if file.startswith("repeat_") and file[len("repeat_") :].isdigit():
-                    index = int(file[len("repeat_") :])
-                    repeat_indexes.append(index)
-            start_idx = max(repeat_indexes) + 1
-        else:
-            start_idx = 0
-
-        for repeat_idx in range(start_idx, self._cfg.repeat_waypoints):
-            # reset anymal to start position
-            self._cfg_anymal.translation_x = start_point[0]
-            self._cfg_anymal.translation_y = start_point[1]
-            self._cfg_anymal.translation_z = 1.0  # start_point[2]
-
-            self.sim.play()
-            # self._anymal.robot._process_info_cfg()
-            # reset robot
-            self._anymal._reset_robot()
-            # reset planner
-            # self._agent.reset()
-            self.sim.pause()
-
-            self.run_single(show_plot=False, repeat_idx=f"repeat_{repeat_idx}")
-            self.reset()
-        return
-
-    def run_multi(self, show_single_plot: bool = False) -> None:
-        """RUN MULTI MODEL"""
-        print(
-            f"[INFO] Running multi model evaluation with the models defined in ANYmalEvaluatorConfig! \n"
-            f"The model defined under VIPlannerCfg will not be used! The used models are: \n"
-            f"{self._cfg.models}"
-        )
-
-        length_path_list = []
-        length_goal_list = []
-        goal_distances_list = []
-        obs_loss_list = []
-
-        for model_dir in self._cfg.models:
-            print(f"[INFO] Running model {model_dir}!")
-            # switch model and update it in the config
-            self._agent.planner.switch_model(model_dir)
-            self._cfg_planner.model_dir = model_dir
-            # run for new model
-            self.run_single(show_plot=show_single_plot)
-            length_path_list.append(self.length_path)
-            length_goal_list.append(self.length_goal)
-            goal_distances_list.append(self.goal_distances)
-            obs_loss_list.append(self.loss_obstacles)
-
-            self.reset()
-
-        self.plt_comparison(
-            length_goal_list,
-            length_path_list,
-            goal_distances_list,
-            self._cfg.models,
-            self._cfg.save_dir,
-            obs_loss_list,
-            model_names=["VIPlanner", "iPlanner"],
-        )
-        return
-
-    def run(self, eval_dir: str, start_idx: int = 0) -> None:
-        # init camera buffers by rendering a first time (otherwise will stop simulation)
-        self.sim.play()
-        self._anymal.sensors_camera[self._agent.planner.cam_path["rgb"]].update(dt=0.0)
-        self._anymal.sensors_camera[self._agent.planner.cam_path["depth"]].update(dt=0.0)
-        self.sim.pause()
-
-        # iterate over all waypoints
-        for idx in range(self._nbr_paths - start_idx):
-            idx_curr = idx + start_idx
-
-            if self.use_waypoint_file:
-                next_goalpoint = self.waypoints["waypoints"][idx_curr]
-            else:
-                next_goalpoint = list(self.waypoints[idx_curr].values())[0]
-
-            # set new goal
-            self._agent.cube.set_world_pose(next_goalpoint)
-            # reset counter and flags
-            counter = 0
-            start_time = self.sim.current_time
-            past_robot_position = self._anymal.robot.data.root_pos_w.numpy()[0, :2].copy()
-            robot_position_time = self.sim.current_time
-            self.length_goal[idx_curr] = (
-                np.linalg.norm(past_robot_position - next_goalpoint[:2]) - self._agent.planner._cfg_vip.conv_dist
-            )
-
-            if self._cfg_anymal.follow_camera:
-                cam_follow_save_path = os.path.join(
-                    eval_dir,
-                    "eval_video",
-                    self._cfg.handcrafted_waypoint_file + f"_waypoint{idx_curr}_of_{self.nbr_paths}"
-                    if self.use_waypoint_file
-                    else f"random_seed{self._cfg.seed}_pairs{self._cfg.num_pairs}_waypoint{idx_curr}_of_{self._nbr_paths}",
-                )
-                os.makedirs(cam_follow_save_path, exist_ok=True)
-            if self._cfg_anymal.rec_sensor:
-                sensor_save_paths = []
-                for sensor in self._anymal.sensors_camera.keys():
-                    sensor_save_path = os.path.join(
-                        eval_dir,
-                        sensor,
-                        self._cfg.handcrafted_waypoint_file + f"_waypoint{idx_curr}_of_{self.nbr_paths}"
-                        if self.use_waypoint_file
-                        else f"random_seed{self._cfg.seed}_pairs{self._cfg.num_pairs}_waypoint{idx_curr}_of_{self._nbr_paths}",
-                    )
-                    os.makedirs(sensor_save_path, exist_ok=True)
-                    sensor_save_paths.append(sensor_save_path)
-
-            self.sim.play()
-
-            base_net_contact_force = self._anymal.base_contact.get_net_contact_forces(
-                clone=False, dt=self.sim.get_physics_dt()
-            )
-            if (base_net_contact_force > 0.0).any():
-                print(f"Waypoint {idx_curr}:\t Start Position Base collides, will discard waypoint!")
-                self.base_collision[idx_curr] = True
-                self.skip_waypoint[idx_curr] = True
-                self.sim_reset(idx_curr, next_goalpoint=next_goalpoint, eval_dir=eval_dir)
-                continue
-            knee_net_contact_force = self._anymal.knee_contact.get_net_contact_forces(
-                clone=False, dt=self.sim.get_physics_dt()
-            )
-            knee_net_contact_force = knee_net_contact_force.view(-1, 4, 3)
-            if (knee_net_contact_force > 0.0).any():
-                print(f"Waypoint {idx_curr}:\t Start Position Knee collides, will discard waypoint!")
-                self.knee_collision[idx_curr] = True
-                self.skip_waypoint[idx_curr] = True
-                self.sim_reset(idx_curr, next_goalpoint=next_goalpoint, eval_dir=eval_dir)
-                continue
-
-            # collect path
-            path = []
-
-            while True:
-                self.sim.step()
-                counter += 1
-
-                if self._agent.twist.goal_reached:
-                    self.goal_reached[idx_curr] = True
-                    self.walking_time[idx_curr] = self.sim.current_time - start_time
-                    print(
-                        f"Waypoint {idx_curr}:\t Goal reached within {self.walking_time[idx_curr]}s ({counter} steps)."
-                    )
-                    break
-
-                # check if robot is stuck and get path length
-                if (
-                    self._anymal.robot.data.root_pos_w.numpy()[0, :2].round(decimals=1)
-                    == past_robot_position.round(decimals=1)
-                ).all():
-                    if self.sim.current_time - robot_position_time > self._cfg.max_remain_time:
-                        print(f"Waypoint {idx_curr}:\t Robot is stuck!")
-                        break
-                else:
-                    self.length_path[idx_curr] += np.linalg.norm(
-                        self._anymal.robot.data.root_pos_w.numpy()[0, :2] - past_robot_position
-                    )
-                    past_robot_position = self._anymal.robot.data.root_pos_w.numpy()[0, :2].copy()
-                    path.append(self._anymal.sensors_camera[self._agent.planner.cam_path["rgb"]]._compute_ros_pose()[0])
-                    robot_position_time = self.sim.current_time
-
-                # contact forces
-                base_net_contact_force = self._anymal.base_contact.get_net_contact_forces(
-                    clone=False, dt=self.sim.get_physics_dt()
-                )
-                if (base_net_contact_force > 0.0).any() and not self.base_collision[idx_curr]:
-                    self.base_collision[idx_curr] = True
-                knee_net_contact_force = self._anymal.knee_contact.get_net_contact_forces(
-                    clone=False, dt=self.sim.get_physics_dt()
-                )
-                knee_net_contact_force = knee_net_contact_force.view(-1, 4, 3)
-                if (knee_net_contact_force > 0.0).any() and not self.knee_collision[idx_curr]:
-                    self.knee_collision[idx_curr] = True
-                # feet_net_contact_force = self._anymal.foot_contact.get_net_contact_forces(clone=False, dt=self.sim.get_physics_dt())
-                # feet_net_contact_force = feet_net_contact_force.view(-1, 4, 3)
-
-                # check for max time
-                if (self.sim.current_time - start_time) >= self._cfg.max_time:
-                    print(f"Waypoint {idx_curr}:\t Goal NOT reached.")
-                    break
-
-                # eval video
-                if self._cfg_anymal.follow_camera and counter % self._cfg_anymal.rec_frequency == 0:
-                    # set to constant height and orientation
-                    pos = (
-                        tf.Rotation.from_quat(
-                            convert_quat(self._anymal.robot.data.root_quat_w.clone().numpy()[0], "xyzw")
-                        ).as_matrix()
-                        @ np.asarray(ANYMAL_FOLLOW.pos)
-                        + self._anymal.robot.data.root_pos_w.numpy()[0]
-                    )
-                    pos[2] = 1.7  # constant height
-                    target = self._anymal.robot.data.root_pos_w.clone().numpy()[0]
-                    extra_world_frame = tf.Rotation.from_quat(
-                        convert_quat(self._anymal.robot.data.root_quat_w.clone().numpy()[0], "xyzw")
-                    ).as_matrix() @ np.array([1, 0, 0])
-                    target += extra_world_frame
-                    target[2] = 0.7  # constant height
-                    self._anymal.follow_cam.set_world_pose_from_view(
-                        pos,
-                        target,
-                    )
-                    self._anymal.follow_cam.update(self._cfg_anymal.sim.dt)
-                    # write image
-                    cv2.imwrite(
-                        os.path.join(cam_follow_save_path, "step" + f"{counter}".zfill(5) + ".png"),
-                        cv2.cvtColor(self._anymal.follow_cam.data.output["rgb"], cv2.COLOR_BGR2RGB),
-                    )
-                if self._cfg_anymal.rec_sensor and counter % self._cfg_anymal.rec_frequency == 0:
-                    for idx, sensor in enumerate(self._anymal.sensors_camera.values()):
-                        for data_type, data_array in sensor.data.output.items():
-                            if data_array is None:
-                                continue
-
-                            if data_type == "rgb" or data_type == "semantic_segmentation":
-                                if isinstance(data_array, dict):
-                                    # collect image and transfer it to viplanner color space
-                                    sem_image = data_array["data"]
-                                    sem_idToLabels = data_array["info"]["idToLabels"]
-                                    data_array = self._agent.planner.sem_color_transfer(sem_image, sem_idToLabels)
-
-                                cv2.imwrite(
-                                    os.path.join(
-                                        sensor_save_paths[idx], data_type + "_step" + f"{counter}".zfill(5) + ".png"
-                                    ),
-                                    cv2.cvtColor(data_array.astype(np.uint8), cv2.COLOR_BGR2RGB),
-                                )
-                            elif data_type == "distance_to_image_plane":
-                                if isinstance(self._agent.planner.planner.train_config.data_cfg, list):
-                                    depth_scale = self._agent.planner.planner.train_config.data_cfg[0].depth_scale
-                                else:
-                                    depth_scale = self._agent.planner.planner.train_config.data_cfg.depth_scale
-
-                                cv2.imwrite(
-                                    os.path.join(
-                                        sensor_save_paths[idx], data_type + "_step" + f"{counter}".zfill(5) + ".png"
-                                    ),
-                                    (data_array * depth_scale).astype(np.uint16),
-                                )
-
-                # add current position to draw interface to show robot path
-                if counter % 100 == 0:
-                    self.draw_interface.draw_points(
-                        self._anymal.robot.data.root_pos_w.tolist(), [(1, 1, 1, 1)], [5]  # white
-                    )
-
-            # pause and reset anymal, planner, ...
-            self.sim.pause()
-            self.draw_interface.clear_points()
-            self.sim_reset(idx_curr, next_goalpoint, eval_dir, path)
-
-            # save intermediate results
-            if idx_curr % self._cfg.save_period == 0:
-                os.makedirs(os.path.join(eval_dir, f"pre_{idx_curr}"), exist_ok=True)
-                self.save_eval_arrays(eval_dir=os.path.join(eval_dir, f"pre_{idx_curr}"), suffix=f"_{idx_curr}")
-                if os.path.exists(os.path.join(eval_dir, f"pre_{int(idx_curr-self._cfg.save_period)}")):
-                    shutil.rmtree(os.path.join(eval_dir, f"pre_{int(idx_curr-self._cfg.save_period)}"))
-
-            # save git if cam follower is activated
-            # if self._cfg_anymal.follow_camera and counter > self._cfg_anymal.rec_frequency:
-            #     try:
-            #         create_gif(
-            #             cam_follow_save_path,
-            #             gif_name=f"waypoint{idx_curr}",
-            #             # speedup by factor of self._cfg_anymal.follow_camera_frequency
-            #             duration=(self.sim.current_time - self._agent.planner.start_time) / counter,
-            #         )
-            #     except:
-            #         carb.log_warn("Could not create gif!")
-        return
-
-    ##
-    # Sim Setup and Reset
-    ##
-
-    def setup(self) -> None:
-        # load scene to init simulation context
-        self.load_scene()
-        # get the simulationContext
-        self.sim: SimulationContext = SimulationContext().instance()
-        # load waypoints
-        self.setup_waypoints()
-        # create buffers
-        self.create_buffers()
-        # setup anymal
-        self.anymal_setup()
-        # post setup script
-        self.post_setup()
-        return
-
-    def anymal_setup(self) -> None:
-        print("Initializing ANYmal and setup callbacks ...")
-        # init anymal
-        self._anymal = ANYmal(self._cfg_anymal)
-        self._anymal.setup_sync()
-        # init anymal agent
-        self._agent_setup()
-        print("ANYmal initialized.")
-        return
-
-    def _agent_setup(self) -> None:
-        self._agent = VIPlannerANYmal(
-            cfg=self._cfg_anymal,
-            camera_sensors=self._anymal.sensors_camera,
-            robot=self._anymal.robot,
-            height_scanner=self._anymal.height_scanner,
-            ros_controller=False,
-            planner_cfg=self._cfg_planner,
-        )
-
-        self._agent.planner.set_planner_callback()
-        asyncio.ensure_future(self._agent.set_walk_callback())
-
-        # prevent local goal to be visible --> messes up semantic and depth images
-        self._agent.cube.set_visibility(False)
-        return
-
-    def _get_rot_to_point(self, start: list, end: list) -> tuple:
-        # set the initial rotation to point to the first waypoint
-        angle = np.arctan2(end[1] - start[1], end[0] - start[0])
-        rot_quat = tf.Rotation.from_euler("z", angle, degrees=False).as_quat()
-        return tuple(convert_quat(rot_quat, "wxyz").tolist())
-
-    def sim_reset(self, idx: int, next_goalpoint: np.array, eval_dir: str, path: List[torch.Tensor] = []) -> None:
-        # save distance depth camera to goal and if goal was within fov at the starting position
-        # NOTE: base position cannot be taken since the path is determined from the camera position which has an offset
-        cam_pos, _ = get_cam_pose(self._anymal.sensors_camera[self._agent.planner.cam_path["depth"]]._sensor_prim)
-        self.goal_within_fov[idx] = not self._agent.planner.goal_outside_fov
-        self.goal_distances[idx] = max(
-            [np.linalg.norm(next_goalpoint[:2] - cam_pos[:2]) - self._agent.planner._cfg_vip.conv_dist, 0.0]
-        )
-
-        if len(path) > 0:
-            straight_distance = np.linalg.norm(path[-1][:2] - path[0][:2])
-            self.path_extension[idx] = (self.length_path[idx] - straight_distance) / straight_distance
-
-            if self._use_cost_map:
-                self.loss_obstacles[idx] = self._get_cost_map_loss(np.vstack(path))
-
-            if self._cfg_anymal.rec_path:
-                np.save(os.path.join(eval_dir, f"waypoint{idx}_path.npy"), np.vstack(path))
-
-        # reset the robot to new start position if necessary
-        if not (self.goal_reached[idx] and self.use_waypoint_file) and idx + 1 < self._nbr_paths:
-            # move anymal to new start position
-            if self.use_waypoint_file:
-                next_goalpoint[2] = 1.0
-                self._anymal.robot.cfg.init_state.pos = tuple(next_goalpoint)
-                self._anymal.robot.cfg.init_state.rot = self._get_rot_to_point(
-                    next_goalpoint, self.waypoints["waypoints"][idx + 1]
-                )
-            else:
-                self._anymal.robot.cfg.init_state.pos = list(self.waypoints[idx + 1].keys())[0]
-                self._anymal.robot.cfg.init_state.rot = self._get_rot_to_point(
-                    np.array(list(self.waypoints[idx + 1].keys())[0]), list(self.waypoints[idx + 1].values())[0]
-                )
-            self._anymal.robot._process_info_cfg()
-            # reset robot
-            self._anymal._reset_robot()
-            # reset planner
-            self._agent.reset()
-
-        # reset pbar
-        self._agent.pbar.close()
-        self._agent._setup_pbar()
-        return
-
-    ##
-    # Eval Stats
-    ##
-    def _filter_statistics(self) -> None:
-        # remove skipped waypoints
-        print(f"Waypoint skipped {sum(self.skip_waypoint)} due to knee or base collision in start position.")
-        self.goal_reached = self.goal_reached[self.skip_waypoint == False]
-        self.goal_within_fov = self.goal_within_fov[self.skip_waypoint == False]
-        self.base_collision = self.base_collision[self.skip_waypoint == False]
-        self.knee_collision = self.knee_collision[self.skip_waypoint == False]
-        self.walking_time = self.walking_time[self.skip_waypoint == False]
-        self.goal_distances = self.goal_distances[self.skip_waypoint == False]
-        self.length_goal = self.length_goal[self.skip_waypoint == False]
-        self.length_path = self.length_path[self.skip_waypoint == False]
-        self.loss_obstacles = self.loss_obstacles[self.skip_waypoint == False]
-        self.path_extension = self.path_extension[self.skip_waypoint == False]
-        return
-
-    def eval_statistics(self) -> None:
-        # perform general eval stats
-        super().eval_statistics()
-
-        # further eval stats
-        within_fov_rate = sum(self.goal_within_fov) / len(self.goal_within_fov)
-        avg_time = (
-            sum(self.walking_time[self.goal_reached]) / len(self.walking_time[self.goal_reached])
-            if len(self.walking_time[self.goal_reached]) > 0
-            else np.inf
-        )
-        base_collision_rate = sum(self.base_collision) / len(self.base_collision)
-        knee_collision_rate = sum(self.knee_collision) / len(self.knee_collision)
-
-        print(
-            f"Avg time (success):           {avg_time} \n"
-            f"Goal within FOV:              {within_fov_rate} \n"
-            f"Base collision rate:          {base_collision_rate} \n"
-            f"Knee collision rate:          {knee_collision_rate}"
-        )
-
-        # extend eval stats
-        self.eval_stats["within_fov_rate"] = within_fov_rate
-        self.eval_stats["avg_time"] = avg_time
-        self.eval_stats["base_collision_rate"] = base_collision_rate
-        self.eval_stats["knee_collision_rate"] = knee_collision_rate
-
-        return
-
-    def save_eval_results(self) -> None:
-        save_name = self._cfg.handcrafted_waypoint_file if self.use_waypoint_file else self.get_env_name()
-        return super().save_eval_results(self._agent._planner_cfg.model_dir, save_name)
-
-    def get_save_prefix(self) -> str:
-        return (
-            self._cfg.handcrafted_waypoint_file
-            if self.use_waypoint_file
-            else self.get_env_name() + f"_seed{self._cfg.seed}_pairs{self._cfg.num_pairs}"
-        )
-
-    def save_eval_arrays(self, eval_dir: str, suffix: str = "") -> None:
-        subdirectories = [name for name in os.listdir(eval_dir) if os.path.isdir(os.path.join(eval_dir, name))]
-        pre_directories = [subdir for subdir in subdirectories if "pre" in subdir] if len(subdirectories) > 0 else []
-        if len(pre_directories) > 0:
-            [shutil.rmtree(os.path.join(eval_dir, pre)) for pre in pre_directories]
-
-        prefix: str = self.get_save_prefix()
-        np.save(os.path.join(eval_dir, prefix + f"_goal_reached{suffix}.npy"), self.goal_reached)
-        np.save(os.path.join(eval_dir, prefix + f"_goal_within_fov{suffix}.npy"), self.goal_within_fov)
-        np.save(os.path.join(eval_dir, prefix + f"_base_collision{suffix}.npy"), self.base_collision)
-        np.save(os.path.join(eval_dir, prefix + f"_knee_collision{suffix}.npy"), self.knee_collision)
-        np.save(os.path.join(eval_dir, prefix + f"_walking_time{suffix}.npy"), self.walking_time)
-        np.save(os.path.join(eval_dir, prefix + f"_goal_distances{suffix}.npy"), self.goal_distances)
-        np.save(os.path.join(eval_dir, prefix + f"_length_goal{suffix}.npy"), self.length_goal)
-        np.save(os.path.join(eval_dir, prefix + f"_length_path{suffix}.npy"), self.length_path)
-        np.save(os.path.join(eval_dir, prefix + f"_loss_obstacles{suffix}.npy"), self.loss_obstacles)
-        np.save(os.path.join(eval_dir, prefix + f"_skip_waypoint{suffix}.npy"), self.skip_waypoint)
-        np.save(os.path.join(eval_dir, prefix + f"_path_extension{suffix}.npy"), self.path_extension)
-        return
-
-    def load_eval_arrays(self, eval_dir: str, suffix: str = "") -> Tuple[bool, int]:
-        try:
-            self._load_eval_arrays(eval_dir, suffix)
-            self._filter_statistics()
-            return True, 0
-        except FileNotFoundError:
-            print(f"[INFO] No previous results found in {eval_dir}, search for preliminary results!")
-
-        subdirectories = [name for name in os.listdir(eval_dir) if os.path.isdir(os.path.join(eval_dir, name))]
-        pre_directories = [subdir for subdir in subdirectories if "pre" in subdir] if len(subdirectories) > 0 else []
-
-        if len(pre_directories) > 1:
-            raise ValueError(f"Multiple pre directories found {pre_directories}, please only keep the most recent one")
-        elif len(pre_directories) == 1:
-            try:
-                eval_dir = os.path.join(eval_dir, pre_directories[0])
-                idx = pre_directories[0][3:]
-                self._load_eval_arrays(eval_dir, idx)
-                print(f"[INFO] Found preliminary results in {eval_dir}, continue from {idx} waypoint!")
-                return False, int(idx[1:])
-            except FileNotFoundError:
-                print(f"[INFO] No preliminary results found in {eval_dir}, start from scratch!")
-                return False, 0
-        else:
-            print(f"[INFO] No preliminary results found in {eval_dir}, start from scratch!")
-            return False, 0
-
-    def _load_eval_arrays(self, eval_dir: str, suffix: str = "") -> None:
-        prefix: str = self.get_save_prefix()
-        self.goal_reached = np.load(os.path.join(eval_dir, prefix + f"_goal_reached{suffix}.npy"))
-        self.goal_within_fov = np.load(os.path.join(eval_dir, prefix + f"_goal_within_fov{suffix}.npy"))
-        self.base_collision = np.load(os.path.join(eval_dir, prefix + f"_base_collision{suffix}.npy"))
-        self.knee_collision = np.load(os.path.join(eval_dir, prefix + f"_knee_collision{suffix}.npy"))
-        self.walking_time = np.load(os.path.join(eval_dir, prefix + f"_walking_time{suffix}.npy"))
-        self.goal_distances = np.load(os.path.join(eval_dir, prefix + f"_goal_distances{suffix}.npy"))
-        self.length_goal = np.load(os.path.join(eval_dir, prefix + f"_length_goal{suffix}.npy"))
-        self.length_path = np.load(os.path.join(eval_dir, prefix + f"_length_path{suffix}.npy"))
-        self.loss_obstacles = np.load(os.path.join(eval_dir, prefix + f"_loss_obstacles{suffix}.npy"))
-        self.skip_waypoint = np.load(os.path.join(eval_dir, prefix + f"_skip_waypoint{suffix}.npy"))
-        self.path_extension = np.load(os.path.join(eval_dir, prefix + f"_path_extension{suffix}.npy"))
-        return
-
-    ##
-    # Waypoint functions
-    ##
-
-    def setup_waypoints(self) -> np.ndarray:
-        if self.use_waypoint_file:
-            print(f"Loading waypoints from {self._cfg.waypoint_file} ...", end=" ")
-            # load waypoints
-            self._load_waypoints()
-            # define start-points
-            start_point = self.waypoints["start"]
-            # set number of waypoint pairs
-            self.set_nbr_paths(len(self.waypoints["waypoints"]))
-            print("Waypoints loaded.")
-        else:
-            save_waypoint_path = os.path.join(
-                self._cfg.waypoint_dir,
-                f"explored_{self.get_env_name()}_seed{self._cfg.seed}_pairs{self._cfg.num_pairs}",
-            )
-
-            if self._cfg.use_existing_explored_waypoints and os.path.isfile(save_waypoint_path + ".pkl"):
-                print(f"[INFO] Loading explored waypoints from {save_waypoint_path} ...", end=" ")
-                with open(save_waypoint_path + ".pkl", "rb") as f:
-                    self.waypoints = pickle.load(f)
-                print("Waypoints loaded.")
-            else:
-                print(
-                    "[INFO] No waypoints specified. Using random exploration to select start-goals. Generating now ..."
-                )
-
-                sample_points, nn_idx, collision, distance = self.explore_env()
-                nbr_points = len(sample_points)
-
-                # get edge indices
-                idx_edge_start = np.repeat(np.arange(nbr_points), repeats=self._cfg.num_connections, axis=0)
-                idx_edge_end = nn_idx.reshape(-1)
-
-                # filter collision edges and distances
-                idx_edge_end = idx_edge_end[~collision.reshape(-1)]
-                idx_edge_start = idx_edge_start[~collision.reshape(-1)]
-                distance = distance[~collision.reshape(-1)]
-
-                # init graph
-                graph = nx.Graph()
-                # add nodes with position attributes
-                graph.add_nodes_from(list(range(nbr_points)))
-                pos_attr = {i: {"pos": sample_points[i]} for i in range(nbr_points)}
-                nx.set_node_attributes(graph, pos_attr)
-                # add edges with distance attributes
-                graph.add_edges_from(list(map(tuple, np.stack((idx_edge_start, idx_edge_end), axis=1))))
-                distance_attr = {
-                    (i, j): {"distance": distance[idx]} for idx, (i, j) in enumerate(zip(idx_edge_start, idx_edge_end))
-                }
-                nx.set_edge_attributes(graph, distance_attr)
-
-                # get all shortest paths
-                odom_goal_distances = dict(
-                    nx.all_pairs_dijkstra_path_length(graph, cutoff=self._cfg.max_goal_dist * 5, weight="distance")
-                )
-
-                # map distance to idx pairs
-                random.seed(self._cfg.seed)
-                distance_map = {}
-                for curr_distance in range(self._cfg.min_goal_dist, int(self._cfg.max_goal_dist)):
-                    # get all nodes with a distance to the goal of curr_distance
-                    pairs = []
-                    for key, value in odom_goal_distances.items():
-                        norm_distance = np.linalg.norm(sample_points[key] - sample_points[list(value.keys())], axis=1)
-                        decisions = np.where(
-                            np.logical_and(norm_distance >= curr_distance, norm_distance <= curr_distance + 1)
-                        )[0]
-                        if len(decisions) > 0:
-                            entries = np.array(list(value.keys()))[decisions]
-                            [pairs.append({key: entry}) for entry in entries]
-
-                    # randomly select certain pairs
-                    distance_map[curr_distance + 1] = random.sample(
-                        pairs,
-                        min(len(pairs), int(self._cfg.num_pairs / (self._cfg.max_goal_dist - self._cfg.min_goal_dist))),
-                    )
-
-                waypoints_idx = []
-                for values in distance_map.values():
-                    waypoints_idx.extend(values)
-
-                self.waypoints = []
-                for idxs in waypoints_idx:
-                    self.waypoints.append(
-                        {tuple(graph.nodes[list(idxs.keys())[0]]["pos"]): graph.nodes[list(idxs.values())[0]]["pos"]}
-                    )
-
-                # save waypoints
-                os.makedirs(self._cfg.waypoint_dir, exist_ok=True)
-                if os.path.isfile(save_waypoint_path + ".pkl"):
-                    print(f"[INFO] File already exists {save_waypoint_path}, will save new one with time!")
-                    now = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
-                    with open(save_waypoint_path + now + ".pkl", "wb") as fp:
-                        pickle.dump(self.waypoints, fp)
-                else:
-                    with open(save_waypoint_path + ".pkl", "wb") as fp:
-                        pickle.dump(self.waypoints, fp)
-
-            # define start points
-            start_point = list(self.waypoints[0].keys())[0]
-            # define number of waypoints / paths
-            self.set_nbr_paths(len(self.waypoints))
-
-            print("Done.")
-
-        # set start position and spawn position for anymal
-        self._cfg_anymal.translation_x = start_point[0]
-        self._cfg_anymal.translation_y = start_point[1]
-        self._cfg_anymal.translation_z = 1.0  # start_point[2]
-
-        return start_point
-
-    def _load_waypoints(self) -> None:
-        """
-        Expected that the waypoints have been recorded with the omni.isaac.waypoint extension and saved in .json format.
-        Structure of the json file:
-        {
-            start: [x, y, z],
-            end: [x, y, z],
-            waypoints: [[x, y, z], [x, y, z], ...]
-        }
-        """
-
-        if self._cfg.waypoint_file.endswith(".json"):
-            self.waypoints = json.load(open(self._cfg.waypoint_file))
-        else:
-            self.waypoints = json.load(open(self._cfg.waypoint_file + ".json"))
-
-        # apply scale
-        self.waypoints["start"] = [x for x in self.waypoints["start"]]
-        self.waypoints["end"] = [x for x in self.waypoints["end"]]
-        self.waypoints["waypoints"] = [[x for x in waypoint] for waypoint in self.waypoints["waypoints"]]
-
-        # draw waypoints
-        self.draw_interface.draw_points([self.waypoints["start"]], [(1.0, 0.4, 0.0, 1.0)], [(10)])  # orange
-        self.draw_interface.draw_points([self.waypoints["end"]], [(0.0, 1.0, 0.0, 1.0)], [(10)])  # green
-        self.draw_interface.draw_points(
-            self.waypoints["waypoints"],
-            [(0.0, 0.0, 1.0, 1.0)] * len(self.waypoints["waypoints"]),  # blue
-            [(10)] * len(self.waypoints["waypoints"]),
-        )
-
-        # attach end as further goal-point
-        self.waypoints["waypoints"].append(self.waypoints["end"])
-
-        return
-
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/vip_algo.py b/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/vip_algo.py
deleted file mode 100644
index 6edde84..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/vip_algo.py
+++ /dev/null
@@ -1,134 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-from typing import Optional
-
-# omni
-import carb
-import numpy as np
-
-# python
-import torch
-
-from viplanner.config import TrainCfg
-
-# viplanner src
-from viplanner.plannernet import (
-    PRE_TRAIN_POSSIBLE,
-    AutoEncoder,
-    DualAutoEncoder,
-    get_m2f_cfg,
-)
-from viplanner.traj_cost_opt.traj_opt import TrajOpt
-
-torch.set_default_dtype(torch.float32)
-
-
-class VIPlannerAlgo:
-    def __init__(self, model_dir: str, m2f_model_dir: Optional[str] = None, viplanner: bool = True) -> None:
-        """Apply VIPlanner Algorithm
-
-        Args:
-            model_dir (str): Directory that include model.pt and model.yaml
-        """
-        super().__init__()
-
-        assert os.path.exists(model_dir), "Model directory does not exist"
-        if viplanner:
-            assert os.path.isfile(os.path.join(model_dir, "model.pt")), "Model file does not exist"
-            assert os.path.isfile(os.path.join(model_dir, "model.yaml")), "Model config file does not exist"
-        else:
-            assert os.path.isfile(os.path.join(model_dir, "plannernet_scripted.pt")), "Model file does not exist"
-
-        # load model
-        self.train_config: TrainCfg = None
-        self.pixel_mean = None
-        self.pixel_std = None
-        self.load_model(model_dir, m2f_model_dir, viplanner)
-
-        self.traj_generate = TrajOpt()
-        return None
-
-    def load_model(self, model_dir: str, m2f_model_dir: Optional[str] = None, viplanner: bool = True) -> None:
-        if viplanner:
-            # load train config
-            self.train_config: TrainCfg = TrainCfg.from_yaml(os.path.join(model_dir, "model.yaml"))
-            carb.log_info(
-                f"Model loaded using sem: {self.train_config.sem}, rgb: {self.train_config.rgb}, knodes: {self.train_config.knodes}, in_channel: {self.train_config.in_channel}"
-            )
-
-            if isinstance(self.train_config.data_cfg, list):
-                self.max_goal_distance = self.train_config.data_cfg[0].max_goal_distance
-                self.max_depth = self.train_config.data_cfg[0].max_depth
-                self.depth_scale = self.train_config.data_cfg[0].depth_scale
-            else:
-                self.max_goal_distance = self.train_config.data_cfg.max_goal_distance
-                self.max_depth = self.train_config.data_cfg.max_depth
-                self.depth_scale = self.train_config.data_cfg.depth_scale
-
-            if self.train_config.rgb or self.train_config.sem:
-                if self.train_config.rgb and self.train_config.pre_train_sem:
-                    assert (
-                        PRE_TRAIN_POSSIBLE
-                    ), "Pretrained model not available since either detectron2 or mask2former not correctly setup"
-                    pre_train_cfg = os.path.join(m2f_model_dir, self.train_config.pre_train_cfg)
-                    pre_train_weights = (
-                        os.path.join(m2f_model_dir, self.train_config.pre_train_weights)
-                        if self.train_config.pre_train_weights
-                        else None
-                    )
-                    m2f_cfg = get_m2f_cfg(pre_train_cfg)
-                    self.pixel_mean = m2f_cfg.MODEL.PIXEL_MEAN
-                    self.pixel_std = m2f_cfg.MODEL.PIXEL_STD
-                else:
-                    m2f_cfg = None
-                    pre_train_weights = None
-
-                self.net = DualAutoEncoder(self.train_config, m2f_cfg=m2f_cfg, weight_path=pre_train_weights)
-            else:
-                self.net = AutoEncoder(self.train_config.in_channel, self.train_config.knodes)
-
-            # get model and load weights
-            try:
-                model_state_dict, _ = torch.load(os.path.join(model_dir, "model.pt"))
-            except ValueError:
-                model_state_dict = torch.load(os.path.join(model_dir, "model.pt"))
-            self.net.load_state_dict(model_state_dict)
-        else:
-            self.train_config: TrainCfg = TrainCfg(rgb=False, sem=False)
-            self.max_goal_distance = self.train_config.data_cfg.max_goal_distance
-            self.max_depth = self.train_config.data_cfg.max_depth
-            self.depth_scale = self.train_config.data_cfg.depth_scale
-            self.net = torch.jit.load(os.path.join(model_dir, "plannernet_scripted.pt"))
-
-        # inference script = no grad for model
-        self.net.eval()
-
-        # move to GPU if available
-        if torch.cuda.is_available():
-            self.net = self.net.cuda()
-            self.cuda_avail = True
-        else:
-            carb.log_warn("CUDA not available, VIPlanner will run on CPU")
-            self.cuda_avail = False
-        return
-
-    def plan(self, image: torch.Tensor, goal_robot_frame: torch.Tensor) -> tuple:
-        image = image.expand(-1, 3, -1, -1)
-        keypoints, fear = self.net(image, goal_robot_frame)
-        traj = self.traj_generate.TrajGeneratorFromPFreeRot(keypoints, step=0.1)
-
-        return keypoints, traj, fear
-
-    def plan_dual(self, dep_image: torch.Tensor, sem_image: torch.Tensor, goal_robot_frame: torch.Tensor) -> tuple:
-        keypoints, fear = self.net(dep_image, sem_image, goal_robot_frame)
-        traj = self.traj_generate.TrajGeneratorFromPFreeRot(keypoints, step=0.1)
-
-        return keypoints, traj, fear
-
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/vip_anymal.py b/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/vip_anymal.py
deleted file mode 100644
index 5df6838..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/vip_anymal.py
+++ /dev/null
@@ -1,574 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-import time
-from typing import Dict, Optional
-
-# omni
-import carb
-
-# python
-import numpy as np
-
-# omni-isaac-core
-import omni.isaac.core.utils.prims as prim_utils
-import open3d as o3d
-
-# ROS
-import rospy
-import scipy.spatial.transform as tf
-import torch
-import torchvision.transforms as transforms
-from geometry_msgs.msg import PoseStamped
-from nav_msgs.msg import Path
-
-# omni-isaac-anymal
-from omni.isaac.anymal.config import ANYmalCfg, VIPlannerCfg
-from omni.isaac.anymal.utils import get_cam_pose
-from omni.isaac.core.simulation_context import SimulationContext
-from omni.isaac.debug_draw import _debug_draw
-from omni.isaac.lab.robots.legged_robot import LeggedRobot
-
-# omni-isaac-lab
-from omni.isaac.lab.sensors.camera import Camera
-from PIL import Image
-from std_msgs.msg import Int16
-
-# viplanner
-from viplanner.config import TrainCfg, VIPlannerSemMetaHandler
-
-from .vip_algo import VIPlannerAlgo
-
-
-class VIPlanner:
-    """
-    Visual Imperative Planner for Anymal
-    """
-
-    debug: bool = False
-
-    def __init__(
-        self,
-        anymal_cfg: ANYmalCfg,
-        vip_cfg: VIPlannerCfg,
-        camera_sensors: Dict[str, Camera],
-    ) -> None:
-        self._cfg_anymal: ANYmalCfg = anymal_cfg
-        self._cfg_vip: VIPlannerCfg = vip_cfg
-        self._camera_sensors = camera_sensors
-
-        # Simulation context
-        self.sim: SimulationContext = SimulationContext.instance()
-
-        # ANYmal model and camera paths
-        if self._cfg_vip.use_mount_cam:
-            self.cam_path: dict = self._cfg_vip.cam_path["mount"]
-        elif self._cfg_anymal.anymal_type == 0:  # ANYmal C
-            self.cam_path: dict = self._cfg_vip.cam_path["ANYmal_C"]
-        elif self._cfg_anymal.anymal_type == 1:  # ANYmal D
-            self.cam_path: dict = self._cfg_vip.cam_path["ANYmal_D"]
-        else:
-            raise ValueError(
-                f"ANYmal type {self._cfg_anymal.anymal_type} not supported!\n"
-                "Either select '0' for ANYmal_C and '1' for ANYmal_D"
-            )
-        if "rgb" in self.cam_path and "depth" in self.cam_path:
-            self.same_cam: bool = False if self.cam_path["rgb"] != self.cam_path["depth"] else True
-
-        # planner status
-        if self._cfg_vip.ros_pub:
-            self.planner_status = Int16()
-            self.planner_status.data = 0
-
-        # additional variables
-        self.fear: float = 0.0
-        self.traj_waypoints_np: np.ndarray = np.zeros(0)
-        self.traj_waypoints_odom: np.ndarray = np.zeros(0)
-        self._step = 0  # number of times the waypoint have been generated, used together with the frequency
-        self.distance_to_goal: float = 0.0
-        self.max_goal_distance: float = 0.0 + 1.0e-9  # to avoid division by zero
-        self.start_time: float = 0.0
-
-        ##
-        # SETUP
-        ##
-
-        # check for cuda
-        self.device = "cuda" if torch.cuda.is_available() else "cpu"
-
-        # setup planner
-        self.planner = VIPlannerAlgo(self._cfg_vip.model_dir, self._cfg_vip.m2f_model_dir, self._cfg_vip.viplanner)
-
-        # check camera sensor
-        self._check_camera()
-
-        # setup goal transforms
-        self.goal_pos = prim_utils.get_prim_at_path(self._cfg_vip.goal_prim).GetAttribute("xformOp:translate")
-        self.goal_pos_prev = np.zeros(3)  # previous goal position to check if goal has changed
-
-        # get field of view
-        self.alpha_fov: float = 0.0
-        self.get_fov()
-
-        # setup pixel array for warping of the semantic image (only if semantics activated)
-        self.pix_depth_cam_frame: np.ndarray = np.zeros(
-            (
-                self._camera_sensors[self.cam_path["depth"]].data.image_shape[0]
-                * self._camera_sensors[self.cam_path["depth"]].data.image_shape[1],
-                3,
-            )
-        )
-        if self.planner.train_config.sem or self.planner.train_config.rgb:
-            self._compute_pixel_tensor()
-
-        # get transforms for images
-        self.transform = transforms.Compose(
-            [
-                transforms.ToTensor(),
-                transforms.Resize(self.planner.train_config.img_input_size),
-            ]
-        )
-
-        # setup waypoint display in Isaac
-        self.draw = _debug_draw.acquire_debug_draw_interface()
-        self.point_list = [(1, 0, 0.5)] * self._cfg_vip.num_points_network_return
-        self.color = [(0.4, 1.0, 0.1, 1.0)]  # green
-        self.color_fear = [(1.0, 0.4, 0.1, 1.0)]  # red
-        self.color_path = [(1.0, 0.5, 0.0, 1.0)]  # orange
-        self.size = [5.0]
-
-        # setup semantic meta data if carla is used
-        if self._cfg_vip.sem_origin == "isaac":
-            self.viplanner_sem_meta = VIPlannerSemMetaHandler()
-
-        # setup ROS
-        if self._cfg_vip.ros_pub:
-            self.path_pub = rospy.Publisher(self._cfg_vip.path_topic, Path, queue_size=10)
-            self.fear_path_pub = rospy.Publisher(self._cfg_vip.path_topic + "_fear", Path, queue_size=10)
-            self.status_pub = rospy.Publisher(self._cfg_vip.status_topic, Int16, queue_size=10)
-
-        # save image
-        if self._cfg_vip.save_images:
-            # Create annotator output directory
-            file_path = os.path.join(os.getcwd(), "_out_annot", "")
-            self.dir = os.path.dirname(file_path)
-            os.makedirs(self.dir, exist_ok=True)
-
-        # time
-        self.time_measurement: bool = False
-        self.time_collect: float = 0.0
-        self.time_save: float = 0.0
-
-        # flags
-        self.goal_outside_fov: bool = False
-        return
-
-    ##
-    # Public Functions
-    ##
-
-    def set_planner_callback(self, val: bool = True) -> None:
-        ##
-        # Setup callbacks
-        ##
-        if val:
-            self.sim.add_physics_callback("vip_callback", callback_fn=self._planner_callback)
-        else:
-            self.sim.remove_physics_callback("vip_callback")
-        return
-
-    def switch_model(self, model_dir: str, m2f_model_dir: Optional[str] = None) -> None:
-        if m2f_model_dir is None and self._cfg_vip.m2f_model_dir is not None:
-            m2f_model_dir = self._cfg_vip.m2f_model_dir
-        # delete previous model from GPU
-        if self.planner.cuda_avail:
-            del self.planner.net
-        # load new model
-        self.planner.load_model(model_dir, m2f_model_dir)
-        return
-
-    ##
-    # Internal Functions
-    ##
-
-    def _check_camera(self) -> None:
-        assert self._camera_sensors[self.cam_path["depth"]]._is_spawned, "Front Depth Camera not spawned!"
-
-        assert (
-            "distance_to_image_plane" in self._camera_sensors[self.cam_path["depth"]].cfg.data_types
-        ), "Missing data_type 'distance_to_image_plane' for front depth camera"
-
-        if self.planner.train_config.sem or self.planner.train_config.rgb:
-            assert self._camera_sensors[self.cam_path["rgb"]]._is_spawned, "Front RGB Camera not spawned!"
-            assert (
-                "semantic_segmentation" in self._camera_sensors[self.cam_path["rgb"]].cfg.data_types
-            ), "Missing data_type 'semantic_segmentation' for front camera"
-        if self._cfg_vip.rgb_debug:
-            assert (
-                "rgb" in self._camera_sensors[self.cam_path["rgb"]].cfg.data_types
-            ), "Missing data_type 'rgb' for front RGB camera"
-        return
-
-    def _planner_callback(self, dt) -> None:
-        # only plan with given frequency
-        if self._step % self._cfg_vip.planner_freq == 0:
-            # reset step counter
-            self._step = 0
-            # compute
-            self._camera_sensors[self.cam_path["depth"]].update(dt)
-            if not self.same_cam and self.planner.train_config.sem:
-                if self._cfg_vip.sem_origin == "isaac":
-                    # run complete update if carla
-                    self._camera_sensors[self.cam_path["rgb"]].update(dt)
-                else:
-                    # for matterport data will be written in camera by matterport callback, only update pose
-                    (
-                        self._camera_sensors[self.cam_path["rgb"]].data.position,
-                        self._camera_sensors[self.cam_path["rgb"]].data.orientation,
-                    ) = self._camera_sensors[self.cam_path["rgb"]]._compute_ros_pose()
-            elif not self.same_cam and self.planner.train_config.rgb:
-                self._camera_sensors[self.cam_path["rgb"]].update(dt)
-            self._compute()
-
-        # increment step counter
-        self._step += 1
-        return
-
-    def _compute(self) -> None:
-        # get goal pos
-        goal = np.asarray(self.goal_pos.Get())
-
-        cam_pos, cam_rot_quat = get_cam_pose(self._camera_sensors[self.cam_path["depth"]]._sensor_prim)
-        cam_rot = tf.Rotation.from_quat(cam_rot_quat).as_matrix()
-
-        # check if goal already reached --> exit here
-        self.distance_to_goal = np.sqrt((goal[0] - cam_pos[0]) ** 2 + (goal[1] - cam_pos[1]) ** 2)
-        if self.distance_to_goal < self._cfg_vip.conv_dist:
-            carb.log_info("GOAL REACHED!")
-            # planner status -> Success
-            if self._cfg_vip.ros_pub and self.planner_status.data == 0:
-                self.planner_status.data = 1
-                self.status_pub.publish(self.planner_status)
-            return
-        elif self._cfg_vip.ros_pub:
-            self.planner_status.data = 0
-            self.status_pub.publish(self.planner_status)
-        carb.log_verbose(f"DISTANCE TO GOAL: {self.distance_to_goal}")
-
-        # if goal is too far away --> project on max_goal_distance circle around robot
-        if self.distance_to_goal > self.planner.max_goal_distance:
-            goal[:2] = cam_pos[:2] + (goal[:2] - cam_pos[:2]) / self.distance_to_goal * self.planner.max_goal_distance
-
-        # apply rotation to goal  --> transform goal into camera frame
-        goal_cam_frame = goal - cam_pos
-        goal_cam_frame[2] = 0  # trained with z difference of 0
-        goal_cam_frame = goal_cam_frame @ cam_rot
-        goal_cam_frame = torch.tensor(goal_cam_frame, dtype=torch.float32, device=self.device).unsqueeze(0)
-
-        # check if goal pos has changed
-        if not np.all(goal == self.goal_pos_prev):
-            self.goal_pos_prev = goal
-            self.max_goal_distance = self.distance_to_goal
-            self.start_time = self.sim.current_time
-            self.is_reset = False
-
-            # check if goal is in fov
-            if abs(torch.atan2(goal_cam_frame[0, 1], goal_cam_frame[0, 0])) > self.alpha_fov / 2:
-                self.goal_outside_fov = True
-            else:
-                self.goal_outside_fov = False
-
-            carb.log_info(
-                f"New goal position: {goal} received in distance {self.distance_to_goal} (out FOV: {self.goal_outside_fov})"
-            )
-            print(
-                f"[VIPlanner INFO] New goal position: {goal} received in distance {self.distance_to_goal} (out FOV: {self.goal_outside_fov})"
-            )
-
-        start = time.time()
-        # Collect Groundtruth
-        depth_image = self._camera_sensors[self.cam_path["depth"]].data.output["distance_to_image_plane"]
-        depth_image[~np.isfinite(depth_image)] = 0  # set all inf or nan values to 0
-        depth_image[depth_image > self.planner.max_depth] = 0.0
-        depth_image_torch = self.transform(depth_image)  # declare as new variable since reused in semantic warp
-        depth_image_torch = depth_image_torch.unsqueeze(0).to(self.device)
-
-        # time for collecting data
-        self.time_collect = time.time() - start
-
-        if self.planner.train_config.sem:
-            # check if semantics available
-            if self._cfg_vip.sem_origin not in ["isaac", "callback"]:
-                carb.log_error(
-                    f"Unknown data source '{self._cfg_vip.sem_origin}'! Select either 'isaac' or 'callback'!"
-                )
-                return
-
-            if self._camera_sensors[self.cam_path["rgb"]].data.output["semantic_segmentation"] is None:
-                carb.log_warn("No semantic segmentation data available! No waypoint generated in this step!")
-                return
-            elif isinstance(self._camera_sensors[self.cam_path["rgb"]].data.output["semantic_segmentation"], dict) and [
-                label_class_dict["class"]
-                for label_class_dict in self._camera_sensors[self.cam_path["rgb"]]
-                .data.output["semantic_segmentation"]["info"]["idToLabels"]
-                .values()
-            ] == ["BACKGROUND", "UNLABELLED"]:
-                carb.log_warn(
-                    "Semantic data only of type BACKGROUND and UNLABELLED! No waypoint generated in this step!"
-                )
-                return
-
-            # handling for carla using lab camera class to generate the data
-            sem_image: np.ndarray = np.zeros(
-                (
-                    self._camera_sensors[self.cam_path["rgb"]].data.image_shape[1],
-                    self._camera_sensors[self.cam_path["rgb"]].data.image_shape[0],
-                )
-            )
-            if self._cfg_vip.sem_origin == "isaac":
-                # collect image
-                sem_image = self._camera_sensors[self.cam_path["rgb"]].data.output["semantic_segmentation"]["data"]
-                sem_idToLabels = self._camera_sensors[self.cam_path["rgb"]].data.output["semantic_segmentation"][
-                    "info"
-                ]["idToLabels"]
-                sem_image = self.sem_color_transfer(sem_image, sem_idToLabels)
-            else:
-                sem_image = self._camera_sensors[self.cam_path["rgb"]].data.output["semantic_segmentation"]
-
-            # overlay semantic image on depth image
-            sem_image = self._get_overlay_semantics(sem_image, depth_image, depth_rot=cam_rot)
-            # move to tensor
-            sem_image = self.transform(sem_image.astype(np.uint8))
-            sem_image = sem_image.unsqueeze(0).to(self.device)
-            # update time
-            self.time_collect = time.time() - start
-
-            # run network
-            _, traj_waypoints, self.fear = self.planner.plan_dual(depth_image_torch, sem_image, goal_cam_frame)
-        elif self.planner.train_config.rgb:
-            if self._camera_sensors[self.cam_path["rgb"]].data.output["rgb"] is None:
-                carb.log_warn("No rgb data available! No waypoint generated in this step!")
-                return
-            rgb_image = self._camera_sensors[self.cam_path["rgb"]].data.output["rgb"]
-
-            # overlay semantic image on depth image
-            rgb_image = self._get_overlay_semantics(rgb_image, depth_image, depth_rot=cam_rot)
-
-            # apply mean and std normalization
-            rgb_image = (rgb_image - self.planner.pixel_mean) / self.planner.pixel_std
-
-            # move to tensor
-            rgb_image = self.transform(rgb_image)
-            rgb_image = rgb_image.unsqueeze(0).to(self.device)
-            # update time
-            self.time_collect = time.time() - start
-
-            # run network
-            _, traj_waypoints, self.fear = self.planner.plan_dual(depth_image_torch, rgb_image, goal_cam_frame)
-        else:
-            # run network
-            _, traj_waypoints, self.fear = self.planner.plan(depth_image_torch, goal_cam_frame)
-
-        self.traj_waypoints_np = traj_waypoints.cpu().detach().numpy().squeeze(0)
-        self.traj_waypoints_np = self.traj_waypoints_np[1:, :]  # first twist command is zero --> remove it
-
-        # plot trajectory
-        self.traj_waypoints_odom = self.traj_waypoints_np @ cam_rot.T + cam_pos  # get waypoints in world frame
-        self.draw.clear_lines()
-        if self.fear > self._cfg_vip.fear_threshold:
-            self.draw.draw_lines(
-                self.traj_waypoints_odom.tolist()[:-1],
-                self.traj_waypoints_odom.tolist()[1:],
-                self.color_fear * len(self.traj_waypoints_odom.tolist()[1:]),
-                self.size * len(self.traj_waypoints_odom.tolist()[1:]),
-            )
-            self.draw.draw_lines(
-                [cam_pos.tolist()],
-                [goal.tolist()],
-                self.color_fear,
-                [2.5],
-            )
-        else:
-            self.draw.draw_lines(
-                self.traj_waypoints_odom.tolist()[:-1],
-                self.traj_waypoints_odom.tolist()[1:],
-                self.color * len(self.traj_waypoints_odom.tolist()[1:]),
-                self.size * len(self.traj_waypoints_odom.tolist()[1:]),
-            )
-            self.draw.draw_lines(
-                [cam_pos.tolist()],
-                [goal.tolist()],
-                self.color_path,
-                [2.5],
-            )
-
-        if self._cfg_vip.ros_pub:
-            self._pub_path(waypoints=self.traj_waypoints_np)
-        else:
-            carb.log_info(f"New waypoints generated! \n {self.traj_waypoints_np}")
-
-        if self._cfg_vip.save_images:
-            start = time.time()
-            self._save_depth(depth_image, self.dir + "/depth_front_step_" + str(self._step))
-            self._save_rgb() if self._cfg_vip.rgb_debug else None
-            self.time_save = time.time() - start
-
-        if self.time_measurement:
-            print(f"Time collect: {self.time_collect} \t Time save: {self.time_save}")
-        return
-
-    def reset(self) -> None:
-        """Reset the planner variables."""
-        self.fear: float = 0.0
-        self.traj_waypoints_np: np.ndarray = np.zeros(0)
-        self.traj_waypoints_odom: np.ndarray = np.zeros(0)
-        self._step = 0
-        self.goal_outside_fov: bool = False
-        self.goal_pos_prev: np.ndarray = np.zeros(3)
-        self.distance_to_goal: float = 0.0
-        self.max_goal_distance: float = 0.0 + 1.0e-9
-        self.start_time: float = 0.0
-        self.is_reset: bool = True
-        return
-
-    def _pub_path(self, waypoints: torch.Tensor) -> None:
-        path = Path()
-        fear_path = Path()
-        curr_time = rospy.Time.from_sec(self.sim.current_time)
-        for p in waypoints:
-            pose = PoseStamped()
-            pose.header.stamp = curr_time
-            pose.header.frame_id = "odom"
-            pose.pose.position.x = p[0]
-            pose.pose.position.y = p[1]
-            pose.pose.position.z = p[2]
-            path.poses.append(pose)
-        # add header
-        path.header.frame_id = fear_path.header.frame_id = "odom"
-        path.header.stamp = fear_path.header.stamp = curr_time
-        # publish fear path
-        # if self.is_fear_reaction:
-        #     fear_path.poses = copy.deepcopy(path.poses)
-        #     path.poses = path.poses[:1]
-        # publish path
-        # self.fear_path_pub.publish(fear_path)
-        self.path_pub.publish(path)
-        return
-
-    def get_fov(self) -> None:
-        # load intrinsics --> used to calculate fov
-        intrinsics = self._camera_sensors[self.cam_path["depth"]].data.intrinsic_matrix
-        self.alpha_fov = 2 * np.arctan(intrinsics[0, 0] / intrinsics[0, 2])
-        return
-
-    """ Helper to warp semantic image to depth image """
-
-    def _get_overlay_semantics(self, sem_img: np.ndarray, depth_img: np.ndarray, depth_rot: np.ndarray) -> np.ndarray:
-        # get semantic rotation matrix
-        sem_pos, sem_rot_quat = get_cam_pose(self._camera_sensors[self.cam_path["rgb"]]._sensor_prim)
-        sem_rot = tf.Rotation.from_quat(sem_rot_quat).as_matrix()
-        sem_rot = sem_rot.astype(np.float64)
-        depth_rot = depth_rot.astype(np.float64)
-
-        # project depth pixels into 3d space
-        # dep_im_reshaped = depth_img.reshape(-1, 1)
-        dep_im_reshaped = depth_img.reshape(-1, 1)
-        points = (
-            dep_im_reshaped * (depth_rot @ self.pix_depth_cam_frame.T).T
-            + self._camera_sensors[self.cam_path["depth"]].data.position
-        )
-
-        # transform points to semantic camera frame
-        points_sem_cam_frame = (sem_rot.T @ (points - sem_pos).T).T
-        # normalize points
-        points_sem_cam_frame_norm = points_sem_cam_frame / points_sem_cam_frame[:, 0][:, np.newaxis]
-        # reorder points be camera convention (z-forward)
-        points_sem_cam_frame_norm = points_sem_cam_frame_norm[:, [1, 2, 0]] * np.array([-1, -1, 1])
-        # transform points to pixel coordinates
-        pixels = (self._camera_sensors[self.cam_path["rgb"]].data.intrinsic_matrix @ points_sem_cam_frame_norm.T).T
-        # filter points outside of image
-        filter_idx = (
-            (pixels[:, 0] >= 0)
-            & (pixels[:, 0] < sem_img.shape[1])
-            & (pixels[:, 1] >= 0)
-            & (pixels[:, 1] < sem_img.shape[0])
-        )
-        # get semantic annotation
-        sem_annotation = np.zeros((pixels.shape[0], 3), dtype=np.uint16)
-        sem_annotation[filter_idx] = sem_img[pixels[filter_idx, 1].astype(int), pixels[filter_idx, 0].astype(int)]
-        # reshape to image
-        sem_img_warped = sem_annotation.reshape(depth_img.shape[0], depth_img.shape[1], 3)
-
-        # DEBUG
-        if self.debug:
-            import matplotlib.pyplot as plt
-
-            f, (ax1, ax2, ax3) = plt.subplots(1, 3)
-            ax1.imshow(depth_img)
-            ax2.imshow(sem_img_warped / 255)
-            ax3.imshow(depth_img)
-            ax3.imshow(sem_img_warped / 255, alpha=0.5)
-            plt.show()
-
-            pcd = o3d.geometry.PointCloud()
-            pcd.points = o3d.utility.Vector3dVector(points)
-            o3d.visualization.draw_geometries([pcd])
-
-        return sem_img_warped
-
-    """Semantic Image Color Transfer"""
-
-    def sem_color_transfer(self, sem_image: np.ndarray, sem_idToLabels: dict) -> np.ndarray:
-        """Convert semantic segmentation image to viplanner color space
-
-        Args:
-            sem_image (np.ndarray): sem_image as received by the simulation
-            sem_idToLabels (dict): information about which class is which index in sem_image
-
-        Returns:
-            np.ndarray: sem_image in viplanner color space
-        """
-        if not sem_idToLabels:
-            carb.log_warn("No semantic segmentation data available! No waypoint generated in this step!")
-            return
-
-        for k, v in sem_idToLabels.items():
-            if not dict(v):
-                sem_idToLabels[k] = {"class": "static"}
-            elif "BACKGROUND" == v["class"]:
-                sem_idToLabels[k] = {"class": "static"}
-            elif "UNLABELLED" == v["class"]:
-                sem_idToLabels[k] = {"class": "static"}
-
-        # color mapping
-        sem_idToColor = np.array(
-            [
-                [
-                    int(k),
-                    self.viplanner_sem_meta.class_color[v["class"]][0],
-                    self.viplanner_sem_meta.class_color[v["class"]][1],
-                    self.viplanner_sem_meta.class_color[v["class"]][2],
-                ]
-                for k, v in sem_idToLabels.items()
-            ]
-        )
-
-        # order colors by their id and necessary to account for missing indices (not guaranteed to be consecutive)
-        sem_idToColorMap = np.zeros((max(sem_idToColor[:, 0]) + 1, 3), dtype=np.uint8)
-        for cls_color in sem_idToColor:
-            sem_idToColorMap[cls_color[0]] = cls_color[1:]
-        # colorize semantic image
-        try:
-            sem_image = sem_idToColorMap[sem_image.reshape(-1)].reshape(sem_image.shape + (3,))
-        except IndexError:
-            print("IndexError: Semantic image contains unknown labels")
-            return
-
-        return sem_image
-
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/vip_config.py b/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/vip_config.py
deleted file mode 100644
index 760ad56..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/vip_config.py
+++ /dev/null
@@ -1,97 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# python
-import os
-from dataclasses import MISSING
-
-import numpy as np
-
-# lab-assets
-from omni.isaac.assets import ASSETS_RESOURCES_DIR
-
-# omni-isaac-lab
-from omni.isaac.lab.utils.configclass import configclass
-
-
-@configclass
-class TwistControllerCfg:
-    lookAheadDistance: float = 0.5
-    minPointsWithinLookAhead: int = 3
-    two_way_drive: bool = False
-    switch_time_threshold: float = 1.0
-    maxSpeed: float = 0.5
-    maxAccel: float = 2.5 / 100.0  # 2.5 / 100
-    joyYaw: float = 1.0
-    yawRateGain: float = 7.0  # 3.5
-    stopYawRateGain: float = 7.0  # 3.5
-    maxYawRate: float = 90.0 * np.pi / 360
-    dirDiffThre: float = 0.7
-    stopDisThre: float = 0.4
-    slowDwnDisThre: float = 0.3
-    slowRate1: float = 0.25
-    slowRate2: float = 0.5
-    noRotAtGoal: bool = True
-    autonomyMode: bool = False
-
-    # extra functionality
-    stuck_time_threshold: float = 2.0
-    stuck_avoidance_duration: int = 30  # number of steps stuck avoidance twist will be executed
-
-
-@configclass
-class VIPlannerCfg:
-    """Configuration for the ROS publishing for Waypoint Follower and VIPlanner (ROS)."""
-
-    viplanner: bool = True
-    """Use VIPlanner or iPlanner"""
-    model_dir: str = os.path.join(
-        ASSETS_RESOURCES_DIR,
-        "vip_models/plannernet_env2azQ1b91cZZ_new_colorspace_ep100_inputDepSem_costSem_optimSGD_new_colorspace_sharpend_indoor",
-    )
-    """Path to the model directory (expects a model.pt and model.yaml file in the directory)."""
-    sem_origin: str = (
-        "isaac"  # "isaac" or "callback (in case the semantics cannot be generated in isaac e.g. matterport)"
-    )
-    """Data source of the environment --> important for color mapping of the semantic segmentation"""
-    m2f_model_dir: str = os.path.join(ASSETS_RESOURCES_DIR, "vip_models", "m2f_models")
-    """Path to mask2former model for direct RGB input (directly including config file and model weights)"""
-    planner_freq: int = 20
-    """Frequency of the planner in Hz."""
-    goal_prim: str = "/World/waypoint"
-    """The prim path of the cube in the USD stage"""
-    cam_path: dict = {
-        "ANYmal_C": {"rgb": "front_depth", "depth": "front_depth"},
-        "ANYmal_D": {"rgb": "front_rgb", "depth": "front_depth"},
-        "mount": {"rgb": "viplanner_rgb", "depth": "viplanner_depth"},
-    }
-    use_mount_cam: bool = False
-    """Camera Path names as defined in config.sensor_cfg that should be used to render the network inputs"""
-    rgb_debug: bool = False
-    """Save RGB images together with depth (mainly for debug reasons)."""
-    num_points_network_return: int = 51
-    """Number of points the network returns."""
-    conv_dist: float = 0.5
-    """Distance to the goal to save that it has been reached successfully"""
-    obs_loss_threshold: float = 0.3
-    """Obstacle threshold to consider a path as successful"""
-    path_topic: str = "/path"
-    """Topic to publish the path."""
-    status_topic: str = "/status"
-    """Topic to publish the planner status."""
-    save_images: bool = False
-    """Save depth images to disk."""
-    ros_pub: bool = False
-    """Publish the path and status to ROS (only needed for VIPlanner ROS)."""
-    look_back_factor: int = 15
-    """Look back factor for the path."""
-    fear_threshold: float = 0.5
-
-    # twist controller config
-    twist_controller_cfg: TwistControllerCfg = TwistControllerCfg()
-
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/visual_imperative_planner.py b/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/visual_imperative_planner.py
deleted file mode 100644
index 988b956..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/visual_imperative_planner.py
+++ /dev/null
@@ -1,203 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from typing import Dict, Optional
-
-# python
-import numpy as np
-import torch
-
-# omni-isaac-anymal
-from omni.isaac.anymal.config import ANYmalCfg, ROSPublisherCfg, VIPlannerCfg
-from omni.isaac.anymal.policy import Agent
-from omni.isaac.anymal.utils import (
-    AnymalROSPublisher,
-    AnymalROSSubscriber,
-    TwistController,
-)
-from omni.isaac.anymal.utils.ros_utils import check_roscore_running, init_rosnode
-from omni.isaac.anymal.utils.twist_controller_new import TwistControllerNew
-from omni.isaac.anymal.viplanner import VIPlanner
-
-# omni-isaac-core
-from omni.isaac.core.objects import VisualCuboid
-
-# omni-isaac-lab
-from omni.isaac.lab.robots.legged_robot import LeggedRobot
-from omni.isaac.lab.sensors.camera import Camera
-from omni.isaac.lab.sensors.height_scanner import HeightScanner
-from tqdm import tqdm
-
-
-class VIPlannerANYmal(Agent):
-    """
-    Visual Imperative Planner to guide ANYway to a waypoint defined by a cube in the world.
-
-    Two versions available:
-    - Isaac Twist Controller (default), Twist Controller is implemented in Python, no ROS exchange has to be done
-    - ROS Twist Controller (old), Twist Controller is implemented in C++, path has to be published to ROS and twist command are received
-
-    """
-
-    def __init__(
-        self,
-        cfg: ANYmalCfg,
-        camera_sensors: Dict[str, Camera],
-        robot: LeggedRobot,
-        height_scanner: HeightScanner,
-        ros_controller: bool = False,
-        planner_cfg: Optional[VIPlannerCfg] = None,
-    ) -> None:
-        # init agent
-        super().__init__(cfg.rl_policy, robot, height_scanner)
-        self._anymal_cfg = cfg
-        self._camera_sensors = camera_sensors
-        self._ros_controller = ros_controller
-
-        # viplanner
-        self.planner: VIPlanner = None
-        # waypoint cube
-        self.cube: VisualCuboid = None
-        # planner cfg
-        self._planner_cfg = planner_cfg if planner_cfg else VIPlannerCfg()
-
-        if self._ros_controller:
-            # init planner config
-            self._planner_cfg.ros_pub = True
-            # init ROS publisher config
-            self._ros_publisher_cfg = ROSPublisherCfg(sensor_pub=False)
-            # setup cube as waypoint and ros connection
-            self.ros_publisher: AnymalROSPublisher = None
-            self.ros_subscriber: AnymalROSSubscriber = None
-        else:
-            self._planner_cfg.ros_pub = False
-            self.twist: TwistController = None
-
-        self._setup()
-
-        # reset once at initialization
-        self.reset()
-
-        # get message
-        self.title += "with Visual Imperative Planner \n"
-        self.msg += "\n\n"
-        self.msg += f""  # TODO: add more info
-        return
-
-    def compute_command_ros(self, step_size: float) -> None:
-        """Compute the command for the robot using the ROS Twist Controller"""
-        # get command from joystick planner
-        last_command, command_time = self.twist.get_command()
-        # check if last command is not too long ago (would happen if goal is reached)
-        if command_time > (self.sim.current_time - self._planner_cfg.look_back_factor * step_size):
-            return torch.tensor(last_command, device=self.robot.device)
-        else:
-            return torch.zeros(3, device=self.robot.device)
-
-    def compute_command_isaac(self, step_size: float) -> None:
-        """Compute the command for the robot using the Python Twist Controller"""
-        # get command from twist controller
-        last_command = self.twist.compute(self.planner.traj_waypoints_odom, self.planner.fear)
-        try:
-            return torch.tensor(last_command, device=self.robot.device)
-        except TypeError:
-            return torch.zeros(3, device=self.robot.device)
-
-    def reset(self) -> None:
-        super().reset()
-        self.planner.reset()
-        if not self._ros_controller:
-            self.twist.reset()
-        # reset pbar
-        self.pbar.reset()
-        return
-
-    ##
-    # Helper Functions
-    ##
-
-    def _setup(self) -> None:
-        """Setup cube and the ros connection to the smart joystick"""
-        # cube
-        self._setup_cube()
-        # viplanner
-        self.planner = VIPlanner(
-            anymal_cfg=self._anymal_cfg, vip_cfg=self._planner_cfg, camera_sensors=self._camera_sensors
-        )
-        # for ROS based controller
-        if self._ros_controller:
-            # init rosnode
-            check_roscore_running()
-            init_rosnode("anymal_node")
-            # init publisher and subscriber
-            self.ros_publisher = AnymalROSPublisher(
-                anymal_cfg=self._anymal_cfg,
-                ros_cfg=self._ros_publisher_cfg,
-                camera_sensors=self._camera_sensors,
-                lidar_sensors=self._lidar_sensors,
-            )
-            self.twist = AnymalROSSubscriber()
-            # define function to compute command
-            self.compute_command = self.compute_command_ros
-        else:
-            # self.twist = TwistController(
-            #     cfg=self._planner_cfg.twist_controller_cfg,
-            #     cfg_vip=self._planner_cfg,
-            #     cfg_anymal=self._anymal_cfg,
-            #     camera_sensors=self._camera_sensors,
-            # )
-            self.twist = TwistControllerNew(
-                cfg=self._planner_cfg.twist_controller_cfg,
-                cfg_vip=self._planner_cfg,
-                cfg_anymal=self._anymal_cfg,
-                robot=self.robot,
-            )
-            # define function to compute command
-            self.compute_command = self.compute_command_isaac
-        # setup pbar
-        self._setup_pbar()
-        return
-
-    def _setup_cube(self) -> None:
-        """cube as the definition of a goalpoint"""
-        self.cube = VisualCuboid(
-            prim_path=self._planner_cfg.goal_prim,  # The prim path of the cube in the USD stage
-            name="waypoint",  # The unique name used to retrieve the object from the scene later on
-            position=np.array([5, 0, 1.0]),  # Using the current stage units which is in meters by default.
-            scale=np.array([0.15, 0.15, 0.15]),  # most arguments accept mainly numpy arrays.
-            size=1.0,
-            color=np.random.uniform((1, 0, 0)),  # RGB channels, going from 0-1
-        )
-        return
-
-    # progress bar
-    def _setup_pbar(self):
-        """Setup progress bar"""
-        self.pbar = tqdm(total=100, position=0, leave=False, bar_format="{desc}{percentage:.0f}%|{bar}|")
-        return
-
-    def _update_pbar(self):
-        """Update progress bar"""
-        if self.planner.is_reset:
-            return
-
-        desc = (
-            f"Time Elapsed: {self.sim.current_time - self.planner.start_time:.2f}s | "
-            f"Walked Distance: {self.planner.max_goal_distance-self.planner.distance_to_goal:.2f}/{self.planner.max_goal_distance:.2f}m | "
-            f"Twist: {self.twist.twist}"
-        )
-        self.pbar.set_description(desc)
-
-        percentage_completed_path = (1 - (self.planner.distance_to_goal / self.planner.max_goal_distance)) * 100
-        update_percentage = percentage_completed_path - self.pbar.n
-        if update_percentage > 0:
-            self.pbar.update(update_percentage)
-        else:
-            self.pbar.update(0)
-        return
-
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/walking_cfg.py b/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/walking_cfg.py
deleted file mode 100644
index 8365af4..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/intern/extension_scripts/walking_cfg.py
+++ /dev/null
@@ -1,95 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""
-Simulation configuration for the robot.
-
-Note:
-    These are originally taken from the locomotion/velocity.py environment in lab.
-"""
-
-# python
-import os
-from dataclasses import dataclass, field
-from typing import List
-
-# lab-assets
-from omni.isaac.assets import ASSETS_DATA_DIR, ASSETS_RESOURCES_DIR
-
-# lab
-from omni.isaac.lab.robots.config.anymal import ANYMAL_B_CFG, ANYMAL_C_CFG
-from omni.isaac.lab.robots.legged_robot import LeggedRobotCfg
-from omni.isaac.lab.sensors.height_scanner import HeightScannerCfg
-from omni.isaac.lab.sensors.height_scanner.utils import create_points_from_grid
-from omni.isaac.lab.utils.configclass import configclass
-
-# omni-isaac-anymal
-from .controller_cfg import LocomotionRlControllerCfg
-
-
-@dataclass
-class ANYmalCfg:
-    """Configuration for the walking extension."""
-
-    # simulator
-    sim: SimCfg = SimCfg()
-    viewer: ViewerCfg = ViewerCfg()
-    # scene
-    terrain: TerrainCfg = TerrainCfg()
-
-    # controller
-    rl_policy: LocomotionRlControllerCfg = LocomotionRlControllerCfg(
-        checkpoint_path=os.path.join(ASSETS_RESOURCES_DIR, "policy", "policy_obs_to_action_exp.pt"),
-    )
-    # robot
-    robot: List[LeggedRobotCfg] = field(default_factory=lambda: [ANYMAL_C_CFG, ANYMAL_C_CFG])  # ANYmal D not available
-    sensor: SensorCfg = SensorCfg()
-    height_scanner: HeightScannerCfg = HeightScannerCfg(
-        sensor_tick=0.0,
-        points=create_points_from_grid(size=(1.6, 1.0), resolution=0.1),
-        offset=(0.0, 0.0, 0.0),
-        direction=(0.0, 0.0, -1.0),
-        max_distance=1.0,
-    )
-    # translation and rotation
-    translation_x: float = 0.0
-    translation_y: float = 0.0
-    translation_z: float = 0.7
-    quat: tuple = (1.0, 0.0, 0.0, 0.0)  # w,x,y,z
-
-    # prim path
-    prim_path: str = "/World/Anymal_c/Robot"
-    # ANYmal type
-    anymal_type: int = 0  # 0: ANYmal C, 1: ANYmal D
-
-    # record data for evaluation
-    follow_camera: bool = True
-    rec_frequency: int = 1  # nbr of sim.steps between two camera updates
-    rec_path: bool = True
-    rec_sensor: bool = True
-
-    # set functions
-    def _set_translation_x(self, value: list):
-        self.translation_x = value
-
-    def _set_translation_y(self, value: list):
-        self.translation_y = value
-
-    def _set_translation_z(self, value: list):
-        self.translation_z = value
-
-    def _set_prim_path(self, value: str):
-        self.prim_path = value
-
-    def _set_anymal_type(self, value: int):
-        self.anymal_type = value
-
-    # get functions
-    def get_translation(self):
-        return (self.translation_x, self.translation_y, self.translation_z)
-
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/intern/matterport_exploration/__init__.py b/isaaclab/extension/omni.isaac.viplanner/intern/matterport_exploration/__init__.py
deleted file mode 100644
index 077fbbb..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/intern/matterport_exploration/__init__.py
+++ /dev/null
@@ -1,11 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from .random_exploration import RandomExplorer
-
-__all__ = ["RandomExplorer"]
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/intern/matterport_exploration/random_exploration.py b/isaaclab/extension/omni.isaac.viplanner/intern/matterport_exploration/random_exploration.py
deleted file mode 100644
index 2cd2c47..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/intern/matterport_exploration/random_exploration.py
+++ /dev/null
@@ -1,649 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-import random
-import time
-from typing import Tuple
-
-import carb
-import numpy as np
-
-# omni
-import omni
-import scipy.spatial.transform as tf
-import torch
-from omni.isaac.matterport.config import SamplerCfg
-
-# omni-isaac-matterport
-from omni.isaac.matterport.semantics import MatterportWarp
-
-# omni-isaac-lab
-from omni.isaac.lab.sensors.camera import Camera, PinholeCameraCfg
-from scipy.spatial import KDTree
-
-# python
-from scipy.stats import qmc
-
-
-class RandomExplorer:
-    debug = False
-    time_measurement: bool = False
-
-    def __init__(self, domains: MatterportWarp, cfg: SamplerCfg = None):
-        # config
-        self._cfg_explorer = SamplerCfg() if cfg is None else cfg
-
-        # domains
-        self.domains: MatterportWarp = domains
-
-        # setup cameras and writer
-        ply_file = os.path.split(self.domains._cfg.import_file_ply)[1]
-        if self._cfg_explorer.suffix is not None and isinstance(self._cfg_explorer.suffix, str):
-            suffix = "_" + self._cfg_explorer.suffix
-        else:
-            suffix = ""
-        self.save_path = os.path.join(self._cfg_explorer.save_path, ply_file[:-4] + suffix)
-
-        # get camera centers
-        self.nbr_points: int = 0
-        self.camera_centers: torch.Tensor = torch.empty((0, 3), dtype=torch.float32)
-
-        # get_variations
-        self.x_angles: np.ndarray = np.zeros(self.nbr_points)
-        self.y_angles: np.ndarray = np.zeros(self.nbr_points)
-        self.z_angles: np.ndarray = np.zeros(self.nbr_points)
-
-        # setup conv
-        self.nbr_faces: int = 0
-        self.face_idx: torch.Tensor = torch.zeros((0,), dtype=torch.int64)
-        self.conv_crit = True
-
-        self.pt_idx = 0
-        self.save_idx = 0
-
-        # time measurements
-        self.time_raycast: float = 0.0
-        self.time_render: float = 0.0
-        self.time_save: float = 0.0
-        self.time_total: float = 0.0
-
-        return
-
-    ##
-    # Public Function
-    ##
-
-    def setup(self) -> None:
-        self._setup_cameras()
-        self.domains.init_save(save_path=self.save_path)
-        self._get_sample_points()
-        self._get_view_variations()
-        self._setup_conv()
-        return
-
-    def explore(self) -> None:
-        # get cam_data
-        cam_data_depth = self.domains.cameras[0]
-        cam_data_sem = self.domains.cameras[1]
-
-        # start sim if RGB images should be rendered
-        if self._cfg_explorer.cam_sem_rgb:
-            self.domains.sim.play()
-
-        while self.conv_crit:
-            total_start = time.time()
-
-            # get current variation in camera position and rotation of the semantic camera
-            # rotations follow the Isaac convention: x-forward, y-left, z-up
-            cam_data_sem.pos = self.camera_centers[self.pt_idx]
-            cam_rot_sem_from_odom = np.array(
-                [self.x_angles[self.pt_idx], self.y_angles[self.pt_idx], int(self.z_angles[self.pt_idx])]
-            )
-            cam_rot_sem_from_odom_mat = tf.Rotation.from_euler("xyz", cam_rot_sem_from_odom, degrees=True).as_matrix()
-            cam_data_sem.rot = torch.tensor(cam_rot_sem_from_odom_mat, dtype=torch.float32)
-            carb.log_verbose(f"Point: {self.pt_idx} \tsem camera pose: {cam_data_sem.pos} {cam_rot_sem_from_odom}")
-
-            # get depth camera rotation relative to the semantic camera rotation and convert it to Isaac convention
-            # Isaac Convention: x-forward, y-left, z-up
-            if self._cfg_explorer.tf_quat_convention == "isaac":
-                cam_rot_sem_from_depth = tf.Rotation.from_quat(self._cfg_explorer.tf_quat).as_matrix()
-            elif self._cfg_explorer.tf_quat_convention == "roll-pitch-yaw":
-                cam_rot_sem_from_depth = tf.Rotation.from_quat(self._cfg_explorer.tf_quat).as_euler("XYZ", degrees=True)
-                cam_rot_sem_from_depth[[1, 2]] *= -1
-                cam_rot_sem_from_depth = tf.Rotation.from_euler("XYZ", cam_rot_sem_from_depth, degrees=True).as_matrix()
-            else:
-                raise ValueError(f"tf_quat_convention {self._cfg_explorer.tf_quat_convention} not supported")
-
-            # get depth camera pose and rotation from the semantic camera pose and rotation
-            cam_rot_depth_from_odom = np.matmul(cam_rot_sem_from_odom_mat, cam_rot_sem_from_depth.T)
-            cam_data_depth.rot = torch.tensor(cam_rot_depth_from_odom, dtype=torch.float32)
-            vec_depth_to_sem_odom_frame = np.matmul(cam_rot_depth_from_odom, self._cfg_explorer.tf_pos)
-            cam_data_depth.pos = cam_data_sem.pos - torch.tensor(vec_depth_to_sem_odom_frame, dtype=torch.float32)
-
-            # do raycasting
-            start = time.time()
-            hit_rate = 1.0
-            for cam_data in self.domains.cameras:
-                # get new ray directions in world frame
-                cam_data.ray_directions = self.domains._get_ray_directions(
-                    cam_data.pos, cam_data.rot, cam_data.pixel_coords
-                )
-
-                # raycast
-                cam_data.ray_hit_coords, cam_data.ray_face_indices, cam_data.ray_distances = self.domains._raycast(
-                    cam_data.pos.repeat(len(cam_data.ray_directions)),
-                    cam_data.ray_directions,
-                    cam_rot=cam_data.rot,
-                    pix_offset=cam_data.pixel_offset,
-                )
-
-                # filter inf values
-                hit_rate_single_cam = torch.isfinite(cam_data.ray_distances).sum() / len(cam_data.ray_distances)
-                hit_rate = min(hit_rate, hit_rate_single_cam)
-                carb.log_verbose(f"Point: {self.pt_idx} \tRate of rays hitting the mesh: {hit_rate_single_cam}")
-                cam_data.ray_hit_coords[~torch.isfinite(cam_data.ray_hit_coords)] = 0
-                cam_data.ray_distances[~torch.isfinite(cam_data.ray_distances)] = 0
-
-            self.time_raycast = time.time() - start
-
-            # filter points with insufficient hit rate and too small min distance (use the semantic camera)
-            if hit_rate < self._cfg_explorer.min_hit_rate:
-                print(f"Point: {self.pt_idx} \trejected due to insufficient hit rate")
-                self.pt_idx += 1
-                continue
-            elif torch.mean(cam_data_sem.ray_distances) < self._cfg_explorer.min_avg_hit_distance:
-                print(f"Point: {self.pt_idx} \trejected due to too small average hit distance")
-                self.pt_idx += 1
-                continue
-            elif torch.std(cam_data_sem.ray_distances) < self._cfg_explorer.min_std_hit_distance:
-                print(f"Point: {self.pt_idx} \trejected due to too small standard deviation of hit distance")
-                self.pt_idx += 1
-                continue
-
-            # DEBUG
-            if self.debug:
-                # set camera to the random selected pose
-                self.domains.draw.clear_points()
-                self.domains.draw.draw_points(
-                    random.choices(cam_data_sem.ray_hit_coords.cpu().tolist(), k=5000),
-                    self.domains.colors_2,
-                    self.domains.sizes,
-                )
-                self.domains.draw.draw_points(
-                    random.choices(cam_data_sem.pixel_coords.cpu().tolist(), k=5000),
-                    self.domains.colors_3,
-                    self.domains.sizes,
-                )
-
-            # render and save data
-            for idx, cam_data in enumerate(self.domains.cameras):
-                start = time.time()
-                self.domains._render(cam_data)
-                self.time_render = time.time() - start
-
-                if cam_data.visualize:
-                    start = time.time()
-                    self.domains._update_visualization(cam_data)
-                    self.time_visualize = time.time() - start
-
-                start = time.time()
-                self.domains._save_data(cam_data, self.save_idx, cam_idx=idx)
-                self.time_save = time.time() - start
-
-            # DEBUG
-            if self.debug:
-                import matplotlib.pyplot as plt
-
-                _, axs = plt.subplots(1, 2, figsize=(15, 5))
-                axs[0].imshow(cam_data_depth.render_depth)
-                axs[1].imshow(cam_data_sem.render_sem)
-                plt.show()
-
-            # check convergence according to semantic camera
-            ray_face_filtered = cam_data_sem.ray_face_indices[cam_data_sem.ray_face_indices != -1]
-            self.face_idx[ray_face_filtered.long().cpu()] += 1
-
-            conv_face = torch.sum(self.face_idx > 2)
-            conv_rate = conv_face / self.nbr_faces
-            if conv_rate > self._cfg_explorer.conv_rate or self.save_idx > self._cfg_explorer.max_images:
-                self.conv_crit = False
-
-            self.time_total = time.time() - total_start
-
-            # Update messages
-            face1_count = torch.sum(self.face_idx >= 1).item()
-            print(
-                f"Point: {self.pt_idx} \t Save Idx: {self.save_idx} \t Faces 1: {face1_count} <=> {(round(float(face1_count / self.nbr_faces * 100), 6))} (%)"
-                f"\t Faces 3: {conv_face} <=> {(round(float(conv_rate*100), 6))} (%) \t in {self.time_total}s"
-            )
-
-            if self.time_measurement:
-                print(
-                    f"Raycast: {self.time_raycast} \t Render: {self.time_render} \t Visualize: {self.time_visualize}"
-                    f"\t Save: {self.time_save} \n Overall: {self.time_total}"
-                )
-
-            # update index
-            self.pt_idx += 1
-            self.save_idx += 1
-
-            if self.pt_idx >= self.nbr_points - 1:
-                self.conv_crit = False
-                print(
-                    f"All points have been sampled, currently {self.save_idx} points saved. If more points are "
-                    f"needed, increase the number of points per m2"
-                )
-
-        self.domains._end_save()
-        return
-
-    ##
-    # Helper Sample Points
-    ##
-
-    def _setup_cameras(self) -> None:
-        """Setup the cameras for the exploration."""
-        stage = omni.usd.get_context().get_stage()
-
-        # depth camera
-        if self._cfg_explorer.cam_depth_intrinsics is not None:
-            intrinscis = np.array(self._cfg_explorer.cam_depth_intrinsics).reshape(3, 3)
-            horizontalAperture = (
-                self._cfg_explorer.cam_depth_resolution[0]
-                * self._cfg_explorer.cam_depth_focal_length
-                / intrinscis[0, 0]
-            )
-        else:
-            horizontalAperture = self._cfg_explorer.cam_depth_aperture
-
-        depth_cam_prim = stage.DefinePrim(self._cfg_explorer.cam_depth_prim, "Camera")
-        depth_cam_prim.GetAttribute("focalLength").Set(self._cfg_explorer.cam_depth_focal_length)  # set focal length
-        depth_cam_prim.GetAttribute("clippingRange").Set(
-            self._cfg_explorer.cam_depth_clipping_range
-        )  # set clipping range
-        depth_cam_prim.GetAttribute("horizontalAperture").Set(horizontalAperture)  # set aperture
-
-        self.domains.register_camera(
-            depth_cam_prim,
-            self._cfg_explorer.cam_depth_resolution[0],
-            self._cfg_explorer.cam_depth_resolution[1],
-            depth=True,
-            visualization=self.debug,
-        )
-
-        # semantic and rgb camera
-        if self._cfg_explorer.cam_sem_intrinsics is not None:
-            intrinscis = np.array(self._cfg_explorer.cam_sem_intrinsics).reshape(3, 3)
-            horizontalAperture = (
-                self._cfg_explorer.cam_sem_resolution[0] * self._cfg_explorer.cam_sem_focal_length / intrinscis[0, 0]
-            )
-        else:
-            horizontalAperture = self._cfg_explorer.cam_sem_aperture
-
-        sem_cam_prim = stage.DefinePrim(self._cfg_explorer.cam_sem_prim, "Camera")
-        sem_cam_prim.GetAttribute("focalLength").Set(self._cfg_explorer.cam_sem_focal_length)  # set focal length
-        sem_cam_prim.GetAttribute("horizontalAperture").Set(horizontalAperture)  # set aperture
-        sem_cam_prim.GetAttribute("clippingRange").Set(self._cfg_explorer.cam_sem_clipping_range)  # set clipping range
-
-        if self._cfg_explorer.cam_sem_rgb:
-            lab_cam_cfg = PinholeCameraCfg(
-                width=self._cfg_explorer.cam_sem_resolution[0],
-                height=self._cfg_explorer.cam_sem_resolution[1],
-            )
-            lab_cam_cfg.usd_params.clipping_range = self._cfg_explorer.cam_sem_clipping_range
-            lab_cam_cfg.usd_params.focal_length = self._cfg_explorer.cam_sem_focal_length
-            lab_cam_cfg.usd_params.horizontal_aperture = horizontalAperture
-            lab_cam = Camera(lab_cam_cfg)
-            lab_cam.spawn(self._cfg_explorer.cam_sem_prim + "_rgb")
-            lab_cam.initialize()
-        else:
-            lab_cam = None
-
-        self.domains.register_camera(
-            sem_cam_prim,
-            self._cfg_explorer.cam_sem_resolution[0],
-            self._cfg_explorer.cam_sem_resolution[1],
-            semantics=True,
-            rgb=self._cfg_explorer.cam_sem_rgb,
-            visualization=self.debug,
-            omni_cam=lab_cam,
-        )
-        return
-
-    def _get_sample_points(self) -> None:
-        # get min, max of the mesh in the xy plane
-        x_min = self.domains.mesh.bounds[0][0]
-        x_max = self.domains.mesh.bounds[1][0]
-        y_min = self.domains.mesh.bounds[0][1]
-        y_max = self.domains.mesh.bounds[1][1]
-        max_area = (x_max - x_min) * (y_max - y_min)
-
-        # init sampler as qmc
-        sampler = qmc.Halton(d=2, scramble=False)
-        # determine number of samples to dram
-        nbr_points = int(max_area * self._cfg_explorer.points_per_m2)
-        # get raw samples origins
-        points = sampler.random(nbr_points)
-        points = qmc.scale(points, [x_min, y_min], [x_max, y_max])
-        heights = np.ones((nbr_points, 1)) * self._cfg_explorer.height
-        ray_origins = torch.from_numpy(np.hstack((points, heights)))
-        ray_origins = ray_origins.type(torch.float32)
-
-        # get ray directions in negative z direction
-        ray_directions = torch.zeros((nbr_points, 3), dtype=torch.float32)
-        ray_directions[:, 2] = -1.0
-
-        # raycast
-        ray_hits_world_down, _, _ = self.domains._raycast(
-            ray_origins * torch.tensor([1, 1, 2]),  # include objects above the robot
-            ray_directions,
-            cam_rot=torch.eye(3),
-            pix_offset=torch.zeros_like(ray_origins),
-        )
-
-        z_depth = torch.abs(ray_hits_world_down[:, 2] - ray_origins[:, 2] * 2)
-        # filter points outside the mesh and within walls
-        filter_inside_mesh = torch.isfinite(z_depth)  # outside mesh
-        filter_inside_mesh[
-            ray_hits_world_down[:, 2] < self._cfg_explorer.ground_height
-        ] = False  # above holes in the ground
-        print(f"filtered {nbr_points - filter_inside_mesh.sum()} points outside of mesh")
-        filter_outside_wall = z_depth > (self._cfg_explorer.min_height + ray_origins[:, 2])
-        print(f"filtered {nbr_points - filter_outside_wall.sum()} points inside wall")
-        filter_combined = torch.all(torch.stack((filter_inside_mesh, filter_outside_wall), dim=1), dim=1)
-        print(f"filtered total of {round(float((1 - filter_combined.sum() / nbr_points) * 100), 4)} % of points")
-
-        if self.debug:
-            import copy
-
-            import open3d as o3d
-
-            o3d_mesh = self.domains.mesh.as_open3d
-            o3d_mesh.compute_vertex_normals()
-            odom_vis_list = [o3d_mesh]
-
-            small_sphere = o3d.geometry.TriangleMesh.create_sphere(0.05)  # successful trajectory points
-
-            camera_centers = ray_origins.cpu().numpy()
-
-            for idx, camera_center in enumerate(camera_centers):
-                if filter_combined[idx]:
-                    small_sphere.paint_uniform_color([0.4, 1.0, 0.1])  # green
-                else:
-                    small_sphere.paint_uniform_color([1.0, 0.1, 0.1])  # red
-
-                odom_vis_list.append(
-                    copy.deepcopy(small_sphere).translate((camera_center[0], camera_center[1], camera_center[2]))
-                )
-
-            o3d.visualization.draw_geometries(odom_vis_list)
-
-        self.camera_centers = ray_origins[filter_combined].type(torch.float32)
-
-        # free gpu memory
-        ray_origins = filter_combined = filter_inside_mesh = filter_outside_wall = z_depth = ray_hits_world_down = None
-
-        # enforce a minimum distance to the walls
-        angles = np.linspace(-np.pi, np.pi, 20)
-        ray_directions = tf.Rotation.from_euler("z", angles, degrees=False).as_matrix() @ np.array([1, 0, 0])
-        ray_hit = []
-
-        for ray_direction in ray_directions:
-            ray_direction_torch = (
-                torch.from_numpy(ray_direction).repeat(self.camera_centers.shape[0], 1).type(torch.float32)
-            )
-            ray_hits_world, _, _ = self.domains._raycast(
-                self.camera_centers,
-                ray_direction_torch,
-                cam_rot=torch.eye(3),
-                pix_offset=torch.zeros_like(ray_direction_torch),
-            )
-            ray_hit.append(
-                torch.norm(ray_hits_world - self.camera_centers, dim=1) > self._cfg_explorer.min_wall_distance
-            )
-
-        # check if every point has the minimum distance in every direction
-        without_wall = torch.all(torch.vstack(ray_hit), dim=0)
-
-        if self.debug:
-            import copy
-
-            import open3d as o3d
-
-            o3d_mesh = self.domains.mesh.as_open3d
-            o3d_mesh.compute_vertex_normals()
-            odom_vis_list = [o3d_mesh]
-
-            small_sphere = o3d.geometry.TriangleMesh.create_sphere(0.05)  # successful trajectory points
-
-            camera_centers = self.camera_centers.cpu().numpy()
-
-            for idx, camera_center in enumerate(camera_centers):
-                if without_wall[idx]:
-                    small_sphere.paint_uniform_color([0.4, 1.0, 0.1])  # green
-                else:
-                    small_sphere.paint_uniform_color([1.0, 0.1, 0.1])  # red
-
-                odom_vis_list.append(
-                    copy.deepcopy(small_sphere).translate((camera_center[0], camera_center[1], camera_center[2]))
-                )
-
-            o3d.visualization.draw_geometries(odom_vis_list)
-
-        print(f"filtered {self.camera_centers.shape[0] - without_wall.sum()} points too close to walls")
-        self.camera_centers = self.camera_centers[without_wall].type(torch.float32)
-        self.nbr_points = self.camera_centers.shape[0]
-        return
-
-    def _construct_kdtree(self, num_neighbors: int = 50) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
-        # construct kdtree to find nearest neighbors of points
-        kdtree = KDTree(self.camera_centers.cpu().numpy())
-        _, nearest_neighbors_idx = kdtree.query(self.camera_centers.cpu().numpy(), k=num_neighbors + 1, workers=-1)
-
-        # remove first neighbor as it is the point itself
-        nearest_neighbors_idx = torch.tensor(nearest_neighbors_idx[:, 1:], dtype=torch.int64)
-
-        # define origin and neighbor points
-        origin_point = torch.repeat_interleave(self.camera_centers, repeats=num_neighbors, axis=0)
-        neighbor_points = self.camera_centers[nearest_neighbors_idx, :].reshape(-1, 3)
-        distance = torch.norm(origin_point - neighbor_points, dim=1)
-
-        # check for collision with raycasting
-        _, _, hit_depth = self.domains._raycast(
-            origin_point,
-            origin_point - neighbor_points,
-            cam_rot=torch.eye(3),
-            pix_offset=torch.zeros_like(origin_point),
-        )
-
-        hit_depth[torch.isnan(hit_depth)] = self.domains._cfg.max_depth
-        # filter connections that collide with the environment
-        collision = (hit_depth < distance).reshape(-1, num_neighbors)
-        return nearest_neighbors_idx, collision, distance
-
-    def _get_view_variations(self):
-        # the variation around the up axis (z-axis) has to be picked in order to avoid that the camera faces a wall
-        # done by construction a graph of all sample points and pick the z angle in order to point at one of the neighbors of the node
-
-        # get nearest neighbors and check for collision
-        nearest_neighbors_idx, collision, _ = self._construct_kdtree()
-
-        # remove nodes
-        all_collision_idx = torch.all(collision, dim=1)
-
-        # select neighbor with the largest distance that is not in collision
-        direction_neighbor_idx = torch.hstack(
-            [
-                (collision_single_node == False).nonzero().reshape(-1)[-1]
-                for collision_single_node in collision[~all_collision_idx, :]
-            ]
-        )
-        direction_neighbor_idx = torch.vstack(
-            (torch.arange(nearest_neighbors_idx.shape[0])[~all_collision_idx], direction_neighbor_idx)
-        ).T
-        selected_neighbor_idx = nearest_neighbors_idx[direction_neighbor_idx[:, 0], direction_neighbor_idx[:, 1]]
-
-        if self.debug:
-            import copy
-
-            import open3d as o3d
-
-            o3d_mesh = self.domains.mesh.as_open3d
-            o3d_mesh.compute_vertex_normals()
-            odom_vis_list = [o3d_mesh]
-
-            small_sphere = o3d.geometry.TriangleMesh.create_sphere(0.05)  # successful trajectory points
-
-            camera_centers = self.camera_centers[nearest_neighbors_idx[0]].cpu().numpy()
-
-            for idx, camera_center in enumerate(camera_centers):
-                if collision[0][idx]:  # in collision or nan
-                    small_sphere.paint_uniform_color([1.0, 0.4, 0.0])  # orange
-                elif idx == direction_neighbor_idx[0][1]:  # selected neighbor
-                    small_sphere.paint_uniform_color([0.0, 0.0, 1.0])  # blue
-                else:
-                    small_sphere.paint_uniform_color([0.1, 1.0, 0.1])  # green
-                odom_vis_list.append(
-                    copy.deepcopy(small_sphere).translate((camera_center[0], camera_center[1], camera_center[2]))
-                )
-
-            small_sphere.paint_uniform_color([1.0, 0.1, 0.1])  # red
-            odom_vis_list.append(
-                copy.deepcopy(small_sphere).translate(
-                    (
-                        self.camera_centers[0][0].cpu().numpy(),
-                        self.camera_centers[0][1].cpu().numpy(),
-                        self.camera_centers[0][2].cpu().numpy(),
-                    )
-                )
-            )
-
-            # check if selected neighbor idx is correct by plotting the neighbor again
-            small_sphere.paint_uniform_color([0.0, 0.0, 1.0])  # blue
-            neighbor = self.camera_centers[selected_neighbor_idx[0]].cpu().numpy()
-            odom_vis_list.append(copy.deepcopy(small_sphere).translate((neighbor[0], neighbor[1], neighbor[2])))
-
-            # draw line
-            line_set = o3d.geometry.LineSet(
-                o3d.utility.Vector3dVector(self.camera_centers.cpu().numpy()),
-                o3d.utility.Vector2iVector(np.array([[0, selected_neighbor_idx[0].cpu().numpy()]])),
-            )
-            line_set.colors = o3d.utility.Vector3dVector([[0.99, 0.99, 0.1]])
-            odom_vis_list.append(line_set)
-
-            o3d.visualization.draw_geometries(odom_vis_list)
-
-        # get the z angle of the neighbor that is closest to the origin point
-        neighbor_direction = self.camera_centers[~all_collision_idx, :] - self.camera_centers[selected_neighbor_idx, :]
-        self.z_angles = np.rad2deg(torch.atan2(neighbor_direction[:, 1], neighbor_direction[:, 0]).cpu().numpy())
-
-        if self.debug:
-            import copy
-
-            import open3d as o3d
-
-            o3d_mesh = self.domains.mesh.as_open3d
-            o3d_mesh.compute_vertex_normals()
-            odom_vis_list = [o3d_mesh]
-
-            small_sphere = o3d.geometry.TriangleMesh.create_sphere(0.05)  # successful trajectory points
-            small_sphere.paint_uniform_color([0.4, 1.0, 0.1])  # green
-
-            camera_centers = self.camera_centers.cpu().numpy()
-
-            for camera_center in camera_centers:
-                odom_vis_list.append(
-                    copy.deepcopy(small_sphere).translate((camera_center[0], camera_center[1], camera_center[2]))
-                )
-
-            colors = [[0.99, 0.99, 0.1] for i in range(len(camera_centers))]
-            neighbor_map = []
-            selected_neighbor_idx_counter = 0
-            for curr_center in range(self.camera_centers.shape[0]):
-                if not all_collision_idx[curr_center]:
-                    neighbor_map.append(
-                        [curr_center, selected_neighbor_idx[selected_neighbor_idx_counter].cpu().numpy()]
-                    )
-                    selected_neighbor_idx_counter += 1
-            line_set = o3d.geometry.LineSet(
-                o3d.utility.Vector3dVector(camera_centers), o3d.utility.Vector2iVector(np.array(neighbor_map))
-            )
-            line_set.colors = o3d.utility.Vector3dVector(colors)
-            odom_vis_list.append(line_set)
-            o3d.visualization.draw_geometries(odom_vis_list)
-
-        # filter points that have no neighbors that are not in collision and update number of points
-        self.camera_centers = self.camera_centers[~all_collision_idx, :]
-        self.nbr_points = self.camera_centers.shape[0]
-
-        # vary the rotation of the forward and horizontal axis (in camera frame) as a uniform distribution within the limits
-        self.x_angles = np.random.uniform(
-            self._cfg_explorer.x_angle_range[0], self._cfg_explorer.y_angle_range[1], self.nbr_points
-        )
-        self.y_angles = np.random.uniform(
-            self._cfg_explorer.y_angle_range[0], self._cfg_explorer.y_angle_range[1], self.nbr_points
-        )
-        return
-
-    def _setup_conv(self):
-        # index array
-        self.nbr_faces = len(self.domains.mesh.metadata["_ply_raw"]["face"]["data"])
-        self.face_idx = torch.zeros(self.nbr_faces, dtype=torch.int64)
-        return
-
-    # # save data helpers ###
-
-    # def _init_save(self, save_path: Optional[str] = None) -> None:
-    #     if save_path is not None:
-    #         self._cfg.save_path = save_path
-
-    #     # create directories
-    #     os.makedirs(self._cfg.save_path, exist_ok=True)
-    #     os.makedirs(os.path.join(self._cfg.save_path, "semantics"), exist_ok=True)
-    #     os.makedirs(os.path.join(self._cfg.save_path, "depth"), exist_ok=True)
-
-    #     # save camera configurations
-    #     intrinsics = np.zeros((len(self.cameras), 9))
-    #     for idx, curr_cam in enumerate(self.cameras):
-    #         intrinsics[idx] = curr_cam.data.intrinsic_matrices[0].cpu().numpy().flatten()
-    #     np.savetxt(os.path.join(self._cfg.save_path, "intrinsics.txt"), intrinsics, delimiter=",")
-
-    # def _save_data(self) -> None:
-    #     # TODO: use replicator writer, currently too slow
-    #     for camera in self.cameras:
-    #         suffix = f"_{camera.cfg.prim_path}"
-    #     cam_suffix = f"_cam{cam_idx}" if len(self.cameras) > 1 else ""
-
-    #     # SEMANTICS
-    #     if cam_data.semantic:
-    #         cv2.imwrite(
-    #             os.path.join(self._cfg.save_path, "semantics", f"{idx}".zfill(4) + cam_suffix + ".png"),
-    #             cv2.cvtColor(cam_data.render_sem.astype(np.uint8), cv2.COLOR_RGB2BGR),
-    #         )
-
-    #     # DEPTH
-    #     if cam_data.depth:
-    #         cv2.imwrite(
-    #             os.path.join(self._cfg.save_path, "depth", f"{idx}".zfill(4) + cam_suffix + ".png"),
-    #             cam_data.render_depth,
-    #         )
-
-    #     # camera pose in robotics frame (x forward, y left, z up)
-    #     rot_quat = tf.Rotation.from_matrix(cam_data.rot.cpu().numpy()).as_quat()  # get quat as (x, y, z, w) format
-    #     pose = np.hstack((cam_data.pos.cpu().numpy(), rot_quat))
-    #     cam_data.poses = np.append(cam_data.poses, pose.reshape(1, -1), axis=0)
-    #     return
-
-    # def _end_save(self) -> None:
-    #     # save camera poses
-    #     for idx, cam_data in enumerate(self.cameras):
-    #         np.savetxt(
-    #             os.path.join(self._cfg.save_path, f"camera_extrinsic_cam{idx}.txt"),
-    #             cam_data.poses[1:],
-    #             delimiter=",",
-    #         )
-    #     return
diff --git a/isaaclab/extension/omni.isaac.viplanner/intern/matterport_exploration/sampler_config.py b/isaaclab/extension/omni.isaac.viplanner/intern/matterport_exploration/sampler_config.py
deleted file mode 100644
index ef851ba..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/intern/matterport_exploration/sampler_config.py
+++ /dev/null
@@ -1,91 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""
-Config for Exploration/ Data Sampling in Matterport3D Dataset
-"""
-
-# python
-from dataclasses import dataclass
-from typing import Optional, Tuple
-
-import torch
-
-
-@dataclass
-class SamplerCfg:
-    points_per_m2: int = 20
-    """Number of random points per m2 of the mesh surface area."""
-    device = "cuda" if torch.cuda.is_available() else "cpu"
-    """Device to use for computations."""
-    height: float = 0.5
-    """Height to use for the random points."""
-    min_height: float = 0.2
-    """Maximum height to be considered an accessible point for the robot"""
-    ground_height: float = -0.1
-    """Height of the ground plane"""
-    min_wall_distance: float = 0.5
-    """Minimum distance to a wall to be considered an accessible point for the robot"""
-    x_angle_range: Tuple[float, float] = (-2.5, 2.5)
-    y_angle_range: Tuple[float, float] = (-2, 5)  # negative angle means in isaac convention: look down
-    # ANYmal D: (-2, 5)  <-> ANYmal C: (25, 35)
-    # NOTE: the angles should follow the isaac convention, i.e. x-forward, y-left, z-up
-    """Range of the x and y angle of the camera (in degrees), will be randomly selected according to a uniform distribution"""
-    min_hit_rate: float = 0.8
-    """Don't use a point if the hit rate is below this value"""
-    min_avg_hit_distance: float = 0.5
-    """Don't use a point if the max hit distance is below this value"""
-    min_std_hit_distance: float = 0.5
-    """Don't use a point if the std hit distance is below this value"""
-    conv_rate: float = 0.9
-    """Rate of faces that are covered by three different images, used to terminate the exploration"""
-
-    # DEPTH CAMERA
-    cam_depth_prim: str = "/cam_depth"
-    cam_depth_resolution: Tuple[int, int] = (848, 480)  # (width, height)
-    cam_depth_focal_length: float = 1.93  # in mm
-    # ANYmal D wide_angle_camera: 1.0 <-> ANYmal C realsense: 1.93 <-> RealSense D455: 1.93
-    cam_depth_clipping_range: Tuple[float, float] = (0.01, 1000.0)
-    cam_depth_aperture: float = 3.8  # in mm
-    cam_depth_intrinsics: Optional[Tuple[float]] = (430.31607, 0.0, 428.28408, 0.0, 430.31607, 244.00695, 0.0, 0.0, 1.0)
-    # ANYmal C/D: (423.54608, 0.0, 427.69815, 0.0, 423.54608, 240.17773, 0.0, 0.0, 1.0)
-    # RealSense D455: (430.31607, 0.0, 428.28408, 0.0, 430.31607, 244.00695, 0.0, 0.0, 1.0)
-    # NOTE: either provide the aperture or the camera matrix (if both, the camera matrix will be used)
-    """Depth camera configuration"""
-    tf_pos: tuple = (0.0, 0.0, 0.0)  # (translation in depth frame)
-    # ANYmal D: (-0.002, 0.025, 0.042)  <-> ANYmal C and RealSense D455: (0.0, 0.0, 0.0)
-    tf_quat: tuple = (0.0, 0.0, 0.0, 1.0)  # xyzw quaternion format (rotation in depth frame)
-    # ANYmal D: (0.001, 0.137, -0.000, 0.991)  <-> ANYmal C and RealSense D455: (0.0, 0.0, 0.0, 1.0)
-    tf_quat_convention: str = "roll-pitch-yaw"  # or "isaac"
-    # NOTE: if the quat follows the roll-pitch-yaw convention, i.e. x-forward, y-right, z-down, will be converted to the isaac convention
-    """transformation from depth (src) to semantic camera (target)"""
-
-    # SEMANTIC CAMERA
-    cam_sem_prim: str = "/cam_sem"
-    cam_sem_resolution: Tuple[int, int] = (1280, 720)
-    # ANYmal D wide_angle_camera: (1440, 1080)  <-> ANYmal C realsense (848, 480) <-> RealSense D455 (1280, 720)
-    cam_sem_focal_length: float = 1.93  # in mm (for ANYmal C100 - https://anymal-research.docs.anymal.com/user_manual/anymal_c100/release-23.02/documents/anymal_c_hardware_guide/main.html?highlight=wide+angle#achg-sssec-wide-angle-cameras)
-    # ANYmal D wide_angle_camera: 1.93  <-> ANYmal C realsense: 1.0 <-> RealSense D455: 1.93
-    cam_sem_clipping_range: Tuple[float, float] = (0.01, 1000.0)
-    cam_sem_aperture: float = 3.8  # in mm
-    cam_sem_intrinsics: Optional[Tuple[float]] = (644.15496, 0.0, 639.53125, 0.0, 643.49212, 366.30880, 0.0, 0.0, 1.0)
-    # ANYmal D wide_angle_camera:   (575.60504, 0.0, 745.73121, 0.0, 578.56484, 519.52070, 0.0, 0.0, 1.0)
-    # ANYmal C realsense:           (423.54608, 0.0, 427.69815, 0.0, 423.54608, 240.17773, 0.0, 0.0, 1.0)
-    # RealSense D455:               (644.15496, 0.0, 639.53125, 0.0, 643.49212, 366.30880, 0.0, 0.0, 1.0)
-    # NOTE: either provide the aperture or the camera matrix (if both, the camera matrix will be used)
-    """Semantic camera configuration"""
-    cam_sem_rgb: bool = True
-    """Whether to record rgb images or not"""
-
-    # SAVING
-    max_images: int = 2000
-    """Maximum number of images recorded"""
-    save_path: str = "/home/pascal/viplanner/imperative_learning/data"
-    suffix: Optional[str] = "cam_mount"
-    """Path to save the data to (directly with env name will be created)"""
-
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/intern/run_scripts/anymal_run_carla.py b/isaaclab/extension/omni.isaac.viplanner/intern/run_scripts/anymal_run_carla.py
deleted file mode 100644
index 2a64e8a..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/intern/run_scripts/anymal_run_carla.py
+++ /dev/null
@@ -1,221 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""
-Launch Omniverse Toolkit first.
-"""
-
-# python
-import argparse
-import json
-
-# omni-isaac-lab
-from omni.isaac.kit import SimulationApp
-
-# add argparse arguments
-parser = argparse.ArgumentParser("Welcome to lab: Omniverse Robotics Environments!")
-parser.add_argument("--headless", action="store_false", default=True, help="Force display off at all times.")
-args_cli = parser.parse_args()
-
-# launch omniverse app
-config = {"headless": args_cli.headless}
-launcher = SimulationApp(config)
-
-
-"""
-Rest everything follows.
-"""
-import os
-
-# python
-from typing import Tuple
-
-import numpy as np
-
-# isaac-core
-from omni.isaac.core.objects.ground_plane import GroundPlane
-from omni.isaac.core.utils import extensions
-
-# enable ROS bridge extension  --> otherwise rospy cannot be imported
-extensions.enable_extension("omni.isaac.ros_bridge")
-extensions.enable_extension("omni.kit.manipulator.viewport")
-
-# isaac-anymal
-from omni.isaac.anymal.config import (
-    ANYmalCfg,
-    ANYmalEvaluatorConfig,
-    SensorCfg,
-    TwistControllerCfg,
-    VIPlannerCfg,
-)
-from omni.isaac.anymal.viplanner.evaluator import ANYmallabEvaluator
-
-# lab-assets
-from omni.isaac.assets import ASSETS_RESOURCES_DIR
-
-# isaac-carla
-from omni.isaac.carla.configs import CarlaExplorerConfig, CarlaLoaderConfig
-from omni.isaac.carla.scripts import CarlaExplorer, CarlaLoader
-
-
-class ANYmalRunCarla(ANYmallabEvaluator):
-    def __init__(
-        self,
-        cfg: ANYmalEvaluatorConfig,
-        cfg_carla: CarlaLoaderConfig = CarlaLoaderConfig(),
-        cfg_explore: CarlaExplorerConfig = CarlaExplorerConfig(),
-        cfg_anymal: ANYmalCfg = ANYmalCfg(),
-        cfg_planner: VIPlannerCfg = VIPlannerCfg(),
-    ) -> None:
-        # configs
-        self._cfg_carla = cfg_carla
-        self._cfg_explore = cfg_explore
-        # run init
-        super().__init__(cfg, cfg_anymal, cfg_planner)
-        return
-
-    def load_scene(self) -> None:
-        print("Loading scene...")
-        if self._cfg_carla.groundplane:
-            self._cfg_carla.groundplane = False
-            self._groundplane = True
-        else:
-            self._groundplane = False
-        self._loader = CarlaLoader(self._cfg_carla)
-        self._loader.load()
-        print("DONE")
-        return
-
-    def explore_env(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
-        explorer: CarlaExplorer = CarlaExplorer(self._cfg_explore, self._cfg_carla)
-        explorer._get_cam_position()
-        nearest_neighor_idx, collision, distance = explorer._construct_kdtree(num_neighbors=self._cfg.num_connections)
-
-        if self._groundplane:
-            # add groundplane back
-            _ = GroundPlane(
-                "/World/GroundPlane", z_position=0.25, physics_material=self._loader.material, visible=False
-            )
-
-        return explorer.camera_positions, nearest_neighor_idx, collision, distance
-
-    def get_env_name(self) -> str:
-        return os.path.splitext(self._cfg_carla.usd_name)[0]
-
-    def _load_waypoints(self, scale: bool = False) -> None:
-        """
-        Expected that the waypoints have been recorded with the omni.isaac.waypoint extension and saved in .json format.
-        Structure of the json file:
-        {
-            start: [x, y, z],
-            end: [x, y, z],
-            waypoints: [[x, y, z], [x, y, z], ...]
-        }
-        """
-
-        if self._cfg.waypoint_file.endswith(".json"):
-            self.waypoints = json.load(open(self._cfg.waypoint_file))
-        else:
-            self.waypoints = json.load(open(self._cfg.waypoint_file + ".json"))
-
-        # apply scale
-        if scale:
-            self.waypoints["start"] = [x * self._cfg_carla.scale for x in self.waypoints["start"]]
-            self.waypoints["end"] = [x * self._cfg_carla.scale for x in self.waypoints["end"]]
-            self.waypoints["waypoints"] = [
-                [x * self._cfg_carla.scale for x in waypoint] for waypoint in self.waypoints["waypoints"]
-            ]
-
-        # draw waypoints
-        self.draw_interface.draw_points([self.waypoints["start"]], [(1.0, 0.4, 0.0, 1.0)], [(10)])  # orange
-        self.draw_interface.draw_points([self.waypoints["end"]], [(0.0, 1.0, 0.0, 1.0)], [(10)])  # green
-        self.draw_interface.draw_points(
-            self.waypoints["waypoints"],
-            [(0.0, 0.0, 1.0, 1.0)] * len(self.waypoints["waypoints"]),  # blue
-            [(10)] * len(self.waypoints["waypoints"]),
-        )
-
-        # attach end as further goal-point
-        self.waypoints["waypoints"].append(self.waypoints["end"])
-
-        return
-
-
-if __name__ == "__main__":
-    # configs
-    cfg = ANYmalEvaluatorConfig(
-        handcrafted_waypoint_file="crosswalk_paper_changed",  # "waypoints_carla_eval", # "crosswalk_paper_extended_3" "crosswalk_paper_extended_5"
-        cost_map_dir="/home/pascal/viplanner/imperative_learning/data/town01_cam_mount_train",  # use the map without people added !
-        cost_map_name="cost_map_sem_sharpend",
-        models=[
-            os.path.join(
-                ASSETS_RESOURCES_DIR,
-                "vip_models/plannernet_env2azQ1b91cZZ_ep100_inputDepSem_costSem_optimSGD_combi_more_data_neg05",
-            ),
-            os.path.join(
-                ASSETS_RESOURCES_DIR, "vip_models/plannernet_env2azQ1b91cZZ_ep100_inputDep_costSem_optimSGD_depth_carla"
-            ),
-        ],
-        multi_model=False,
-        num_pairs=500,
-        use_prev_results=False,
-        repeat_waypoints=5,
-    )
-    cfg_carla = CarlaLoaderConfig(
-        groundplane=True,
-    )
-    cfg_carla_explore = CarlaExplorerConfig(
-        nb_more_people=0,
-        max_cam_recordings=15000,
-        points_per_m2=0.3,
-    )
-    cfg_planner = VIPlannerCfg(
-        # model_dir=os.path.join(ASSETS_RESOURCES_DIR,"vip_models/plannernet_env2azQ1b91cZZ_ep100_inputDep_costSem_optimSGD_depth_carla"),
-        model_dir=os.path.join(
-            ASSETS_RESOURCES_DIR,
-            "vip_models/plannernet_env2azQ1b91cZZ_cam_mount_ep100_inputDepSem_costSem_optimSGD_new_cam_mount_combi_lossWidthMod_wgoal4.0_warehouse",
-            # "/home/pascal/viplanner/imperative_learning/code/iPlanner/iplanner/models",
-        ),
-        sem_origin="isaac",
-        twist_controller_cfg=TwistControllerCfg(
-            lookAheadDistance=1.2,
-        ),
-        use_mount_cam=True,
-        conv_dist=0.8,
-        viplanner=True,
-        # fear_threshold=1.0,
-    )
-    cfg_anymal = ANYmalCfg(
-        anymal_type=1,  # 0: ANYmal C, 1: ANYmal D
-        sensor=SensorCfg(
-            cam_front_rgb=False,
-            cam_front_depth=False,
-            cam_viplanner_rgb=True,
-            cam_viplanner_depth=True,
-        ),
-    )
-
-    # init class
-    run = ANYmalRunCarla(
-        cfg=cfg,
-        cfg_carla=cfg_carla,
-        cfg_explore=cfg_carla_explore,
-        cfg_anymal=cfg_anymal,
-        cfg_planner=cfg_planner,
-    )
-    run.setup()
-
-    if not cfg.multi_model and cfg.repeat_waypoints is None:
-        run.run_single()
-    elif not cfg.multi_model:
-        run.run_repeat()
-    else:
-        run.run_multi()
-
-    # Close the simulator
-    launcher.close()
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/intern/run_scripts/anymal_run_warehouse.py b/isaaclab/extension/omni.isaac.viplanner/intern/run_scripts/anymal_run_warehouse.py
deleted file mode 100644
index 2bf2e1c..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/intern/run_scripts/anymal_run_warehouse.py
+++ /dev/null
@@ -1,231 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""
-Launch Omniverse Toolkit first.
-"""
-
-# python
-import argparse
-import json
-
-# omni-isaac-lab
-from omni.isaac.kit import SimulationApp
-
-# add argparse arguments
-parser = argparse.ArgumentParser("Welcome to lab: Omniverse Robotics Environments!")
-parser.add_argument("--headless", action="store_false", default=True, help="Force display off at all times.")
-args_cli = parser.parse_args()
-
-# launch omniverse app
-config = {"headless": args_cli.headless}
-launcher = SimulationApp(config)
-
-
-"""
-Rest everything follows.
-"""
-import os
-
-# python
-from typing import Tuple
-
-import numpy as np
-
-# isaac-core
-from omni.isaac.core.objects.ground_plane import GroundPlane
-from omni.isaac.core.utils import extensions
-
-# enable ROS bridge extension  --> otherwise rospy cannot be imported
-extensions.enable_extension("omni.isaac.ros_bridge")
-extensions.enable_extension("omni.kit.manipulator.viewport")
-
-# isaac-anymal
-from omni.isaac.anymal.config import (
-    ANYmalCfg,
-    ANYmalEvaluatorConfig,
-    SensorCfg,
-    TwistControllerCfg,
-    VIPlannerCfg,
-)
-from omni.isaac.anymal.viplanner.evaluator import ANYmallabEvaluator
-
-# lab-assets
-from omni.isaac.assets import ASSETS_RESOURCES_DIR
-
-# isaac-carla
-from omni.isaac.carla.configs import DATA_DIR, CarlaExplorerConfig, CarlaLoaderConfig
-from omni.isaac.carla.scripts import CarlaExplorer, CarlaLoader
-
-
-class ANYmalRunCarla(ANYmallabEvaluator):
-    def __init__(
-        self,
-        cfg: ANYmalEvaluatorConfig,
-        cfg_carla: CarlaLoaderConfig = CarlaLoaderConfig(),
-        cfg_explore: CarlaExplorerConfig = CarlaExplorerConfig(),
-        cfg_anymal: ANYmalCfg = ANYmalCfg(),
-        cfg_planner: VIPlannerCfg = VIPlannerCfg(),
-    ) -> None:
-        # configs
-        self._cfg_carla = cfg_carla
-        self._cfg_explore = cfg_explore
-        # run init
-        super().__init__(cfg, cfg_anymal, cfg_planner)
-        return
-
-    def load_scene(self) -> None:
-        print("Loading scene...")
-        if self._cfg_carla.groundplane:
-            self._cfg_carla.groundplane = False
-            self._groundplane = True
-        else:
-            self._groundplane = False
-        self._loader = CarlaLoader(self._cfg_carla)
-        self._loader.load()
-        print("DONE")
-        return
-
-    def explore_env(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
-        explorer: CarlaExplorer = CarlaExplorer(self._cfg_explore, self._cfg_carla)
-        explorer._get_cam_position()
-        nearest_neighor_idx, collision, distance = explorer._construct_kdtree(num_neighbors=self._cfg.num_connections)
-
-        if self._groundplane:
-            # add groundplane back
-            _ = GroundPlane(
-                "/World/GroundPlane", z_position=0.25, physics_material=self._loader.material, visible=False
-            )
-
-        return explorer.camera_positions, nearest_neighor_idx, collision, distance
-
-    def get_env_name(self) -> str:
-        return os.path.splitext(self._cfg_carla.usd_name)[0]
-
-    def _load_waypoints(self) -> None:
-        """
-        Expected that the waypoints have been recorded with the omni.isaac.waypoint extension and saved in .json format.
-        Structure of the json file:
-        {
-            start: [x, y, z],
-            end: [x, y, z],
-            waypoints: [[x, y, z], [x, y, z], ...]
-        }
-        """
-
-        if self._cfg.waypoint_file.endswith(".json"):
-            self.waypoints = json.load(open(self._cfg.waypoint_file))
-        else:
-            self.waypoints = json.load(open(self._cfg.waypoint_file + ".json"))
-
-        # apply scale
-        self.waypoints["start"] = [x * self._cfg_carla.scale for x in self.waypoints["start"]]
-        self.waypoints["end"] = [x * self._cfg_carla.scale for x in self.waypoints["end"]]
-        self.waypoints["waypoints"] = [
-            [x * self._cfg_carla.scale for x in waypoint] for waypoint in self.waypoints["waypoints"]
-        ]
-
-        # draw waypoints
-        self.draw_interface.draw_points([self.waypoints["start"]], [(1.0, 0.4, 0.0, 1.0)], [(10)])  # orange
-        self.draw_interface.draw_points([self.waypoints["end"]], [(0.0, 1.0, 0.0, 1.0)], [(10)])  # green
-        self.draw_interface.draw_points(
-            self.waypoints["waypoints"],
-            [(0.0, 0.0, 1.0, 1.0)] * len(self.waypoints["waypoints"]),  # blue
-            [(10)] * len(self.waypoints["waypoints"]),
-        )
-
-        # attach end as further goal-point
-        self.waypoints["waypoints"].append(self.waypoints["end"])
-
-        return
-
-
-if __name__ == "__main__":
-    # configs
-    cfg = ANYmalEvaluatorConfig(
-        handcrafted_waypoint_file=None,  # "warehouse_paper",  #
-        cost_map_dir="/home/pascal/viplanner/imperative_learning/data/warehouse_multiple_shelves_without_ppl_ext_sem_space",  #
-        cost_map_name="cost_map_sem",
-        models=[
-            os.path.join(
-                ASSETS_RESOURCES_DIR,
-                "vip_models/plannernet_env2azQ1b91cZZ_ep100_inputDepSem_costSem_optimSGD_combi_more_data_neg05",
-            ),
-            os.path.join(
-                ASSETS_RESOURCES_DIR, "vip_models/plannernet_env2azQ1b91cZZ_ep100_inputDep_costSem_optimSGD_depth_carla"
-            ),
-        ],
-        multi_model=False,
-        num_pairs=500,
-        use_prev_results=True,
-        repeat_waypoints=None,
-    )
-    cfg_carla = CarlaLoaderConfig(
-        root_path="/home/pascal/viplanner/env/warehouse",
-        usd_name="warehouse_multiple_shelves_without_ppl_ext_sem_space.usd",  #
-        suffix="",
-        prim_path="/World/Warehouse",
-        scale=1.0,
-        axis_up="Z",
-        cw_config_file=None,
-        sem_mesh_to_class_map=os.path.join(DATA_DIR, "warehouse", "keyword_mapping.yml"),
-        groundplane=False,
-        people_config_file=os.path.join(DATA_DIR, "warehouse", "people_cfg.yml"),
-        vehicle_config_file=None,
-    )
-    cfg_carla_explore = CarlaExplorerConfig(
-        nb_more_people=None,
-        max_cam_recordings=100,
-        points_per_m2=0.3,
-        space_limiter="SM_WallA",
-        carla_filter=None,
-        indoor_filter=False,
-    )
-    cfg_planner = VIPlannerCfg(
-        model_dir="/home/pascal/viplanner/imperative_learning/code/iPlanner/iplanner/models",
-        sem_origin="isaac",
-        twist_controller_cfg=TwistControllerCfg(
-            lookAheadDistance=1.2,
-            stopDisThre=0.2,
-        ),
-        use_mount_cam=True,
-        conv_dist=0.8,
-        viplanner=False,
-    )
-    cfg_anymal = ANYmalCfg(
-        anymal_type=1,  # 0: ANYmal C, 1: ANYmal D
-        sensor=SensorCfg(
-            cam_front_rgb=False,
-            cam_front_depth=False,
-            cam_viplanner_rgb=True,
-            cam_viplanner_depth=True,
-        ),
-        rec_path=True,
-        rec_sensor=False,
-        follow_camera=True,
-    )
-
-    # init class
-    run = ANYmalRunCarla(
-        cfg=cfg,
-        cfg_carla=cfg_carla,
-        cfg_explore=cfg_carla_explore,
-        cfg_anymal=cfg_anymal,
-        cfg_planner=cfg_planner,
-    )
-    run.setup()
-
-    if not cfg.multi_model and cfg.repeat_waypoints is None:
-        run.run_single()
-    elif not cfg.multi_model:
-        run.run_repeat()
-    else:
-        run.run_multi()
-
-    # Close the simulator
-    launcher.close()
-
-# EoF
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/__init__.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/__init__.py
deleted file mode 100644
index fa55fbf..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/__init__.py
+++ /dev/null
@@ -1,26 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# from .carla_cfg import ViPlannerCarlaCfg
-# from .matterport_cfg import ViPlannerMatterportCfg
-# from .warehouse_cfg import ViPlannerWarehouseCfg
-# from .custom_cfg import CustomSceneCfg
-
-# from .obj_nav_cfg import ObjNavSceneCfg
-# from .train_obj_nav_cfg import TrainObjNavSceneCfg, AnymalCObjNavPPORunnerCfg
-# from .train_obj_nav_low_cfg import TrainObjNavLowSceneCfg, AnymalCFlatPPORunnerCfg
-# from .warehouse_objnav_cfg import WarehouseObjnavCfg
-# from .custom_objnav_cfg import CustomObjNavCfg
-# from .test_obj_nav_matterport_cfg import TestObjNavMatterportCfg
-# from .train_h1_low_cfg import H1RoughCfg, H1RoughPPORunnerCfg
-
-# from .h1_matterport_deploy_cfg import H1MatterportDeployCfg
-# from .h1_matterport_deploy_gpt_cfg import H1MatterportDeployGPTCfg
-
-from .h1 import *
-from .g1 import *
-from .go1 import *
-from .go2 import *
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/base_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/base_cfg.py
deleted file mode 100644
index cb5cdd3..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/base_cfg.py
+++ /dev/null
@@ -1,200 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RandomizationTermCfg as RandTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.utils import configclass
-# from omni.isaac.lab.utils.assets import LOCAL_ISAAC_DIR
-from omni.isaac.lab.utils.assets import ISAAC_NUCLEUS_DIR
-
-##
-# MDP settings
-##
-
-
-@configclass
-class RewardsCfg:
-    pass
-
-
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    paths = mdp.NavigationActionCfg(
-        asset_name="robot",
-        low_level_decimation=4,
-        low_level_action=mdp.JointPositionActionCfg(
-            asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True
-        ),
-        low_level_policy_file=os.path.join(ISAAC_NUCLEUS_DIR, "Policies", "ANYmal-C", "policy.pt"),
-    )
-
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel)
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "vel_command"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel)
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel)
-        actions = ObsTerm(func=mdp.low_level_actions)
-        height_scan = ObsTerm(
-            func=mdp.height_scan,
-            params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-            clip=(-1.0, 1.0),
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    @configclass
-    class PlannerImageCfg(ObsGroup):
-        depth_measurement = ObsTerm(
-            func=mdp.isaac_camera_data,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera"), "data_type": "distance_to_image_plane"},
-        )
-        semantic_measurement = ObsTerm(
-            func=mdp.isaac_camera_data,
-            params={"sensor_cfg": SceneEntityCfg("semantic_camera"), "data_type": "rgb"},
-        )
-
-        def __post_init__(self):
-            self.concatenate_terms = False
-            self.enable_corruption = False
-
-    @configclass
-    class PlannerTransformCfg(ObsGroup):
-        cam_position = ObsTerm(
-            func=mdp.cam_position,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-        cam_orientation = ObsTerm(
-            func=mdp.cam_orientation,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-        cam_orientation_ros = ObsTerm(
-            func=mdp.cam_orientation_ros,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-        cam_int_matrix = ObsTerm(
-            func=mdp.cam_int_matrix,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-
-        def __post_init__(self):
-            self.concatenate_terms = False
-            self.enable_corruption = False
-
-    # observation groups
-    low_level_policy: PolicyCfg = PolicyCfg()
-    planner_image: PlannerImageCfg = PlannerImageCfg()
-    planner_transform: PlannerTransformCfg = PlannerTransformCfg()
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="base"), "threshold": 1.0},
-    )
-
-
-@configclass
-class RandomizationCfg:
-    """Configuration for randomization."""
-
-    reset_base = RandTerm(
-        func=mdp.reset_root_state_uniform,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot"),
-            "pose_range": {"x": (0, 0), "y": (0, 0), "yaw": (-1.57, -1.57)},
-            "velocity_range": {
-                "x": (0.0, 0.0),
-                "y": (0.0, 0.0),
-                "z": (0.0, 0.0),
-                "roll": (0.0, 0.0),
-                "pitch": (0.0, 0.0),
-                "yaw": (0.0, 0.0),
-            },
-        },
-    )
-
-    reset_robot_joints = RandTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot"),
-            "position_range": (0.0, 0.0),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    vel_command: mdp.PathFollowerCommandGeneratorCfg = mdp.PathFollowerCommandGeneratorCfg(
-        robot_attr="robot",
-        lookAheadDistance=1.0,
-        debug_vis=True,
-    )
-
-
-##
-# Environment configuration
-##
-
-
-@configclass
-class ViPlannerBaseCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # managers
-    terminations: TerminationsCfg = TerminationsCfg()
-    randomization: RandomizationCfg = RandomizationCfg()
-    rewards: RewardsCfg = RewardsCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 20  # 10 Hz
-        self.episode_length_s = 60.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        self.scene.height_scanner.update_period = 4 * self.sim.dt  # should we low-level decimation
-        self.scene.contact_forces.update_period = self.sim.dt
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/carla_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/carla_cfg.py
deleted file mode 100644
index ef5b38d..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/carla_cfg.py
+++ /dev/null
@@ -1,112 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import AssetBaseCfg
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import CameraCfg, ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.lab.utils import configclass
-from omni.isaac.viplanner.utils import UnRealImporterCfg
-
-##
-# Pre-defined configs
-##
-# isort: off
-from omni.isaac.lab_assets.anymal import ANYMAL_C_CFG
-from .base_cfg import ViPlannerBaseCfg
-from ..viplanner import DATA_DIR
-
-##
-# Scene definition
-##
-
-
-@configclass
-class TerrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = UnRealImporterCfg(
-        prim_path="/World/Carla",
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        usd_path="/home/yji/Biped/biped_vision/viplanner/assets/carla.usd",
-        groundplane=True,
-        cw_config_file=os.path.join(DATA_DIR, "town01", "cw_multiply_cfg.yml"),
-        sem_mesh_to_class_map=os.path.join(DATA_DIR, "town01", "keyword_mapping.yml"),
-        people_config_file=os.path.join(DATA_DIR, "town01", "people_cfg.yml"),
-        vehicle_config_file=os.path.join(DATA_DIR, "town01", "vehicle_cfg.yml"),
-        axis_up="Z",
-    )
-
-    # robots
-    robot = ANYMAL_C_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot.init_state.pos = (8.0, -0.5, 0.6)
-    robot.init_state.rot = (0.5253, 0.0, 0.0, 0.8509)
-
-    # sensors
-    height_scanner = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.5)),
-        attach_yaw_only=True,
-        pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-        debug_vis=True,
-        mesh_prim_paths=["/World/GroundPlane"],
-    )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    # camera
-    depth_camera = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base/depth_camera",
-        offset=CameraCfg.OffsetCfg(pos=(0.510, 0.0, 0.015), rot=(-0.5, 0.5, -0.5, 0.5)),
-        spawn=sim_utils.PinholeCameraCfg(),
-        width=848,
-        height=480,
-        data_types=["distance_to_image_plane"],
-    )
-    semantic_camera = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base/semantic_camera",
-        offset=CameraCfg.OffsetCfg(pos=(0.510, 0.0, 0.015), rot=(-0.5, 0.5, -0.5, 0.5)),
-        spawn=sim_utils.PinholeCameraCfg(),
-        width=1280,
-        height=720,
-        data_types=["semantic_segmentation"],
-    )
-
-
-##
-# Environment configuration
-##
-
-
-@configclass
-class ViPlannerCarlaCfg(ViPlannerBaseCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Scene settings
-    scene: TerrainSceneCfg = TerrainSceneCfg(num_envs=1, env_spacing=1.0, replicate_physics=False)
-
-    def __post_init__(self):
-        """Post initialization."""
-        super().__post_init__()
-        # adapt viewer
-        self.viewer.eye = (105, -132, 6.5)
-        self.viewer.lookat = (113.5, -132, 1.0)
-        # change ANYmal position
-        self.scene.robot.init_state.pos = (118.0, -126.0, 1.0)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/custom_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/custom_cfg.py
deleted file mode 100644
index 7d4dd85..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/custom_cfg.py
+++ /dev/null
@@ -1,121 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import AssetBaseCfg
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import CameraCfg, ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.lab.utils import configclass
-from omni.isaac.viplanner.utils import UnRealImporterCfg
-
-from ..viplanner import DATA_DIR
-from .base_cfg import ViPlannerBaseCfg
-
-##
-# Pre-defined configs
-##
-# isort: off
-from omni.isaac.lab_assets.anymal import ANYMAL_C_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_H1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_G1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_GO2_CFG
-
-##
-# Scene definition
-##
-
-
-@configclass
-class TerrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = UnRealImporterCfg(
-        prim_path="/World/Custom",
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        # usd_path="/home/yji/Biped/biped_vision/viplanner/assets/warehouse_new.usd",
-        # usd_path="/home/yji/Biped/biped_vision/viplanner/assets/bowl_actual.usd",
-        usd_path="/home/yji/Biped/biped_vision/viplanner/assets/bowl_toy_w_plane.usd",
-        # usd_path="/home/yji/Biped/biped_vision/viplanner/assets/matterport/raw_data/v1/scans/17DRP5sb8fy/matterport_mesh/17DRP5sb8fy/matterport_mesh/bed1a77d92d64f5cbbaaae4feed64ec1/matterport.usd",
-        groundplane=True,
-        sem_mesh_to_class_map=None,
-        people_config_file=None,
-        axis_up="Z",
-    )
-
-    # robots
-    robot = ANYMAL_C_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_H1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_G1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_GO2_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot.init_state.pos = (1.0, 1.6, 1.1)
-    robot.init_state.rot = (0.7, 0.0, 0.0, 0.7)
-
-    # sensors
-    height_scanner = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.5)),
-        attach_yaw_only=True,
-        pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-        debug_vis=True,
-        mesh_prim_paths=["/World/Custom"],
-        # mesh_prim_paths=["/home/yji/Biped/biped_vision/viplanner/assets/bowl_toy_w_plane.ply"],
-    )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    # camera
-    depth_camera = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base/depth_camera",
-        offset=CameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.5), rot=(-0.5, 0.5, -0.5, 0.5)),
-        spawn=sim_utils.PinholeCameraCfg(),
-        width=25,
-        height=15,
-        data_types=["distance_to_image_plane"],
-        # mesh_prim_paths=["/home/yji/Biped/biped_vision/viplanner/assets/bowl_toy_w_plane.ply"]
-    )
-    semantic_camera = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base/semantic_camera",
-        offset=CameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.5), rot=(-0.5, 0.5, -0.5, 0.5)),
-        spawn=sim_utils.PinholeCameraCfg(),
-        width=848,
-        height=480,
-        data_types=["rgb"],
-        # mesh_prim_paths=["/home/yji/Biped/biped_vision/viplanner/assets/bowl_toy_w_plane.ply"]
-    )
-
-
-##
-# Environment configuration
-##
-
-
-@configclass
-class CustomSceneCfg(ViPlannerBaseCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Scene settings
-    scene: TerrainSceneCfg = TerrainSceneCfg(num_envs=1, env_spacing=1.0)
-
-    def __post_init__(self):
-        """Post initialization."""
-        super().__post_init__()
-        # adapt viewer
-        self.viewer.eye = (5, 12, 5)
-        self.viewer.lookat = (5, 0, 0.0)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/custom_objnav_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/custom_objnav_cfg.py
deleted file mode 100644
index 78919e7..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/custom_objnav_cfg.py
+++ /dev/null
@@ -1,376 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.utils import configclass
-# from omni.isaac.lab.utils.assets import LOCAL_ISAAC_DIR
-from omni.isaac.lab.utils.assets import ISAACLAB_NUCLEUS_DIR
-from ..viplanner import DATA_DIR
-
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import AssetBaseCfg
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import CameraCfg, ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.matterport.domains import MatterportRayCasterCfg, MatterportRayCasterCamera, MatterportRayCasterCfg
-from omni.isaac.viplanner.utils import UnRealImporterCfg, MatterportRayCasterCameraCfg
-
-
-from omni.isaac.lab_assets.anymal import ANYMAL_C_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_H1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_G1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_GO2_CFG
-
-##
-# MDP settings
-##
-
-@configclass
-class RewardsCfg:
-    pass
-
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    paths = mdp.NavigationActionCfg(
-        asset_name="robot",
-        low_level_decimation=4,
-        low_level_action=mdp.JointPositionActionCfg(
-            asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True
-        ),
-        low_level_policy_file=os.path.join(LOCAL_ISAAC_DIR, "low_level_policy", "policy.jit"),
-        image_size=(58, 87),
-    )
-
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel)
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "lowlevel_command"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel)
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel)
-        actions = ObsTerm(func=mdp.low_level_actions)
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    @configclass
-    class MidLevelPlannerCfg(ObsGroup):
-        """Observations for the mid-level planner."""
-
-        # observation terms (order preserved)
-        base_lin_accel = ObsTerm(func=mdp.base_lin_acc, params={"asset_cfg": SceneEntityCfg("robot", body_names="base")})
-        base_ang_accel = ObsTerm(func=mdp.base_ang_acc, params={"asset_cfg": SceneEntityCfg("robot", body_names="base")})
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        goal_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "midlevel_command"})
-        last_mid_actions = ObsTerm(func=mdp.last_mid_actions)
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-        depth_measurement = ObsTerm(
-            func=mdp.process_depth_image,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera_raycast"), "data_type": "distance_to_image_plane"},
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    @configclass
-    class PlannerImageCfg(ObsGroup):
-        depth_measurement = ObsTerm(
-            func=mdp.isaac_camera_data,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera"), "data_type": "distance_to_image_plane"},
-        )
-        semantic_measurement = ObsTerm(
-            func=mdp.isaac_camera_data,
-            params={"sensor_cfg": SceneEntityCfg("semantic_camera"), "data_type": "rgb"},
-        )
-
-        def __post_init__(self):
-            self.concatenate_terms = False
-            self.enable_corruption = False
-
-    @configclass
-    class PlannerTransformCfg(ObsGroup):
-        cam_position = ObsTerm(
-            func=mdp.cam_position,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-        cam_orientation = ObsTerm(
-            func=mdp.cam_orientation,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-        cam_orientation_ros = ObsTerm(
-            func=mdp.cam_orientation_ros,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-        cam_int_matrix = ObsTerm(
-            func=mdp.cam_int_matrix,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-
-        def __post_init__(self):
-            self.concatenate_terms = False
-            self.enable_corruption = False
-
-    
-
-    # observation groups
-    low_level_policy: PolicyCfg = PolicyCfg()
-    mid_level_planner: MidLevelPlannerCfg = MidLevelPlannerCfg()
-    planner_image: PlannerImageCfg = PlannerImageCfg()
-    planner_transform: PlannerTransformCfg = PlannerTransformCfg()
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="base"), "threshold": 1.0},
-    )
-
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    # startup
-    physics_material = EventTerm(
-        func=mdp.randomize_rigid_body_material,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-            "static_friction_range": (0.8, 0.8),
-            "dynamic_friction_range": (0.6, 0.6),
-            "restitution_range": (0.0, 0.0),
-            "num_buckets": 64,
-        },
-    )
-
-    add_base_mass = EventTerm(
-        func=mdp.randomize_rigid_body_mass,
-        mode="startup",
-        params={"asset_cfg": SceneEntityCfg("robot", body_names="base"), "mass_range": (-0.0, 0.0), "operation": "add"},
-    )
-
-    # reset
-    base_external_force_torque = EventTerm(
-        func=mdp.apply_external_force_torque,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="base"),
-            "force_range": (0.0, 0.0),
-            "torque_range": (-0.0, 0.0),
-        },
-    )
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_uniform,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-1.57, -1.57)},
-            "velocity_range": {
-                "x": (-0.0, 0.0),
-                "y": (-0.0, 0.0),
-                "z": (-0.0, 0.0),
-                "roll": (-0.0, 0.0),
-                "pitch": (-0.0, 0.0),
-                "yaw": (-0.0, 0.0),
-            },
-        },
-    )
-
-    reset_robot_joints = EventTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "position_range": (0.0, 0.0),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-    # interval
-    push_robot = EventTerm(
-        func=mdp.push_by_setting_velocity,
-        mode="interval",
-        interval_range_s=(10.0, 15.0),
-        params={"velocity_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0)}},
-    )
-
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    midlevel_command: mdp.MidLevelCommandGeneratorCfg = mdp.MidLevelCommandGeneratorCfg(
-        robot_attr="robot",
-        debug_vis=True,
-    )
-    lowlevel_command: mdp.LowLevelCommandGeneratorCfg = mdp.LowLevelCommandGeneratorCfg(
-        robot_attr="robot",
-        debug_vis=True,
-    )
-
-@configclass
-class TerrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = UnRealImporterCfg(
-        prim_path="/World/Custom",
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        usd_path="/home/yji/Biped/biped_vision/viplanner/assets/bowl_toy_origin.usd",
-        # usd_path="/home/yji/Biped/biped_vision/viplanner/assets/Bowl_bleach.usdz",
-        # usd_path="/home/yji/Biped/biped_vision/viplanner/assets/bowl_toy_w_plane.usd",
-        # usd_path="/home/yji/Biped/biped_vision/viplanner/assets/matterport/raw_data/v1/scans/17DRP5sb8fy/matterport_mesh/17DRP5sb8fy/matterport_mesh/bed1a77d92d64f5cbbaaae4feed64ec1/matterport.usd",
-        groundplane=True,
-        # sem_mesh_to_class_map=None,
-        # mesh_prim_paths=["/home/yji/Biped/biped_vision/viplanner/assets/bowl_toy_origin.ply"],
-        sem_mesh_to_class_map=None,
-        people_config_file=None,
-        axis_up="Z",
-    )
-
-    
-
-    # robots
-    robot = ANYMAL_C_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_H1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_G1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_GO2_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot.init_state.pos = (1.0, -1.6, 0.6)
-    robot.init_state.rot = (0.7, 0.0, 0.0, -0.7)
-
-    # sensors
-    height_scanner = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.5)),
-        attach_yaw_only=True,
-        pattern_cfg=patterns.GridPatternCfg(resolution=0.2, size=[3.0, 2.0]),
-        debug_vis=True,
-        mesh_prim_paths=["/World/Custom"],
-    )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    # camera
-    semantic_camera = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base/semantic_camera",
-        offset=CameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.5), rot=(-0.5, 0.5, -0.5, 0.5)),
-        spawn=sim_utils.PinholeCameraCfg(),
-        width=848,
-        height=480,
-        data_types=["rgb"],
-    )
-    
-    depth_camera = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base/depth_camera",
-        offset=CameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.5), rot=(-0.5, 0.5, -0.5, 0.5)),
-        spawn=sim_utils.PinholeCameraCfg(),
-        width=848,
-        height=480,
-        data_types=["distance_to_image_plane"],
-    )
-    # depth_camera_resized = CameraCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/base/depth_camera_resized",
-    #     offset=CameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(-0.5, 0.5, -0.5, 0.5)),
-    #     spawn=sim_utils.PinholeCameraCfg(),
-    #     width=25,
-    #     height=15,
-    #     data_types=["distance_to_image_plane"],
-    # )
-    depth_camera_raycast = MatterportRayCasterCameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        # mesh_prim_paths=["/home/yji/Biped/biped_vision/viplanner/assets/bowl_toy_origin.ply"],
-        # mesh_prim_paths=["/World/Custom"],
-        update_period=0.1,
-        offset=MatterportRayCasterCameraCfg.OffsetCfg(pos=(0.60, 0.0, 0.0), rot=(-0.5, 0.5, -0.5, 0.5)),
-        data_types=["distance_to_image_plane"],
-        debug_vis=False,
-        pattern_cfg=patterns.PinholeCameraPatternCfg(
-            focal_length=24.0,
-            horizontal_aperture=46,
-            height=15,
-            width=25,
-        ),
-        mesh_prim_paths=["/home/yji/Biped/biped_vision/viplanner/assets/bowl_toy_origin.ply"],
-    )
-    
-
-##
-# Environment configuration
-##
-
-@configclass
-class CustomObjNavCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # managers
-    terminations: TerminationsCfg = TerminationsCfg()
-    events: EventCfg = EventCfg()
-    rewards: RewardsCfg = RewardsCfg()
-    scene: TerrainSceneCfg = TerrainSceneCfg(num_envs=1, env_spacing=1.0)
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 20  # 10 Hz
-        self.episode_length_s = 60.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        self.scene.height_scanner.update_period = 4 * self.sim.dt  # should we low-level decimation
-        self.scene.contact_forces.update_period = self.sim.dt
-
-        self.viewer.eye = (5, 12, 5)
-        self.viewer.lookat = (5, 0, 0.0)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/g1/__init__.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/g1/__init__.py
deleted file mode 100644
index 917ca0e..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/g1/__init__.py
+++ /dev/null
@@ -1,52 +0,0 @@
-import gymnasium as gym
-
-from .g1_low_base_cfg import G1BaseRoughEnvCfg, G1BaseRoughEnvCfg_PLAY, G1RoughPPORunnerCfg
-from .g1_low_vision_cfg import G1VisionRoughEnvCfg, G1VisionRoughEnvCfg_PLAY, G1VisionRoughPPORunnerCfg
-
-##
-# Register Gym environments.
-##
-
-
-gym.register(
-    id="g1_base",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": G1BaseRoughEnvCfg,
-        "rsl_rl_cfg_entry_point": G1RoughPPORunnerCfg,
-    },
-)
-
-
-gym.register(
-    id="g1_base_play",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": G1BaseRoughEnvCfg,
-        "rsl_rl_cfg_entry_point": G1RoughPPORunnerCfg,
-    },
-)
-
-
-gym.register(
-    id="g1_vision",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": G1VisionRoughEnvCfg,
-        "rsl_rl_cfg_entry_point": G1VisionRoughPPORunnerCfg,
-    },
-)
-
-
-gym.register(
-    id="g1_vision_play",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": G1VisionRoughEnvCfg_PLAY,
-        "rsl_rl_cfg_entry_point": G1VisionRoughPPORunnerCfg,
-    },
-)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/g1/g1_low_base_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/g1/g1_low_base_cfg.py
deleted file mode 100644
index a41bcb4..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/g1/g1_low_base_cfg.py
+++ /dev/null
@@ -1,563 +0,0 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import math
-from dataclasses import MISSING
-
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import ArticulationCfg, AssetBaseCfg
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import CurriculumTermCfg as CurrTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg, FlatPatchSamplingCfg
-import omni.isaac.lab.terrains as terrain_gen
-from omni.isaac.lab.utils import configclass
-from omni.isaac.lab.utils.assets import ISAAC_NUCLEUS_DIR, ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.utils.noise import AdditiveUniformNoiseCfg as Unoise
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg, patterns
-from omni.isaac.lab.assets.articulation import ArticulationCfg
-from omni.isaac.lab.actuators import ImplicitActuatorCfg
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import ActionsCfg, CurriculumCfg, RewardsCfg, EventCfg, LocomotionVelocityRoughEnvCfg
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.config.g1.rough_env_cfg import G1Rewards
-
-##
-# Pre-defined configs
-##
-from omni.isaac.lab_assets import G1_MINIMAL_CFG
-
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-
-
-@configclass
-class G1RoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 32
-    max_iterations = 3000
-    save_interval = 500
-    experiment_name = "g1_base_rough"
-    empirical_normalization = False
-    policy = RslRlPpoActorCriticCfg(
-        init_noise_std=1.0,
-        actor_hidden_dims=[512, 256, 128],
-        critic_hidden_dims=[512, 256, 128],
-        activation="elu",
-    )
-    algorithm = RslRlPpoAlgorithmCfg(
-        value_loss_coef=1.0,
-        use_clipped_value_loss=True,
-        clip_param=0.2,
-        entropy_coef=0.008,
-        num_learning_epochs=5,
-        num_mini_batches=4,
-        learning_rate=1.0e-3,
-        schedule="adaptive",
-        gamma=0.99,
-        lam=0.95,
-        desired_kl=0.01,
-        max_grad_norm=1.0,
-    )
-
-
-"""Configuration for custom terrains."""
-ROUGH_TERRAINS_CFG = TerrainGeneratorCfg(
-    size=(8.0, 8.0),
-    border_width=20.0,
-    num_rows=10,
-    num_cols=20,
-    horizontal_scale=0.1,
-    vertical_scale=0.005,
-    slope_threshold=0.75,
-    use_cache=False,
-    sub_terrains={
-        "pyramid_stairs": terrain_gen.MeshPyramidStairsTerrainCfg(
-            proportion=0.2,
-            step_height_range=(0.05, 0.23),
-            step_width=0.3,
-            platform_width=3.0,
-            border_width=1.0,
-            holes=False,
-        ),
-        "pyramid_stairs_inv": terrain_gen.MeshInvertedPyramidStairsTerrainCfg(
-            proportion=0.2,
-            step_height_range=(0.05, 0.23),
-            step_width=0.3,
-            platform_width=3.0,
-            border_width=1.0,
-            holes=False,
-        ),
-        "boxes": terrain_gen.MeshRandomGridTerrainCfg(
-            proportion=0.2, grid_width=0.45, grid_height_range=(0.05, 0.2), platform_width=2.0
-        ),
-        "random_rough": terrain_gen.HfRandomUniformTerrainCfg(
-            proportion=0.2, noise_range=(0.02, 0.10), noise_step=0.02, border_width=0.25
-        ),
-        "hf_pyramid_slope": terrain_gen.HfPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.3), platform_width=2.0, border_width=0.25
-        ),
-        "hf_pyramid_slope_inv": terrain_gen.HfInvertedPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.3), platform_width=2.0, border_width=0.25
-        ),
-        # "init_pos": terrain_gen.HfDiscreteObstaclesTerrainCfg(
-        #     proportion=1.0, 
-        #     num_obstacles=10,
-        #     obstacle_height_mode="choice",
-        #     obstacle_height_range=(3.0, 3.0), obstacle_width_range=(0.5, 1.5), 
-        #     platform_width=2.0
-        # ),
-    },
-)
-
-# for sub_terrain_name, sub_terrain_cfg in ROUGH_TERRAINS_CFG.sub_terrains.items():
-#             sub_terrain_cfg.flat_patch_sampling = {
-#                 sub_terrain_name: FlatPatchSamplingCfg(num_patches=2, patch_radius=[0.01,0.1,0.5,1.0], max_height_diff=0.5)
-#             }
-"""Rough terrains configuration."""
-
-##
-# Scene definition
-##
-@configclass
-class G1SceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="generator",
-        terrain_generator=ROUGH_TERRAINS_CFG,
-        max_init_terrain_level=5,
-        collision_group=-1,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        visual_material=sim_utils.MdlFileCfg(
-            mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
-            project_uvw=True,
-            texture_scale=(0.25, 0.25),
-        ),
-        debug_vis=False,
-    )
-    # robots
-    robot: ArticulationCfg = MISSING
-    # sensors
-    height_scanner = None
-    # height_scanner = RayCasterCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/base",
-    #     offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 20.0)),
-    #     attach_yaw_only=True,
-    #     pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-    #     debug_vis=True,
-    #     mesh_prim_paths=["/World/ground"],
-    # )
-    lidar_sensor = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-        mesh_prim_paths=["/World/ground"],
-        update_period=0.1,
-        attach_yaw_only=False,
-        # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.60, 0.0, 0.0), rot=(-0.5, 0.5, -0.5, 0.5)),
-        # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(0.579, -0.579, 0.406, -0.406)),
-        offset=RayCasterCfg.OffsetCfg(pos=(0.047, 0.0, 0.400), rot=(-0.119, 0.0,0.993,0.0)),
-        # data_types=["distance_to_image_plane"],
-        debug_vis=False,
-        pattern_cfg=patterns.BpearlPatternCfg(
-            vertical_ray_angles=[
-                 51.125, 48.0, 45.0, 42.0, 39.0, 36, 33, 30,27,24,20,17,14,11,8,5,2, -1
-    ]
-        ),
-        max_distance=5,
-    )
-
-    depth_sensor = RayCasterCameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-        mesh_prim_paths=["/World/ground"],
-        update_period=0.1,
-        offset=RayCasterCameraCfg.OffsetCfg(pos=(0.108, -0.0325, 0.420), rot=(0.336, 0.0, 0.942, 0.0)),
-        data_types=["distance_to_image_plane"],
-        debug_vis=False,
-        pattern_cfg=patterns.PinholeCameraPatternCfg(
-            # focal_length=24.0,  #24.0
-            # horizontal_aperture=46.0,#87,#20.955,
-            focal_length=1.93, horizontal_aperture=3.8,
-            height=9,
-            width=16,
-        ),
-        max_distance=3,
-    )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True)
-    # lights
-    sky_light = AssetBaseCfg(
-        prim_path="/World/skyLight",
-        spawn=sim_utils.DomeLightCfg(
-            intensity=750.0,
-            texture_file=f"{ISAAC_NUCLEUS_DIR}/Materials/Textures/Skies/PolyHaven/kloofendal_43d_clear_puresky_4k.hdr",
-        ),
-    )
-
-
-##
-# Observations
-##
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        projected_gravity = ObsTerm(
-            func=mdp.projected_gravity,
-            noise=Unoise(n_min=-0.05, n_max=0.05),
-        )
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-        lidar_measurement = ObsTerm(
-            func=mdp.process_lidar,
-            params={"sensor_cfg": SceneEntityCfg("lidar_sensor")},
-            noise=Unoise(n_min=-0.1, n_max=0.1),
-        )
-        realsense_depth_measurement = ObsTerm(
-            func=mdp.process_depth_image,
-            params={"sensor_cfg": SceneEntityCfg("depth_sensor"), "data_type": "distance_to_image_plane"},
-            noise=Unoise(n_min=-0.1, n_max=0.1),
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-
-## 
-# Actions
-##
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    joint_pos = mdp.JointPositionActionCfg(asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True)
-
-##
-# Events (for domain randomization)
-##
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    # startup
-    physics_material = EventTerm(
-        func=mdp.randomize_rigid_body_material,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-            "static_friction_range": (0.4, 0.4),
-            "dynamic_friction_range": (0.4, 0.4),
-            "restitution_range": (0.0, 0.0),
-            "num_buckets": 64,
-        },
-    )
-
-    add_base_mass = EventTerm(
-        func=mdp.randomize_rigid_body_mass,
-        mode="startup",
-        params={"asset_cfg": SceneEntityCfg("robot", body_names="pelvis"), "mass_distribution_params": (-5.0, 5.0), "operation": "add"},
-    )
-
-    # reset
-    base_external_force_torque = EventTerm(
-        func=mdp.apply_external_force_torque,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="pelvis"),
-            "force_range": (0.0, 0.0),
-            "torque_range": (-0.0, 0.0),
-        },
-    )
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_uniform,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (-0.5, 0.5),
-                "y": (-0.5, 0.5),
-                "z": (-0.5, 0.5),
-                "roll": (-0.5, 0.5),
-                "pitch": (-0.5, 0.5),
-                "yaw": (-0.5, 0.5),
-            },
-        },
-    )
-
-    # reset_base = EventTerm(
-    #     func=mdp.reset_root_state_from_terrain,
-    #     mode="reset",
-    #     params={
-    #         "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-3.14, 3.14)},
-    #         "velocity_range": {
-    #             "x": (-0.5, 0.5),
-    #             "y": (-0.5, 0.5),
-    #             "z": (-0.5, 0.5),
-    #             "roll": (-0.5, 0.5),
-    #             "pitch": (-0.5, 0.5),
-    #             "yaw": (-0.5, 0.5),
-    #         },
-    #     },
-    # )
-
-    reset_robot_joints = EventTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "position_range": (0.5, 1.5),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-    # # interval
-    # push_robot = EventTerm(
-    #     func=mdp.push_by_setting_velocity,
-    #     mode="interval",
-    #     interval_range_s=(10.0, 15.0),
-    #     params={"velocity_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5)}},
-    # )
-
-
-##
-# Rewards
-#
-@configclass
-class RewardsCfg2DoF:
-    """
-    Rewards configuration for the 2-DoF H1 (arm fixed) robot.
-    """
-    # -- task
-    termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    track_lin_vel_xy_exp = RewTerm(
-        func=mdp.track_lin_vel_xy_yaw_frame_exp,
-        weight=1.0,
-        params={"command_name": "base_velocity", "std": 0.5},
-    )
-    track_ang_vel_z_exp = RewTerm(
-        func=mdp.track_ang_vel_z_world_exp, weight=2.0, params={"command_name": "base_velocity", "std": 0.5}
-    )
-    feet_air_time = RewTerm(
-        func=mdp.feet_air_time_positive_biped,
-        weight=0.25,
-        params={
-            "command_name": "base_velocity",
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*_ankle_roll_link"),
-            "threshold": 0.4,
-        },
-    )
-    feet_slide = RewTerm(
-        func=mdp.feet_slide,
-        weight=-0.1,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*_ankle_roll_link"),
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*_ankle_roll_link"),
-        },
-    )
-
-    # Penalize ankle joint limits
-    dof_pos_limits = RewTerm(
-        func=mdp.joint_pos_limits,
-        weight=-1.0,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_ankle_pitch_joint", ".*_ankle_roll_joint"])},
-    )
-    # Penalize deviation from default of the joints that are not essential for locomotion
-    joint_deviation_hip = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.1,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_yaw_joint", ".*_hip_roll_joint"])},
-    )
-    joint_deviation_arms = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.1,
-        params={
-            "asset_cfg": SceneEntityCfg(
-                "robot",
-                joint_names=[
-                    ".*_shoulder_pitch_joint",
-                    ".*_shoulder_roll_joint",
-                    ".*_shoulder_yaw_joint",
-                    ".*_elbow_pitch_joint",
-                    ".*_elbow_roll_joint",
-                ],
-            )
-        },
-    )
-    joint_deviation_fingers = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.05,
-        params={
-            "asset_cfg": SceneEntityCfg(
-                "robot",
-                joint_names=[
-                    ".*_five_joint",
-                    ".*_three_joint",
-                    ".*_six_joint",
-                    ".*_four_joint",
-                    ".*_zero_joint",
-                    ".*_one_joint",
-                    ".*_two_joint",
-                ],
-            )
-        },
-    )
-    joint_deviation_torso = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.1,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names="torso_joint")},
-    )
-
-
-@configclass
-class CustomG1Rewards(G1Rewards):
-    feet_stumble = RewTerm(
-        func=mdp.feet_stumble,
-        weight=-0.1,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_roll_link"),
-        },
-    )
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="torso_link"), "threshold": 1.0},
-    )
-
-
-@configclass
-class G1BaseRoughEnvCfg(LocomotionVelocityRoughEnvCfg):
-    scene: G1SceneCfg = G1SceneCfg(num_envs=4096, env_spacing=2.5)
-    observations: ObservationsCfg = ObservationsCfg()
-    rewards: CustomG1Rewards = CustomG1Rewards()
-    terminations: TerminationsCfg = TerminationsCfg()
-
-
-    def __post_init__(self):
-        # post init of parent
-        super().__post_init__()
-        # Scene
-        self.scene.robot = G1_MINIMAL_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-        # self.scene.height_scanner.prim_path = "{ENV_REGEX_NS}/Robot/torso_link"
-
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-
-        # Randomization
-        self.events.push_robot = None
-        self.events.add_base_mass = None
-        self.events.reset_robot_joints.params["position_range"] = (1.0, 1.0)
-        self.events.base_external_force_torque.params["asset_cfg"].body_names = ["torso_link"]
-        self.events.reset_base.params = {
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (0.0, 0.0),
-                "y": (0.0, 0.0),
-                "z": (0.0, 0.0),
-                "roll": (0.0, 0.0),
-                "pitch": (0.0, 0.0),
-                "yaw": (0.0, 0.0),
-            },
-        }
-
-        # Rewards
-        self.rewards.lin_vel_z_l2.weight = 0.0
-        self.rewards.undesired_contacts = None
-        self.rewards.flat_orientation_l2.weight = -1.0
-        self.rewards.action_rate_l2.weight = -0.005
-        self.rewards.dof_acc_l2.weight = -1.25e-7
-        self.rewards.dof_acc_l2.params["asset_cfg"] = SceneEntityCfg(
-            "robot", joint_names=[".*_hip_.*", ".*_knee_joint"]
-        )
-        self.rewards.dof_torques_l2.weight = -1.5e-7
-        self.rewards.dof_torques_l2.params["asset_cfg"] = SceneEntityCfg(
-            "robot", joint_names=[".*_hip_.*", ".*_knee_joint", ".*_ankle_.*"]
-        )
-
-         # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        # self.scene.lidar_sensor = None
-        if self.scene.lidar_sensor is not None:
-            self.scene.lidar_sensor.update_period = self.decimation * self.sim.dt
-        if self.scene.depth_sensor is not None:
-            self.scene.depth_sensor.update_period = self.decimation * self.sim.dt
-        self.scene.contact_forces.update_period = self.sim.dt
-        if self.scene.height_scanner is not None:
-            self.scene.height_scanner.update_period = self.decimation * self.sim.dt
-        if self.scene.contact_forces is not None:
-            self.scene.contact_forces.update_period = self.sim.dt
-
-        # Commands
-        self.commands.base_velocity.ranges.lin_vel_x = (0.0, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (-0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-
-
-@configclass
-class G1BaseRoughEnvCfg_PLAY(G1BaseRoughEnvCfg):
-    def __post_init__(self):
-        # post init of parent
-        super().__post_init__()
-
-        # make a smaller scene for play
-        self.scene.num_envs = 50
-        self.scene.env_spacing = 2.5
-        self.episode_length_s = 40.0
-        # spawn the robot randomly in the grid (instead of their terrain levels)
-        self.scene.terrain.max_init_terrain_level = None
-        # reduce the number of terrains to save memory
-        if self.scene.terrain.terrain_generator is not None:
-            self.scene.terrain.terrain_generator.num_rows = 5
-            self.scene.terrain.terrain_generator.num_cols = 5
-            self.scene.terrain.terrain_generator.curriculum = False
-
-        self.commands.base_velocity.ranges.lin_vel_x = (1.0, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-        self.commands.base_velocity.ranges.heading = (0.0, 0.0)
-        # disable randomization for play
-        self.observations.policy.enable_corruption = False
-        # remove random pushing
-        self.events.base_external_force_torque = None
-        self.events.push_robot = None
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/g1/g1_low_vision_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/g1/g1_low_vision_cfg.py
deleted file mode 100644
index e914b7a..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/g1/g1_low_vision_cfg.py
+++ /dev/null
@@ -1,791 +0,0 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import math
-from dataclasses import MISSING
-
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import ArticulationCfg, AssetBaseCfg
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import CurriculumTermCfg as CurrTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg, FlatPatchSamplingCfg
-import omni.isaac.lab.terrains as terrain_gen
-from omni.isaac.lab.utils import configclass
-from omni.isaac.lab.utils.assets import ISAAC_NUCLEUS_DIR, ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.utils.noise import AdditiveUniformNoiseCfg as Unoise
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg, patterns
-from omni.isaac.lab.assets.articulation import ArticulationCfg
-from omni.isaac.lab.actuators import ImplicitActuatorCfg
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import ActionsCfg, CurriculumCfg, RewardsCfg, EventCfg, LocomotionVelocityRoughEnvCfg
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.config.g1.rough_env_cfg import G1Rewards
-# from omni.isaac.lab_tasks.manager_based.locomotion.velocity.config.h1.rough_env_cfg import H1RoughEnvCfg as OriH1RoughEnvCfg
-
-##
-# Pre-defined configs
-##
-from omni.isaac.lab.terrains.config.rough import ROUGH_TERRAINS_CFG  # isort: skip
-from omni.isaac.lab_assets import G1_MINIMAL_CFG  # isort: skip
-
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-
-
-@configclass
-class G1VisionRoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 24
-    max_iterations = 3000
-    save_interval = 500
-    experiment_name = "g1_vision_rough"
-    empirical_normalization = False
-    policy = RslRlPpoActorCriticCfg(
-        init_noise_std=1.0,
-        actor_hidden_dims=[512, 256, 128],
-        critic_hidden_dims=[512, 256, 128],
-        activation="elu",
-    )
-    algorithm = RslRlPpoAlgorithmCfg(
-        value_loss_coef=1.0,
-        use_clipped_value_loss=True,
-        clip_param=0.2,
-        entropy_coef=0.008,
-        num_learning_epochs=5,
-        num_mini_batches=4,
-        learning_rate=1.0e-3,
-        schedule="adaptive",
-        gamma=0.99,
-        lam=0.95,
-        desired_kl=0.01,
-        max_grad_norm=1.0,
-    )
-
-
-"""Configuration for the Unitree G1 Humanoid robot without arms."""
-G1_NO_ARMS_CFG = ArticulationCfg(
-    spawn=sim_utils.UsdFileCfg(
-        usd_path="/home/zhaojing/unitree_ros/robots/g1_description/g1_arm_fixed/g1_hand_fixed_minimal.usd",
-        activate_contact_sensors=True,
-        rigid_props=sim_utils.RigidBodyPropertiesCfg(
-            disable_gravity=False,
-            retain_accelerations=False,
-            linear_damping=0.0,
-            angular_damping=0.0,
-            max_linear_velocity=1000.0,
-            max_angular_velocity=1000.0,
-            max_depenetration_velocity=1.0,
-        ),
-        articulation_props=sim_utils.ArticulationRootPropertiesCfg(
-            enabled_self_collisions=False, solver_position_iteration_count=8, solver_velocity_iteration_count=4
-        ),
-    ),
-    init_state=ArticulationCfg.InitialStateCfg(
-        pos=(0.0, 0.0, 0.74),
-        joint_pos={
-            ".*_hip_pitch_joint": -0.20,
-            ".*_knee_joint": 0.42,
-            ".*_ankle_pitch_joint": -0.23,
-            ".*_elbow_pitch_joint": 0.87,
-            "left_shoulder_roll_joint": 0.16,
-            "left_shoulder_pitch_joint": 0.35,
-            "right_shoulder_roll_joint": -0.16,
-            "right_shoulder_pitch_joint": 0.35,
-            # "left_one_joint": 1.0,
-            # "right_one_joint": -1.0,
-            # "left_two_joint": 0.52,
-            # "right_two_joint": -0.52,
-        },
-        joint_vel={".*": 0.0},
-    ),
-    soft_joint_pos_limit_factor=0.9,
-    actuators={
-        "legs": ImplicitActuatorCfg(
-            joint_names_expr=[
-                ".*_hip_yaw_joint",
-                ".*_hip_roll_joint",
-                ".*_hip_pitch_joint",
-                ".*_knee_joint",
-                "torso_joint",
-            ],
-            effort_limit=300,
-            velocity_limit=100.0,
-            stiffness={
-                ".*_hip_yaw_joint": 150.0,
-                ".*_hip_roll_joint": 150.0,
-                ".*_hip_pitch_joint": 200.0,
-                ".*_knee_joint": 200.0,
-                "torso_joint": 200.0,
-            },
-            damping={
-                ".*_hip_yaw_joint": 5.0,
-                ".*_hip_roll_joint": 5.0,
-                ".*_hip_pitch_joint": 5.0,
-                ".*_knee_joint": 5.0,
-                "torso_joint": 5.0,
-            },
-            armature={
-                ".*_hip_.*": 0.01,
-                ".*_knee_joint": 0.01,
-                "torso_joint": 0.01,
-            },
-        ),
-        "feet": ImplicitActuatorCfg(
-            effort_limit=20,
-            joint_names_expr=[".*_ankle_pitch_joint", ".*_ankle_roll_joint"],
-            stiffness=20.0,
-            damping=2.0,
-            armature=0.01,
-        ),
-        "arms": ImplicitActuatorCfg(
-            joint_names_expr=[
-                ".*_shoulder_pitch_joint",
-                ".*_shoulder_roll_joint",
-                ".*_shoulder_yaw_joint",
-                ".*_elbow_pitch_joint",
-                ".*_elbow_roll_joint",
-                # ".*_five_joint",
-                # ".*_three_joint",
-                # ".*_six_joint",
-                # ".*_four_joint",
-                # ".*_zero_joint",
-                # ".*_one_joint",
-                # ".*_two_joint",
-            ],
-            effort_limit=300,
-            velocity_limit=100.0,
-            stiffness=40.0,
-            damping=10.0,
-            armature={
-                ".*_shoulder_.*": 0.01,
-                ".*_elbow_.*": 0.01,
-                # ".*_five_joint": 0.001,
-                # ".*_three_joint": 0.001,
-                # ".*_six_joint": 0.001,
-                # ".*_four_joint": 0.001,
-                # ".*_zero_joint": 0.001,
-                # ".*_one_joint": 0.001,
-                # ".*_two_joint": 0.001,
-            },
-        ),
-    },
-)
-
-
-"""Configuration for custom terrains."""
-ROUGH_TERRAINS_CFG = TerrainGeneratorCfg(
-    size=(8.0, 8.0),
-    border_width=20.0,
-    num_rows=10,
-    num_cols=20,
-    horizontal_scale=0.1,
-    vertical_scale=0.005,
-    slope_threshold=0.75,
-    use_cache=False,
-    sub_terrains={
-        "pyramid_stairs": terrain_gen.MeshPyramidStairsTerrainCfg(
-            proportion=0.2,
-            step_height_range=(0.05, 0.2),
-            step_width=0.3,
-            platform_width=3.0,
-            border_width=1.0,
-            holes=False,
-        ),
-        "pyramid_stairs_inv": terrain_gen.MeshInvertedPyramidStairsTerrainCfg(
-            proportion=0.2,
-            step_height_range=(0.05, 0.2),
-            step_width=0.3,
-            platform_width=3.0,
-            border_width=1.0,
-            holes=False,
-        ),
-        # "boxes": terrain_gen.MeshRandomGridTerrainCfg(
-        #     proportion=0.2, grid_width=0.45, grid_height_range=(0.05, 0.1), platform_width=2.0
-        # ),
-        "random_rough": terrain_gen.HfRandomUniformTerrainCfg(
-            proportion=0.2, noise_range=(0.02, 0.10), noise_step=0.02, border_width=0.25
-        ),
-        "hf_pyramid_slope": terrain_gen.HfPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25
-        ),
-        "hf_pyramid_slope_inv": terrain_gen.HfInvertedPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25
-        ),
-        # "init_pos": terrain_gen.HfDiscreteObstaclesTerrainCfg(
-        #     proportion=1.0, 
-        #     num_obstacles=10,
-        #     obstacle_height_mode="choice",
-        #     obstacle_height_range=(3.0, 3.0), obstacle_width_range=(0.5, 1.5), 
-        #     platform_width=2.0
-        # ),
-    },
-)
-
-# for sub_terrain_name, sub_terrain_cfg in ROUGH_TERRAINS_CFG.sub_terrains.items():
-#             sub_terrain_cfg.flat_patch_sampling = {
-#                 sub_terrain_name: FlatPatchSamplingCfg(num_patches=2, patch_radius=[0.01,0.1,0.5,1.0], max_height_diff=0.5)
-#             }
-"""Rough terrains configuration."""
-
-##
-# Scene definition
-##
-@configclass
-class TrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="generator",
-        terrain_generator=ROUGH_TERRAINS_CFG,
-        max_init_terrain_level=5, # TRY 9 AS WELL
-        collision_group=-1,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        visual_material=sim_utils.MdlFileCfg(
-            mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
-            project_uvw=True,
-            texture_scale=(0.25, 0.25),
-        ),
-        debug_vis=False,
-    )
-    # robots
-    # robot = G1_MINIMAL_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot = G1_NO_ARMS_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # sensors
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    
-    height_scanner = None
-    height_scanner = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 20.0)),
-        attach_yaw_only=True,
-        pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-        debug_vis=False,
-        mesh_prim_paths=["/World/ground"],
-    )
-    height_scanner.prim_path = "{ENV_REGEX_NS}/Robot/torso_link"
-    # camera
-    lidar_sensor = None
-    # lidar_sensor = RayCasterCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-    #     # mesh_prim_paths=["/home/zhaojing/terrain-generator/results/generated_terrain/mesh_1/mesh.ply"],
-    #     mesh_prim_paths=["/World/ground"],
-    #     attach_yaw_only=False,
-    #     # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.60, 0.0, 0.0), rot=(-0.5, 0.5, -0.5, 0.5)),
-    #     # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(0.579, -0.579, 0.406, -0.406)),
-    #     offset=RayCasterCfg.OffsetCfg(pos=(0.047, 0.0, 0.400), rot=(-0.119, 0.0,0.993,0.0)),
-    #     # data_types=["distance_to_image_plane"],
-    #     debug_vis=False,
-    #     pattern_cfg=patterns.BpearlPatternCfg(
-    #         vertical_ray_angles=[
-    #              51.125, 48.0, 45.0, 42.0, 39.0, 36, 33, 30,27,24,20,17,14,11,8,5,2, -1
-    # ]
-    #     ),
-    #     max_distance=5,
-    # )
-
-    depth_sensor = None
-    depth_sensor = RayCasterCameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-        mesh_prim_paths=["/World/ground"],
-        offset=RayCasterCameraCfg.OffsetCfg(pos=(0.108, -0.0325, 0.420), rot=(0.389, 0.0, 0.921, 0.0)),
-        data_types=["distance_to_image_plane"],
-        debug_vis=False,
-        pattern_cfg=patterns.PinholeCameraPatternCfg(
-            focal_length=1.93, horizontal_aperture=3.8,
-            height=53,
-            width=30,
-        ),
-        max_distance=10,
-    )
-
-
-##
-# Observations
-##
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        projected_gravity = ObsTerm(
-            func=mdp.projected_gravity,
-            noise=Unoise(n_min=-0.05, n_max=0.05),
-        )
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-        # lidar_measurement = ObsTerm(
-        #     func=mdp.process_lidar,
-        #     params={"sensor_cfg": SceneEntityCfg("lidar_sensor")},
-        #     noise=Unoise(n_min=-0.1, n_max=0.1)
-        # )
-        realsense_depth_measurement = ObsTerm(
-            func=mdp.process_depth_image,
-            params={"sensor_cfg": SceneEntityCfg("depth_sensor"), "data_type": "distance_to_image_plane"},
-            noise=Unoise(n_min=-0.1, n_max=0.1),
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-    
-    @configclass
-    class CriticObsCfg(ObsGroup):
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        projected_gravity = ObsTerm(
-            func=mdp.projected_gravity,
-            noise=Unoise(n_min=-0.05, n_max=0.05),
-        )
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-        height_scan = ObsTerm(
-            func=mdp.height_scan,
-            params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-            clip=(-1.0, 1.0),
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-    
-    @configclass
-    class ProprioCfg(ObsGroup):
-        """Observations for proprioceptive group."""
-
-        # observation terms
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        projected_gravity = ObsTerm(
-            func=mdp.projected_gravity,
-            noise=Unoise(n_min=-0.05, n_max=0.05),
-        )
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-
-        def __post_init__(self):
-            self.concatenate_terms = True
-
-    # @configclass
-    # class HighResDepth(ObsGroup):
-    #     realsense_depth_measurement = ObsTerm(
-    #         func=mdp.process_depth_image,
-    #         params={"sensor_cfg": SceneEntityCfg("depth_sensor"), "data_type": "distance_to_image_plane"},
-    #         noise=Unoise(n_min=-0.1, n_max=0.1),
-    #     )
-
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-    critic: CriticObsCfg = CriticObsCfg()
-    proprio: ProprioCfg = ProprioCfg()
-    # high_res_depth: HighResDepth = HighResDepth()
-
-## 
-# Actions
-##
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    joint_pos = mdp.JointPositionActionCfg(asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True)
-
-##
-# Events (for domain randomization)
-##
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    # startup
-    physics_material = EventTerm(
-        func=mdp.randomize_rigid_body_material,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-            "static_friction_range": (0.8, 0.8),
-            "dynamic_friction_range": (0.6, 0.6),
-            "restitution_range": (0.0, 0.0),
-            "num_buckets": 64,
-        },
-    )
-
-    add_base_mass = EventTerm(
-        func=mdp.randomize_rigid_body_mass,
-        mode="startup",
-        params={"asset_cfg": SceneEntityCfg("robot", body_names="pelvis"), "mass_distribution_params": (-5.0, 5.0), "operation": "add"},
-    )
-
-    # reset
-    base_external_force_torque = EventTerm(
-        func=mdp.apply_external_force_torque,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="pelvis"),
-            "force_range": (0.0, 0.0),
-            "torque_range": (-0.0, 0.0),
-        },
-    )
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_uniform,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (-0.5, 0.5),
-                "y": (-0.5, 0.5),
-                "z": (-0.5, 0.5),
-                "roll": (-0.5, 0.5),
-                "pitch": (-0.5, 0.5),
-                "yaw": (-0.5, 0.5),
-            },
-        },
-    )
-
-    # reset_base = EventTerm(
-    #     func=mdp.reset_root_state_from_terrain,
-    #     mode="reset",
-    #     params={
-    #         "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-3.14, 3.14)},
-    #         "velocity_range": {
-    #             "x": (-0.5, 0.5),
-    #             "y": (-0.5, 0.5),
-    #             "z": (-0.5, 0.5),
-    #             "roll": (-0.5, 0.5),
-    #             "pitch": (-0.5, 0.5),
-    #             "yaw": (-0.5, 0.5),
-    #         },
-    #     },
-    # )
-
-    reset_robot_joints = EventTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "position_range": (0.5, 1.5),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-    # # interval
-    # push_robot = EventTerm(
-    #     func=mdp.push_by_setting_velocity,
-    #     mode="interval",
-    #     interval_range_s=(10.0, 15.0),
-    #     params={"velocity_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5)}},
-    # )
-
-    reset_depth_sensor = EventTerm(
-        func=mdp.reset_camera_pos_uniform,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("depth_sensor"),
-            "pose_delta_range": {
-                "x": (-0.01, 0.01),
-                "y": (-0.01, 0.01),
-                "z": (-0.02, 0.02),
-                "roll": (-0.1, 0.1),
-                "pitch": (-0.1, 0.1),
-                "yaw": (-0.1, 0.1),
-            },
-        }
-    )
-
-
-##
-# Rewards
-#
-@configclass
-class G1NoArmsRewardsCfg:
-    termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    track_lin_vel_xy_exp = RewTerm(
-        func=mdp.track_lin_vel_xy_yaw_frame_exp,
-        weight=1.0,
-        params={"command_name": "base_velocity", "std": 0.5},
-    )
-    track_ang_vel_z_exp = RewTerm(
-        func=mdp.track_ang_vel_z_world_exp, weight=2.0, params={"command_name": "base_velocity", "std": 0.5}
-    )
-    feet_air_time = RewTerm(
-        func=mdp.feet_air_time_positive_biped,
-        weight=0.25,
-        params={
-            "command_name": "base_velocity",
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*_ankle_roll_link"),
-            "threshold": 0.4,
-        },
-    )
-    feet_slide = RewTerm(
-        func=mdp.feet_slide,
-        weight=-0.1,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*_ankle_roll_link"),
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*_ankle_roll_link"),
-        },
-    )
-    # feet_stumble = RewTerm(
-    #     func=mdp.feet_stumble,
-    #     weight=-0.1,
-    #     params={
-    #         "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_roll_link"),
-    #     },
-    # )
-
-    # -- penalties
-    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-2.0)
-    ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
-    dof_torques_l2 = RewTerm(func=mdp.joint_torques_l2, weight=-1.0e-5)
-    dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-2.5e-7)
-    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
-
-    # Penalize ankle joint limits
-    dof_pos_limits = RewTerm(
-        func=mdp.joint_pos_limits,
-        weight=-1.0,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_ankle_pitch_joint", ".*_ankle_roll_joint", ".*_hip_yaw_joint"])},
-    )
-    # Penalize deviation from default of the joints that are not essential for locomotion
-    joint_deviation_hip = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.2,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_yaw_joint", ".*_hip_roll_joint"])},
-    )
-    joint_deviation_arms = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.1,
-        params={
-            "asset_cfg": SceneEntityCfg(
-                "robot",
-                joint_names=[
-                    ".*_shoulder_pitch_joint",
-                    ".*_shoulder_roll_joint",
-                    ".*_shoulder_yaw_joint",
-                    ".*_elbow_pitch_joint",
-                    ".*_elbow_roll_joint",
-                ],
-            )
-        },
-    )
-    joint_deviation_torso = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.1,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names="torso_joint")},
-    )
-    # -- optional penalties
-    flat_orientation_l2 = RewTerm(func=mdp.flat_orientation_l2, weight=0.0)
-    dof_pos_limits = RewTerm(func=mdp.joint_pos_limits, weight=0.0)
-
-
-@configclass
-class CustomG1Rewards(G1Rewards):
-    feet_stumble = RewTerm(
-        func=mdp.feet_stumble,
-        weight=-0.2,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_roll_link"),
-        },
-    )
-
-
-
-##
-# Commands
-##
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    base_velocity = mdp.UniformVelocityCommandCfg(
-        asset_name="robot",
-        resampling_time_range=(10.0, 10.0),
-        rel_standing_envs=0.02,
-        rel_heading_envs=1.0,
-        heading_command=True,
-        heading_control_stiffness=0.5,
-        debug_vis=True,
-        ranges=mdp.UniformVelocityCommandCfg.Ranges(
-            lin_vel_x=(-1.0, 1.0), lin_vel_y=(-1.0, 1.0), ang_vel_z=(-1.0, 1.0), heading=(-math.pi, math.pi)
-        ),
-    )
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*torso_link"), "threshold": 1.0},
-    )
-
-
-@configclass
-class G1VisionRoughEnvCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Scene settings
-    scene: TrainSceneCfg = TrainSceneCfg(num_envs=4096, env_spacing=2.5)
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # MDP settings
-    # rewards: RewardsCfg = G1Rewards()
-    rewards: RewardsCfg = G1NoArmsRewardsCfg()
-    # rewards: RewardsCfg = CustomG1Rewards()
-    terminations: TerminationsCfg = TerminationsCfg()
-    events: EventCfg = EventCfg()
-    curriculum: CurriculumCfg = CurriculumCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 4
-        self.episode_length_s = 20.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material = self.scene.terrain.physics_material
-        # self.sim.physics_material.static_friction = 1.0
-        # self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "average"
-        self.sim.physics_material.restitution_combine_mode = "average"
-
-        # Randomization
-        self.events.push_robot = None
-        self.events.add_base_mass = None
-        self.events.reset_robot_joints.params["position_range"] = (1.0, 1.0)
-        self.events.base_external_force_torque.params["asset_cfg"].body_names = [".*torso_link"]
-        self.events.reset_base.params = {
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (0.0, 0.0),
-                "y": (0.0, 0.0),
-                "z": (0.0, 0.0),
-                "roll": (0.0, 0.0),
-                "pitch": (0.0, 0.0),
-                "yaw": (0.0, 0.0),
-            },
-        }
-
-        # Rewards
-        self.rewards.lin_vel_z_l2.weight = 0.0
-        self.rewards.undesired_contacts = None
-        self.rewards.flat_orientation_l2.weight = -1.0
-        self.rewards.action_rate_l2.weight = -0.005
-        self.rewards.dof_acc_l2.weight = -1.25e-7
-        self.rewards.dof_acc_l2.params["asset_cfg"] = SceneEntityCfg(
-            "robot", joint_names=[".*_hip_.*", ".*_knee_joint"]
-        )
-        self.rewards.dof_torques_l2.weight = -1.5e-7
-        self.rewards.dof_torques_l2.params["asset_cfg"] = SceneEntityCfg(
-            "robot", joint_names=[".*_hip_.*", ".*_knee_joint", ".*_ankle_.*"]
-        )
-        # self.rewards.feet_air_time.weight = 0.5
-
-        # Commands
-        self.commands.base_velocity.ranges.lin_vel_x = (0.0, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (-0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        # self.scene.lidar_sensor = None
-        if self.scene.lidar_sensor is not None:
-            self.scene.lidar_sensor.update_period = self.decimation * self.sim.dt
-        if self.scene.depth_sensor is not None:
-            self.scene.depth_sensor.update_period = self.decimation * self.sim.dt
-        if self.scene.height_scanner is not None:
-            self.scene.height_scanner.update_period = self.decimation * self.sim.dt
-        if self.scene.contact_forces is not None:
-            self.scene.contact_forces.update_period = self.sim.dt
-
-        # check if terrain levels curriculum is enabled - if so, enable curriculum for terrain generator
-        # this generates terrains with increasing difficulty and is useful for training
-        if getattr(self.curriculum, "terrain_levels", None) is not None:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = True
-        else:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = False
-        # self.curriculum.terrain_levels = None
-
-
-@configclass
-class G1VisionRoughEnvCfg_PLAY(G1VisionRoughEnvCfg):
-    def __post_init__(self):
-        # post init of parent
-        super().__post_init__()
-
-        # make a smaller scene for play
-        self.scene.num_envs = 40
-        self.scene.env_spacing = 2.5
-        self.episode_length_s = 40.0
-        # spawn the robot randomly in the grid (instead of their terrain levels)
-        self.scene.terrain.max_init_terrain_level = None
-        # reduce the number of terrains to save memory
-        if self.scene.terrain.terrain_generator is not None:
-            self.scene.terrain.terrain_generator.num_rows = 5
-            self.scene.terrain.terrain_generator.num_cols = 5
-            self.scene.terrain.terrain_generator.curriculum = False
-            self.scene.terrain.terrain_generator.sub_terrains["pyramid_stairs"].step_height_range = (0.17, 0.17)
-            self.scene.terrain.terrain_generator.sub_terrains["pyramid_stairs_inv"].step_height_range = (0.17, 0.17)
-
-        self.commands.base_velocity.ranges.lin_vel_x = (0.5, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-        self.commands.base_velocity.ranges.heading = (0.0, 0.0)
-        self.commands.base_velocity.heading_command = True
-        # disable randomization for play
-        self.observations.policy.enable_corruption = False
-        # remove random pushing
-        self.events.base_external_force_torque = None
-        self.events.push_robot = None
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go1/__init__.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go1/__init__.py
deleted file mode 100644
index 98fd3ca..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go1/__init__.py
+++ /dev/null
@@ -1,52 +0,0 @@
-import gymnasium as gym
-
-from .go1_low_base_cfg import Go1BaseRoughEnvCfg, Go1BaseRoughEnvCfg_PLAY, Go1RoughPPORunnerCfg
-from .go1_low_vision_cfg import Go1VisionRoughEnvCfg, Go1VisionRoughEnvCfg_PLAY, Go1VisionRoughPPORunnerCfg
-
-##
-# Register Gym environments.
-##
-
-
-gym.register(
-    id="go1_base",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": Go1BaseRoughEnvCfg,
-        "rsl_rl_cfg_entry_point": Go1RoughPPORunnerCfg,
-    },
-)
-
-
-gym.register(
-    id="go1_base_play",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": Go1BaseRoughEnvCfg_PLAY,
-        "rsl_rl_cfg_entry_point": Go1RoughPPORunnerCfg,
-    },
-)
-
-
-gym.register(
-    id="go1_vision",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": Go1VisionRoughEnvCfg,
-        "rsl_rl_cfg_entry_point": Go1VisionRoughPPORunnerCfg,
-    },
-)
-
-
-gym.register(
-    id="go1_vision_play",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": Go1VisionRoughEnvCfg_PLAY,
-        "rsl_rl_cfg_entry_point": Go1VisionRoughPPORunnerCfg,
-    },
-)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go1/go1_low_base_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go1/go1_low_base_cfg.py
deleted file mode 100644
index edd0121..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go1/go1_low_base_cfg.py
+++ /dev/null
@@ -1,539 +0,0 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from omni.isaac.lab.utils import configclass
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import ArticulationCfg, AssetBaseCfg
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import LocomotionVelocityRoughEnvCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg
-import omni.isaac.lab.terrains as terrain_gen
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import ContactSensorCfg, RayCasterCfg, patterns
-# from omni.isaac.lab.actuators import ImplicitActuatorCfg, DelayedPDActuatorCfg, DeplayedActuatorNetMLPCfg
-from omni.isaac.lab.utils.assets import ISAAC_NUCLEUS_DIR, ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.utils.noise import AdditiveUniformNoiseCfg as Unoise
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import ActionsCfg, CurriculumCfg, RewardsCfg, EventCfg, TerminationsCfg, CommandsCfg
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-##
-# Pre-defined configs
-##
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-
-from omni.isaac.lab_assets.unitree import GO1_ACTUATOR_CFG
-
-
-@configclass
-class Go1RoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 24
-    max_iterations = 5000
-    save_interval = 50
-    experiment_name = "go1_base_flat"
-    empirical_normalization = False
-    policy = RslRlPpoActorCriticCfg(
-        init_noise_std=1.0,
-        actor_hidden_dims=[512, 256, 128],
-        critic_hidden_dims=[512, 256, 128],
-        activation="elu",
-    )
-    algorithm = RslRlPpoAlgorithmCfg(
-        value_loss_coef=1.0,
-        use_clipped_value_loss=True,
-        clip_param=0.2,
-        entropy_coef=0.01,
-        num_learning_epochs=5,
-        num_mini_batches=4,
-        learning_rate=1.0e-3,
-        schedule="adaptive",
-        gamma=0.99,
-        lam=0.95,
-        desired_kl=0.01,
-        max_grad_norm=1.0,
-    )
-
-##
-# Robot configuration
-##
-# GO1_ACTUATOR_CFG = DeplayedActuatorNetMLPCfg(
-#     joint_names_expr=[".*_hip_joint", ".*_thigh_joint", ".*_calf_joint"],
-#     network_file=f"{ISAACLAB_NUCLEUS_DIR}/ActuatorNets/Unitree/unitree_go1.pt",
-#     pos_scale=-1.0,
-#     vel_scale=1.0,
-#     torque_scale=1.0,
-#     input_order="pos_vel",
-#     input_idx=[0, 1, 2],
-#     effort_limit=23.7,  # taken from spec sheet
-#     velocity_limit=30.0,  # taken from spec sheet
-#     saturation_effort=23.7,  # same as effort limit
-#     min_delay=4,
-#     max_delay=4
-# )
-
-UNITREE_GO1_CFG = ArticulationCfg(
-    spawn=sim_utils.UsdFileCfg(
-        usd_path=f"{ISAACLAB_NUCLEUS_DIR}/Robots/Unitree/Go1/go1.usd",
-        activate_contact_sensors=True,
-        rigid_props=sim_utils.RigidBodyPropertiesCfg(
-            disable_gravity=False,
-            retain_accelerations=False,
-            linear_damping=0.0,
-            angular_damping=0.0,
-            max_linear_velocity=1000.0,
-            max_angular_velocity=1000.0,
-            max_depenetration_velocity=1.0,
-        ),
-        articulation_props=sim_utils.ArticulationRootPropertiesCfg(
-            enabled_self_collisions=False, solver_position_iteration_count=4, solver_velocity_iteration_count=0
-        ),
-    ),
-    init_state=ArticulationCfg.InitialStateCfg(
-        pos=(0.0, 0.0, 0.4),
-        joint_pos={
-            ".*L_hip_joint": 0.1,
-            ".*R_hip_joint": -0.1,
-            "F[L,R]_thigh_joint": 0.8,
-            "R[L,R]_thigh_joint": 1.0,
-            ".*_calf_joint": -1.5,
-        },
-        joint_vel={".*": 0.0},
-    ),
-    soft_joint_pos_limit_factor=0.9,
-    actuators={
-        # "base_legs": DelayedPDActuatorCfg(
-        #     joint_names_expr=[".*_hip_joint", ".*_thigh_joint", ".*_calf_joint"],
-        #     effort_limit=23.5,
-        #     velocity_limit=30.0,
-        #     stiffness=30.0,
-        #     damping=0.5,
-        #     min_delay=4,
-        #     max_delay=4,
-        # )
-        "base_legs": GO1_ACTUATOR_CFG
-    }
-)
-
-
-##
-# Configuration for custom terrains.
-##
-Go1_BASE_TERRAINS_CFG = TerrainGeneratorCfg(
-    size=(8.0, 8.0),
-    border_width=20.0,
-    num_rows=10,
-    num_cols=20,
-    horizontal_scale=0.1,
-    vertical_scale=0.005,
-    slope_threshold=0.75,
-    use_cache=False,
-    sub_terrains={
-        # "pyramid_stairs": terrain_gen.MeshPyramidStairsTerrainCfg(
-        #     proportion=0.2,
-        #     step_height_range=(0.05, 0.2),
-        #     step_width=0.3,
-        #     platform_width=3.0,
-        #     border_width=1.0,
-        #     holes=False,
-        # ),
-        # "pyramid_stairs_inv": terrain_gen.MeshInvertedPyramidStairsTerrainCfg(
-        #     proportion=0.2,
-        #     step_height_range=(0.05, 0.2),
-        #     step_width=0.3,
-        #     platform_width=3.0,
-        #     border_width=1.0,
-        #     holes=False,
-        # ),
-        # "boxes": terrain_gen.MeshRandomGridTerrainCfg(
-        #     proportion=0.2, grid_width=0.45, grid_height_range=(0.05, 0.1), platform_width=2.0
-        # ),
-        "flat": terrain_gen.MeshPlaneTerrainCfg(proportion=0.4),
-        "random_rough": terrain_gen.HfRandomUniformTerrainCfg(
-            proportion=0.4, noise_range=(0.02, 0.05), noise_step=0.02, border_width=0.25
-        ),
-        # "gaps": terrain_gen.MeshGapTerrainCfg(
-        #     proportion=0.1, gap_width_range=(0.5, 1.0), platform_width=2.0
-        # ),
-        # "hf_pyramid_slope": terrain_gen.HfPyramidSlopedTerrainCfg(
-        #     proportion=0.2, slope_range=(0.0, 0.02), platform_width=2.0, border_width=0.25
-        # ),
-        # "hf_pyramid_slope_inv": terrain_gen.HfInvertedPyramidSlopedTerrainCfg(
-        #     proportion=0.2, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25 ),
-    },
-)
-
-
-##
-# Scene definition
-##
-@configclass
-class Go1SceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="generator",
-        terrain_generator=Go1_BASE_TERRAINS_CFG,
-        max_init_terrain_level=5,
-        collision_group=-1,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        visual_material=sim_utils.MdlFileCfg(
-            mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
-            project_uvw=True,
-            texture_scale=(0.25, 0.25),
-        ),
-        debug_vis=False,
-    )
-
-    # robots
-    robot: ArticulationCfg = UNITREE_GO1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-
-    # sensors
-    height_scanner = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/trunk",
-        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 20.0)),
-        attach_yaw_only=True,
-        pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-        debug_vis=False,
-        mesh_prim_paths=["/World/ground"],
-    )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True)
-
-    # lights
-    sky_light = AssetBaseCfg(
-        prim_path="/World/skyLight",
-        spawn=sim_utils.DomeLightCfg(
-            intensity=750.0,
-            texture_file=f"{ISAAC_NUCLEUS_DIR}/Materials/Textures/Skies/PolyHaven/kloofendal_43d_clear_puresky_4k.hdr",
-        ),
-    )
-
-
-##
-# Rewards
-##
-@configclass 
-class CustomGo1RewardsCfg(RewardsCfg):
-    hip_deviation = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.4,
-        # params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_joint", ".*_thigh_joint", ".*_calf_joint"])},
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_joint"])},
-    )
-    joint_deviation = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.04,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_thigh_joint", ".*_calf_joint"])},
-    )
-    base_height = RewTerm(
-        func=mdp.base_height_l2,
-        weight=-5.0,
-        params={"target_height": 0.25},
-    )
-
-##
-# Observations
-##
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        # base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        # projected_gravity = ObsTerm(
-        #     func=mdp.projected_gravity,
-        #     noise=Unoise(n_min=-0.05, n_max=0.05),
-        # )
-        base_rpy = ObsTerm(func=mdp.base_rpy, noise=Unoise(n_min=-0.1, n_max=0.1))
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    
-    @configclass
-    class ProprioCfg(ObsGroup):
-        """Observations for proprioceptive group."""
-
-        # observation terms
-        # base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        # projected_gravity = ObsTerm(
-        #     func=mdp.projected_gravity,
-        #     noise=Unoise(n_min=-0.05, n_max=0.05),
-        # )
-        base_rpy = ObsTerm(func=mdp.base_rpy, noise=Unoise(n_min=-0.1, n_max=0.1))
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-
-        def __post_init__(self):
-            self.concatenate_terms = True
-    
-    @configclass
-    class CriticObsCfg(ObsGroup):
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        projected_gravity = ObsTerm(
-            func=mdp.projected_gravity,
-            noise=Unoise(n_min=-0.05, n_max=0.05),
-        )
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-        height_scan = ObsTerm(
-            func=mdp.height_scan,
-            params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-            clip=(-1.0, 1.0),
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = False
-            self.concatenate_terms = True
-
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-    proprio: ProprioCfg = ProprioCfg()
-    critic: CriticObsCfg = CriticObsCfg()
-
-
-##
-# Events (for domain randomization)
-##
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    # startup
-    physics_material = EventTerm(
-        func=mdp.randomize_rigid_body_material,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-            "static_friction_range": (0.4, 2.0),
-            "dynamic_friction_range": (0.4, 1.0),
-            "restitution_range": (0.0, 0.0),
-            "num_buckets": 64,
-        },
-    )
-
-    add_base_mass = EventTerm(
-        func=mdp.randomize_rigid_body_mass,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="base"),
-            "mass_distribution_params": (-5.0, 5.0),
-            "operation": "add",
-        },
-    )
-
-    actuator_gains = EventTerm(
-        func=mdp.randomize_actuator_gains,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", joint_names=".*"),
-            "stiffness_distribution_params": (0.8, 1.2),
-            "operation": "scale",
-        },
-    )
-
-    # reset
-    base_external_force_torque = EventTerm(
-        func=mdp.apply_external_force_torque,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="base"),
-            "force_range": (0.0, 0.0),
-            "torque_range": (-0.0, 0.0),
-        },
-    )
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_uniform,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (-0.5, 0.5),
-                "y": (-0.5, 0.5),
-                "z": (-0.5, 0.5),
-                "roll": (-0.5, 0.5),
-                "pitch": (-0.5, 0.5),
-                "yaw": (-0.5, 0.5),
-            },
-        },
-    )
-
-    reset_robot_joints = EventTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "position_range": (0.5, 1.5),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-    # interval
-    push_robot = EventTerm(
-        func=mdp.push_by_setting_velocity,
-        mode="interval",
-        interval_range_s=(10.0, 15.0),
-        params={"velocity_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5)}},
-    )
-
-
-@configclass
-class Go1BaseRoughEnvCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the Go1 locomotion velocity-tracking environment."""
-
-    scene: Go1SceneCfg = Go1SceneCfg(num_envs=4096, env_spacing=2.5)
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # MDP settings
-    rewards: RewardsCfg = CustomGo1RewardsCfg()
-    terminations: TerminationsCfg = TerminationsCfg()
-    events: EventCfg = EventCfg()
-    curriculum: CurriculumCfg = CurriculumCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 4
-        self.sim.render_interval = 4
-        self.episode_length_s = 20.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material = self.scene.terrain.physics_material
-        # self.sim.physics_material.static_friction = 1.0
-        # self.sim.physics_material.dynamic_friction = 1.0
-        # self.sim.physics_material.friction_combine_mode = "multiply"
-        # self.sim.physics_material.restitution_combine_mode = "multiply"
-
-        # scale down the terrains because the robot is small
-        # self.scene.terrain.terrain_generator.sub_terrains["boxes"].grid_height_range = (0.025, 0.1)
-        self.scene.terrain.terrain_generator.sub_terrains["random_rough"].noise_range = (0.01, 0.04)
-        self.scene.terrain.terrain_generator.sub_terrains["random_rough"].noise_step = 0.01
-
-        # reduce action scale
-        self.actions.joint_pos.scale = 0.25
-
-        # event
-        # self.events.push_robot = None
-        self.events.actuator_gains = None
-        self.events.add_base_mass.params["mass_distribution_params"] = (-3.0, 3.0)
-        self.events.add_base_mass.params["asset_cfg"].body_names = "trunk"
-        self.events.base_external_force_torque.params["asset_cfg"].body_names = "trunk"
-        self.events.reset_robot_joints.params["position_range"] = (1.0, 1.0)
-        self.events.reset_base.params = {
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (0.0, 0.0),
-                "y": (0.0, 0.0),
-                "z": (0.0, 0.0),
-                "roll": (0.0, 0.0),
-                "pitch": (0.0, 0.0),
-                "yaw": (0.0, 0.0),
-            },
-        }
-
-        # rewards
-        self.rewards.feet_air_time.params["sensor_cfg"].body_names = ".*_foot"
-        # self.rewards.feet_air_time.weight = 0.01
-        self.rewards.undesired_contacts = None
-        self.rewards.dof_torques_l2.weight = -0.0002
-        self.rewards.track_lin_vel_xy_exp.weight = 1.5
-        self.rewards.track_ang_vel_z_exp.weight = 0.75
-        self.rewards.dof_acc_l2.weight = -2.5e-7
-        # self.rewards.dof_pos_limits.weight = -0.0001
-
-        # Commands
-        self.commands.base_velocity.ranges.lin_vel_x = (-1.0, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (-0.5, 0.5)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-        # self.commands.base_velocity.rel_standing_envs = 0.1
-
-        # terminations
-        self.terminations.base_contact.params["sensor_cfg"].body_names = "trunk"
-        self.events.add_base_mass.params["asset_cfg"].body_names = "trunk"
-        # check if terrain levels curriculum is enabled - if so, enable curriculum for terrain generator
-        # this generates terrains with increasing difficulty and is useful for training
-
-        # flat terrain
-        # self.scene.terrain.terrain_type = "plane"
-        # self.scene.terrain.terrain_generator = None
-        # self.curriculum.terrain_levels = None
-        self.rewards.flat_orientation_l2.weight = -2.5
-        self.rewards.feet_air_time.weight = 0.25
-
-        # update sensor period
-        self.scene.contact_forces.update_period = self.sim.dt
-
-        if getattr(self.curriculum, "terrain_levels", None) is not None:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = True
-        else:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = False
-
-
-@configclass
-class Go1BaseRoughEnvCfg_PLAY(Go1BaseRoughEnvCfg):
-    def __post_init__(self):
-        # post init of parent
-        super().__post_init__()
-
-        # make a smaller scene for play
-        self.scene.num_envs = 50
-        self.scene.env_spacing = 2.5
-        # spawn the robot randomly in the grid (instead of their terrain levels)
-        self.scene.terrain.max_init_terrain_level = None
-        # reduce the number of terrains to save memory
-        if self.scene.terrain.terrain_generator is not None:
-            self.scene.terrain.terrain_generator.num_rows = 5
-            self.scene.terrain.terrain_generator.num_cols = 5
-            self.scene.terrain.terrain_generator.curriculum = False
-
-        # disable randomization for play
-        self.observations.policy.enable_corruption = False
-        # remove random pushing event
-        self.events.base_external_force_torque = None
-        self.events.push_robot = None
-        self.events.actuator_gains = None
-
-        # Commands
-        self.commands.base_velocity.ranges.lin_vel_x = (0.5, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-0.5, 0.5)
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go1/go1_low_vision_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go1/go1_low_vision_cfg.py
deleted file mode 100644
index 4e70c20..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go1/go1_low_vision_cfg.py
+++ /dev/null
@@ -1,515 +0,0 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from omni.isaac.lab.utils import configclass
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import ArticulationCfg, AssetBaseCfg
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import LocomotionVelocityRoughEnvCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg
-import omni.isaac.lab.terrains as terrain_gen
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import ContactSensorCfg, RayCasterCfg
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg, patterns
-from omni.isaac.lab.actuators import ImplicitActuatorCfg
-from omni.isaac.lab.utils.assets import ISAAC_NUCLEUS_DIR, ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.utils.noise import AdditiveUniformNoiseCfg as Unoise
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import ActionsCfg, CurriculumCfg, RewardsCfg, TerminationsCfg, CommandsCfg
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-##
-# Pre-defined configs
-##
-from omni.isaac.lab.terrains.config.rough import ROUGH_TERRAINS_CFG
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-
-
-@configclass
-class Go1VisionRoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 24
-    max_iterations = 5000
-    save_interval = 50
-    experiment_name = "go1_vision_rough"
-    empirical_normalization = False
-    policy = RslRlPpoActorCriticCfg(
-        init_noise_std=1.0,
-        actor_hidden_dims=[512, 256, 128],
-        critic_hidden_dims=[512, 256, 128],
-        activation="elu",
-    )
-    algorithm = RslRlPpoAlgorithmCfg(
-        value_loss_coef=1.0,
-        use_clipped_value_loss=True,
-        clip_param=0.2,
-        entropy_coef=0.01,
-        num_learning_epochs=5,
-        num_mini_batches=4,
-        learning_rate=1.0e-3,
-        schedule="adaptive",
-        gamma=0.99,
-        lam=0.95,
-        desired_kl=0.01,
-        max_grad_norm=1.0,
-    )
-
-##
-# Robot configuration
-##
-UNITREE_GO1_CFG = ArticulationCfg(
-    spawn=sim_utils.UsdFileCfg(
-        usd_path=f"{ISAACLAB_NUCLEUS_DIR}/Robots/Unitree/Go1/go1.usd",
-        activate_contact_sensors=True,
-        rigid_props=sim_utils.RigidBodyPropertiesCfg(
-            disable_gravity=False,
-            retain_accelerations=False,
-            linear_damping=0.0,
-            angular_damping=0.0,
-            max_linear_velocity=1000.0,
-            max_angular_velocity=1000.0,
-            max_depenetration_velocity=1.0,
-        ),
-        articulation_props=sim_utils.ArticulationRootPropertiesCfg(
-            enabled_self_collisions=False, solver_position_iteration_count=4, solver_velocity_iteration_count=0
-        ),
-    ),
-    init_state=ArticulationCfg.InitialStateCfg(
-        pos=(0.0, 0.0, 0.4),
-        joint_pos={
-            ".*L_hip_joint": 0.1,
-            ".*R_hip_joint": -0.1,
-            "F[L,R]_thigh_joint": 0.8,
-            "R[L,R]_thigh_joint": 1.0,
-            ".*_calf_joint": -1.5,
-        },
-        joint_vel={".*": 0.0},
-    ),
-    soft_joint_pos_limit_factor=0.9,
-    actuators={
-        "base_legs": ImplicitActuatorCfg(
-            joint_names_expr=[".*_hip_joint", ".*_thigh_joint", ".*_calf_joint"],
-            effort_limit=23.5,
-            velocity_limit=30.0,
-            stiffness=30.0,
-            damping=0.5,
-        )
-    },
-)
-
-
-##
-# Scene definition
-##
-@configclass
-class Go1SceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="generator",
-        terrain_generator=ROUGH_TERRAINS_CFG,
-        max_init_terrain_level=5,
-        collision_group=-1,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        visual_material=sim_utils.MdlFileCfg(
-            mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
-            project_uvw=True,
-            texture_scale=(0.25, 0.25),
-        ),
-        debug_vis=False,
-    )
-
-    # robots
-    robot: ArticulationCfg = UNITREE_GO1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-
-    # sensors
-    # height_scanner = RayCasterCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/base",
-    #     offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 20.0)),
-    #     attach_yaw_only=True,
-    #     pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-    #     debug_vis=False,
-    #     mesh_prim_paths=["/World/ground"],
-    # )
-    depth_sensor = RayCasterCameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/trunk",
-        mesh_prim_paths=["/World/ground"],
-        offset=RayCasterCameraCfg.OffsetCfg(pos=(0.245+0.027, 0.0075, 0.072+0.02), rot=(0.389, 0.0, 0.921, 0.0)),
-        data_types=["distance_to_image_plane"],
-        debug_vis=False,
-        pattern_cfg=patterns.PinholeCameraPatternCfg(
-            # focal_length=24.0,
-            # horizontal_aperture=46.0,
-            focal_length=1.93, horizontal_aperture=3.8,
-            height=24,
-            width=32,
-        ),
-        max_distance=3,
-    )
-    # depth_sensor2 = RayCasterCameraCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/trunk",
-    #     mesh_prim_paths=["/World/ground"],
-    #     offset=RayCasterCameraCfg.OffsetCfg(pos=(0.245+0.027, 0.0075, 0.072+0.02), rot=(0.389, 0.0, 0.921, 0.0)),
-    #     data_types=["distance_to_image_plane"],
-    #     debug_vis=True,
-    #     pattern_cfg=patterns.PinholeCameraPatternCfg(
-    #         # focal_length=24.0,
-    #         # horizontal_aperture=46.0,
-    #         focal_length=1.93, horizontal_aperture=3.8,
-    #         height=53,
-    #         width=30,
-    #     ),
-    #     max_distance=3,
-    # )
-
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True)
-
-    # lights
-    sky_light = AssetBaseCfg(
-        prim_path="/World/skyLight",
-        spawn=sim_utils.DomeLightCfg(
-            intensity=750.0,
-            texture_file=f"{ISAAC_NUCLEUS_DIR}/Materials/Textures/Skies/PolyHaven/kloofendal_43d_clear_puresky_4k.hdr",
-        ),
-    )
-
-
-##
-# Rewards
-##
-@configclass 
-class CustomGo1RewardsCfg(RewardsCfg):
-    joint_deviation = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.1,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_joint", ".*_thigh_joint", ".*_calf_joint"])},
-    )
-    # feet_stumble = RewTerm(
-    #     func=mdp.feet_stumble,
-    #     weight=-0.02,
-    #     params={
-    #         "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*_foot"),
-    #     },
-    # )
-
-##
-# Observations
-##
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        # base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        # projected_gravity = ObsTerm(
-        #     func=mdp.projected_gravity,
-        #     noise=Unoise(n_min=-0.05, n_max=0.05),
-        # )
-        base_rpy = ObsTerm(func=mdp.base_rpy, noise=Unoise(n_min=-0.1, n_max=0.1))
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-
-        depth_image = ObsTerm(
-            func=mdp.process_depth_image, 
-            params={
-                "sensor_cfg": SceneEntityCfg("depth_sensor"), 
-                "data_type": "distance_to_image_plane", 
-                "visualize": False
-            },
-            noise=Unoise(n_min=-0.1, n_max=0.1))
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    @configclass
-    class ProprioCfg(ObsGroup):
-        """Observations for proprioceptive group."""
-
-        # observation terms
-        # base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        # projected_gravity = ObsTerm(
-        #     func=mdp.projected_gravity,
-        #     noise=Unoise(n_min=-0.05, n_max=0.05),
-        # )
-        base_rpy = ObsTerm(func=mdp.base_rpy, noise=Unoise(n_min=-0.1, n_max=0.1))
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-
-        def __post_init__(self):
-            self.concatenate_terms = True
-
-    # @configclass
-    # class TestDepthCfg(ObsGroup):
-    #     """Observations for depth sensor."""
-
-    #     # observation terms
-    #     depth_image = ObsTerm(
-    #         func=mdp.process_depth_image, 
-    #         params={"sensor_cfg": SceneEntityCfg("depth_sensor2"), "data_type": "distance_to_image_plane", "visualize": True},
-    #         noise=Unoise(n_min=-0.1, n_max=0.1)
-    #     )
-
-    #     def __post_init__(self):
-    #         self.concatenate_terms = True
-
-    # test_depth: TestDepthCfg = TestDepthCfg()
-
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-    proprio: ProprioCfg = ProprioCfg()
-
-
-##
-# Events (for domain randomization)
-##
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    # startup
-    physics_material = EventTerm(
-        func=mdp.randomize_rigid_body_material,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-            "static_friction_range": (0.8, 0.8),
-            "dynamic_friction_range": (0.6, 0.6),
-            "restitution_range": (0.0, 0.0),
-            "num_buckets": 64,
-        },
-    )
-
-    add_base_mass = EventTerm(
-        func=mdp.randomize_rigid_body_mass,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="base"),
-            "mass_distribution_params": (-5.0, 5.0),
-            "operation": "add",
-        },
-    )
-
-    actuator_gains = EventTerm(
-        func=mdp.randomize_actuator_gains,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", joint_names=".*"),
-            "stiffness_distribution_params": (0.8, 1.2),
-            "operation": "scale",
-        },
-    )
-
-    # reset
-    base_external_force_torque = EventTerm(
-        func=mdp.apply_external_force_torque,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="base"),
-            "force_range": (0.0, 0.0),
-            "torque_range": (-0.0, 0.0),
-        },
-    )
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_uniform,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (-0.5, 0.5),
-                "y": (-0.5, 0.5),
-                "z": (-0.5, 0.5),
-                "roll": (-0.5, 0.5),
-                "pitch": (-0.5, 0.5),
-                "yaw": (-0.5, 0.5),
-            },
-        },
-    )
-
-    reset_robot_joints = EventTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "position_range": (0.5, 1.5),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-    reset_depth_sensor = EventTerm(
-        func=mdp.reset_camera_pos_uniform,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("depth_sensor"),
-            "pose_delta_range": {
-                "x": (-0.01, 0.01),
-                "y": (-0.01, 0.01),
-                "z": (-0.03, 0.03),
-                "roll": (-0.1, 0.1),
-                "pitch": (-0.1, 0.1),
-                "yaw": (-0.1, 0.1),
-            },
-        }
-    )
-
-    # interval
-    push_robot = EventTerm(
-        func=mdp.push_by_setting_velocity,
-        mode="interval",
-        interval_range_s=(8.0, 12.0),
-        params={"velocity_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5)}},
-    )
-
-
-@configclass
-class Go1VisionRoughEnvCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the Go1 locomotion velocity-tracking environment."""
-
-    scene: Go1SceneCfg = Go1SceneCfg(num_envs=4096, env_spacing=2.5)
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # MDP settings
-    rewards: RewardsCfg = CustomGo1RewardsCfg()
-    terminations: TerminationsCfg = TerminationsCfg()
-    events: EventCfg = EventCfg()
-    curriculum: CurriculumCfg = CurriculumCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 4
-        self.episode_length_s = 20.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material = self.scene.terrain.physics_material
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "average"
-        self.sim.physics_material.restitution_combine_mode = "average"
-
-        # scale down the terrains because the robot is small
-        self.scene.terrain.terrain_generator.sub_terrains["boxes"].grid_height_range = (0.025, 0.1)
-        self.scene.terrain.terrain_generator.sub_terrains["random_rough"].noise_range = (0.01, 0.06)
-        self.scene.terrain.terrain_generator.sub_terrains["random_rough"].noise_step = 0.01
-
-        # reduce action scale
-        self.actions.joint_pos.scale = 0.25
-
-        # event
-        self.events.push_robot = None
-        self.events.add_base_mass.params["mass_distribution_params"] = (0.0, 3.0)
-        self.events.add_base_mass.params["asset_cfg"].body_names = "trunk"
-        self.events.base_external_force_torque.params["asset_cfg"].body_names = "trunk"
-        self.events.reset_robot_joints.params["position_range"] = (1.0, 1.0)
-        self.events.reset_base.params = {
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (0.0, 0.0),
-                "y": (0.0, 0.0),
-                "z": (0.0, 0.0),
-                "roll": (0.0, 0.0),
-                "pitch": (0.0, 0.0),
-                "yaw": (0.0, 0.0),
-            },
-        }
-
-        # rewards
-        self.rewards.feet_air_time.params["sensor_cfg"].body_names = ".*_foot"
-        self.rewards.feet_air_time.weight = 0.01
-        self.rewards.undesired_contacts = None
-        self.rewards.dof_torques_l2.weight = -0.0002
-        self.rewards.track_lin_vel_xy_exp.weight = 1.5
-        self.rewards.track_ang_vel_z_exp.weight = 0.75
-        self.rewards.dof_acc_l2.weight = -2.5e-7
-        # self.rewards.dof_pos_limits.weight = -0.0002
-
-        # Commands
-        self.commands.base_velocity.ranges.lin_vel_x = (0.0, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-
-        # update sensor update periods
-        if self.scene.depth_sensor is not None:
-            self.scene.depth_sensor.update_period = self.decimation * self.sim.dt
-
-        # terminations
-        self.terminations.base_contact.params["sensor_cfg"].body_names = "trunk"
-
-        # check if terrain levels curriculum is enabled - if so, enable curriculum for terrain generator
-        # this generates terrains with increasing difficulty and is useful for training
-        if getattr(self.curriculum, "terrain_levels", None) is not None:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = True
-        else:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = False
-
-
-@configclass
-class Go1VisionRoughEnvCfg_PLAY(Go1VisionRoughEnvCfg):
-    def __post_init__(self):
-        # post init of parent
-        super().__post_init__()
-
-        # make a smaller scene for play
-        self.scene.num_envs = 50
-        self.scene.env_spacing = 2.5
-        # spawn the robot randomly in the grid (instead of their terrain levels)
-        self.scene.terrain.max_init_terrain_level = None
-        # reduce the number of terrains to save memory
-        if self.scene.terrain.terrain_generator is not None:
-            self.scene.terrain.terrain_generator.num_rows = 5
-            self.scene.terrain.terrain_generator.num_cols = 5
-            self.scene.terrain.terrain_generator.curriculum = False
-            self.scene.terrain.terrain_generator.sub_terrains["pyramid_stairs"].step_height_range = (0.17, 0.17)
-            self.scene.terrain.terrain_generator.sub_terrains["pyramid_stairs_inv"].step_height_range = (0.17, 0.17)
-
-        # disable randomization for play
-        self.observations.policy.enable_corruption = False
-        # remove random pushing event
-        self.events.base_external_force_torque = None
-        self.events.push_robot = None
-        self.events.actuator_gains = None
-        self.events.reset_depth_sensor = EventTerm(
-            func=mdp.reset_camera_pos_uniform,
-            mode="reset",
-            params={
-                "asset_cfg": SceneEntityCfg("depth_sensor"),
-                "pose_delta_range": {},
-            }
-        )
-
-        # Commands
-        self.commands.base_velocity.ranges.lin_vel_x = (0.5, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/__init__.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/__init__.py
deleted file mode 100644
index 5e50558..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/__init__.py
+++ /dev/null
@@ -1,60 +0,0 @@
-import gymnasium as gym
-
-from .go2_low_base_cfg import Go2BaseRoughEnvCfg, Go2BaseRoughEnvCfg_PLAY, Go2RoughPPORunnerCfg
-from .go2_low_vision_cfg import Go2VisionRoughEnvCfg, Go2VisionRoughEnvCfg_PLAY, Go2VisionRoughPPORunnerCfg
-from .go2_matterport_cfg import Go2MatterportCfg
-
-##
-# Register Gym environments.
-##
-
-gym.register(
-    id="go2_base",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": Go2BaseRoughEnvCfg,
-        "rsl_rl_cfg_entry_point": Go2RoughPPORunnerCfg,
-    },
-)
-
-
-gym.register(
-    id="go2_base_play",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": Go2BaseRoughEnvCfg_PLAY,
-        "rsl_rl_cfg_entry_point": Go2RoughPPORunnerCfg,
-    },
-)
-
-gym.register(
-    id="go2_vision",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": Go2VisionRoughEnvCfg,
-        "rsl_rl_cfg_entry_point": Go2VisionRoughPPORunnerCfg,
-    },
-)
-
-gym.register(
-    id="go2_vision_play",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": Go2VisionRoughEnvCfg_PLAY,
-        "rsl_rl_cfg_entry_point": Go2VisionRoughPPORunnerCfg,
-    },
-)
-
-gym.register(
-    id="go2_matterport",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": Go2MatterportCfg,
-        "rsl_rl_cfg_entry_point": Go2VisionRoughPPORunnerCfg,
-    },
-)
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_low_base_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_low_base_cfg.py
deleted file mode 100644
index 2aa17a0..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_low_base_cfg.py
+++ /dev/null
@@ -1,536 +0,0 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from omni.isaac.lab.utils import configclass
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import ArticulationCfg, AssetBaseCfg
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import LocomotionVelocityRoughEnvCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg
-import omni.isaac.lab.terrains as terrain_gen
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.lab.actuators import ImplicitActuatorCfg, DelayedPDActuatorCfg
-from omni.isaac.lab.utils.assets import ISAAC_NUCLEUS_DIR, ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.utils.noise import AdditiveUniformNoiseCfg as Unoise
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import ActionsCfg, CurriculumCfg, RewardsCfg, EventCfg, TerminationsCfg, CommandsCfg
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-##
-# Pre-defined configs
-##
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-
-
-@configclass
-class Go2RoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 24
-    max_iterations = 5000
-    save_interval = 50
-    experiment_name = "go2_base"
-    empirical_normalization = False
-    policy = RslRlPpoActorCriticCfg(
-        init_noise_std=1.0,
-        actor_hidden_dims=[512, 256, 128],
-        critic_hidden_dims=[512, 256, 128],
-        activation="elu",
-    )
-    algorithm = RslRlPpoAlgorithmCfg(
-        value_loss_coef=1.0,
-        use_clipped_value_loss=True,
-        clip_param=0.2,
-        entropy_coef=0.01,
-        num_learning_epochs=5,
-        num_mini_batches=4,
-        learning_rate=1.0e-3,
-        schedule="adaptive",
-        gamma=0.99,
-        lam=0.95,
-        desired_kl=0.01,
-        max_grad_norm=1.0,
-    )
-
-
-UNITREE_GO2_CFG = ArticulationCfg(
-    spawn=sim_utils.UsdFileCfg(
-        usd_path=f"{ISAACLAB_NUCLEUS_DIR}/Robots/Unitree/Go2/go2.usd",
-        activate_contact_sensors=True,
-        rigid_props=sim_utils.RigidBodyPropertiesCfg(
-            disable_gravity=False,
-            retain_accelerations=False,
-            linear_damping=0.0,
-            angular_damping=0.0,
-            max_linear_velocity=1000.0,
-            max_angular_velocity=1000.0,
-            max_depenetration_velocity=1.0,
-        ),
-        articulation_props=sim_utils.ArticulationRootPropertiesCfg(
-            enabled_self_collisions=False, solver_position_iteration_count=4, solver_velocity_iteration_count=0
-        ),
-    ),
-    init_state=ArticulationCfg.InitialStateCfg(
-        pos=(0.0, 0.0, 0.5),
-        joint_pos={
-            ".*L_hip_joint": 0.1,
-            ".*R_hip_joint": -0.1,
-            "F[L,R]_thigh_joint": 0.8,
-            "R[L,R]_thigh_joint": 1.0,
-            ".*_calf_joint": -1.5,
-        },
-        joint_vel={".*": 0.0},
-    ),
-    soft_joint_pos_limit_factor=0.9,
-    actuators={
-        "base_legs": DelayedPDActuatorCfg(
-            joint_names_expr=[".*_hip_joint", ".*_thigh_joint", ".*_calf_joint"],
-            effort_limit=40.0,
-            velocity_limit=30.0,
-            stiffness=40.0,
-            damping=1.0,
-            friction=0.0,
-            min_delay=4,
-            max_delay=4,
-        )
-    }
-)
-
-
-##
-# Configuration for custom terrains.
-##
-Go2_BASE_TERRAINS_CFG = TerrainGeneratorCfg(
-    size=(8.0, 8.0),
-    border_width=20.0,
-    num_rows=10,
-    num_cols=20,
-    horizontal_scale=0.1,
-    vertical_scale=0.005,
-    slope_threshold=0.75,
-    use_cache=False,
-    sub_terrains={
-        # "pyramid_stairs": terrain_gen.MeshPyramidStairsTerrainCfg(
-        #     proportion=0.2,
-        #     step_height_range=(0.05, 0.2),
-        #     step_width=0.3,
-        #     platform_width=3.0,
-        #     border_width=1.0,
-        #     holes=False,
-        # ),
-        # "pyramid_stairs_inv": terrain_gen.MeshInvertedPyramidStairsTerrainCfg(
-        #     proportion=0.2,
-        #     step_height_range=(0.05, 0.2),
-        #     step_width=0.3,
-        #     platform_width=3.0,
-        #     border_width=1.0,
-        #     holes=False,
-        # ),
-        # "boxes": terrain_gen.MeshRandomGridTerrainCfg(
-        #     proportion=0.2, grid_width=0.45, grid_height_range=(0.05, 0.1), platform_width=2.0
-        # ),
-        "flat": terrain_gen.MeshPlaneTerrainCfg(proportion=0.4),
-        "random_rough": terrain_gen.HfRandomUniformTerrainCfg(
-            proportion=0.4, noise_range=(0.02, 0.05), noise_step=0.02, border_width=0.25
-        ),
-        # "gaps": terrain_gen.MeshGapTerrainCfg(
-        #     proportion=0.1, gap_width_range=(0.5, 1.0), platform_width=2.0
-        # ),
-        # "hf_pyramid_slope": terrain_gen.HfPyramidSlopedTerrainCfg(
-        #     proportion=0.2, slope_range=(0.0, 0.02), platform_width=2.0, border_width=0.25
-        # ),
-        # "hf_pyramid_slope_inv": terrain_gen.HfInvertedPyramidSlopedTerrainCfg(
-        #     proportion=0.2, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25 ),
-    },
-)
-
-
-##
-# Scene definition
-##
-@configclass
-class Go2SceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="generator",
-        terrain_generator=Go2_BASE_TERRAINS_CFG,
-        max_init_terrain_level=2,
-        collision_group=-1,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        visual_material=sim_utils.MdlFileCfg(
-            mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
-            project_uvw=True,
-            texture_scale=(0.25, 0.25),
-        ),
-        debug_vis=False,
-    )
-
-    # robots
-    robot: ArticulationCfg = UNITREE_GO2_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-
-    # sensors
-    height_scanner = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 20.0)),
-        attach_yaw_only=True,
-        pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-        debug_vis=False,
-        mesh_prim_paths=["/World/ground"],
-    )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True)
-
-    # lights
-    sky_light = AssetBaseCfg(
-        prim_path="/World/skyLight",
-        spawn=sim_utils.DomeLightCfg(
-            intensity=750.0,
-            texture_file=f"{ISAAC_NUCLEUS_DIR}/Materials/Textures/Skies/PolyHaven/kloofendal_43d_clear_puresky_4k.hdr",
-        ),
-    )
-
-
-##
-# Rewards
-##
-@configclass 
-class CustomGo2RewardsCfg(RewardsCfg):
-    hip_deviation = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.4,
-        # params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_joint", ".*_thigh_joint", ".*_calf_joint"])},
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_joint"])},
-    )
-    joint_deviation = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.04,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_thigh_joint", ".*_calf_joint"])},
-    )
-    base_height = RewTerm(
-        func=mdp.base_height_l2,
-        weight=-5.0,
-        params={"target_height": 0.32},
-    )
-    action_smoothness = RewTerm(
-        func=mdp.action_smoothness_penalty,
-        weight=-0.01,
-    )
-    joint_power = RewTerm(
-        func=mdp.power_penalty,
-        weight=-2e-5,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=".*")},
-    )
-    # feet_slide = RewTerm(
-    #     func=mdp.feet_slide,
-    #     weight=-0.05,
-    #     params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*_foot")},
-    # )
-
-
-##
-# Observations
-##
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        # base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        # projected_gravity = ObsTerm(
-        #     func=mdp.projected_gravity,
-        #     noise=Unoise(n_min=-0.05, n_max=0.05),
-        # )
-        base_rpy = ObsTerm(func=mdp.base_rpy, noise=Unoise(n_min=-0.1, n_max=0.1))
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    
-    @configclass
-    class ProprioCfg(ObsGroup):
-        """Observations for proprioceptive group."""
-
-        # observation terms
-        # base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        # projected_gravity = ObsTerm(
-        #     func=mdp.projected_gravity,
-        #     noise=Unoise(n_min=-0.05, n_max=0.05),
-        # )
-        base_rpy = ObsTerm(func=mdp.base_rpy, noise=Unoise(n_min=-0.1, n_max=0.1))
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-
-        def __post_init__(self):
-            self.concatenate_terms = True
-    
-    @configclass
-    class CriticObsCfg(ObsGroup):
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        projected_gravity = ObsTerm(
-            func=mdp.projected_gravity,
-            noise=Unoise(n_min=-0.05, n_max=0.05),
-        )
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-        height_scan = ObsTerm(
-            func=mdp.height_scan,
-            params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-            clip=(-1.0, 1.0),
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = False
-            self.concatenate_terms = True
-
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-    proprio: ProprioCfg = ProprioCfg()
-    critic: CriticObsCfg = CriticObsCfg()
-
-
-##
-# Events (for domain randomization)
-##
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    # startup
-    physics_material = EventTerm(
-        func=mdp.randomize_rigid_body_material,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-            "static_friction_range": (0.4, 4.0),
-            "dynamic_friction_range": (0.4, 2.0),
-            "restitution_range": (0.0, 0.0),
-            "num_buckets": 64,
-        },
-    )
-
-    add_base_mass = EventTerm(
-        func=mdp.randomize_rigid_body_mass,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="base"),
-            "mass_distribution_params": (-5.0, 5.0),
-            "operation": "add",
-        },
-    )
-
-    actuator_gains = EventTerm(
-        func=mdp.randomize_actuator_gains,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", joint_names=".*"),
-            "stiffness_distribution_params": (0.8, 1.2),
-            "operation": "scale",
-        },
-    )
-
-    # reset
-    base_external_force_torque = EventTerm(
-        func=mdp.apply_external_force_torque,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="base"),
-            "force_range": (0.0, 0.0),
-            "torque_range": (-0.0, 0.0),
-        },
-    )
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_uniform,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (-0.5, 0.5),
-                "y": (-0.5, 0.5),
-                "z": (-0.5, 0.5),
-                "roll": (-0.5, 0.5),
-                "pitch": (-0.5, 0.5),
-                "yaw": (-0.5, 0.5),
-            },
-        },
-    )
-
-    reset_robot_joints = EventTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "position_range": (0.5, 1.5),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-    # interval
-    push_robot = EventTerm(
-        func=mdp.push_by_setting_velocity,
-        mode="interval",
-        interval_range_s=(10.0, 15.0),
-        params={"velocity_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5)}},
-    )
-
-
-@configclass
-class Go2BaseRoughEnvCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the Go2 locomotion velocity-tracking environment."""
-
-    scene: Go2SceneCfg = Go2SceneCfg(num_envs=4096, env_spacing=2.5)
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # MDP settings
-    rewards: RewardsCfg = CustomGo2RewardsCfg()
-    terminations: TerminationsCfg = TerminationsCfg()
-    events: EventCfg = EventCfg()
-    curriculum: CurriculumCfg = CurriculumCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 4
-        self.sim.render_interval = 4
-        self.episode_length_s = 20.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material = self.scene.terrain.physics_material
-        # self.sim.physics_material.static_friction = 1.0
-        # self.sim.physics_material.dynamic_friction = 1.0
-        # self.sim.physics_material.friction_combine_mode = "multiply"
-        # self.sim.physics_material.restitution_combine_mode = "multiply"
-
-        # scale down the terrains because the robot is small
-        # self.scene.terrain.terrain_generator.sub_terrains["boxes"].grid_height_range = (0.025, 0.1)
-        # self.scene.terrain.terrain_generator.sub_terrains["random_rough"].noise_range = (0.01, 0.04)
-        # self.scene.terrain.terrain_generator.sub_terrains["random_rough"].noise_step = 0.01
-
-        # reduce action scale
-        self.actions.joint_pos.scale = 0.25
-
-        # event
-        # self.events.push_robot = None
-        self.events.actuator_gains = None
-        self.events.add_base_mass.params["mass_distribution_params"] = (-3.0, 3.0)
-        self.events.add_base_mass.params["asset_cfg"].body_names = "base"
-        self.events.base_external_force_torque.params["asset_cfg"].body_names = "base"
-        self.events.reset_robot_joints.params["position_range"] = (1.0, 1.0)
-        self.events.reset_base.params = {
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (0.0, 0.0),
-                "y": (0.0, 0.0),
-                "z": (0.0, 0.0),
-                "roll": (0.0, 0.0),
-                "pitch": (0.0, 0.0),
-                "yaw": (0.0, 0.0),
-            },
-        }
-
-        # rewards
-        self.rewards.feet_air_time.params["sensor_cfg"].body_names = ".*_foot"
-        # self.rewards.feet_air_time.weight = 0.01
-        self.rewards.undesired_contacts = None
-        self.rewards.dof_torques_l2.weight = -0.0002
-        self.rewards.track_lin_vel_xy_exp.weight = 1.5
-        self.rewards.track_ang_vel_z_exp.weight = 0.75
-        self.rewards.dof_acc_l2.weight = -2.5e-7
-        # self.rewards.dof_pos_limits.weight = -0.0001
-
-        # Commands
-        self.commands.base_velocity.ranges.lin_vel_x = (-1.0, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (-0.5, 0.5)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-        self.commands.base_velocity.rel_standing_envs = 0.1
-
-        # terminations
-        self.terminations.base_contact.params["sensor_cfg"].body_names = "base"
-        self.events.add_base_mass.params["asset_cfg"].body_names = "base"
-        # check if terrain levels curriculum is enabled - if so, enable curriculum for terrain generator
-        # this generates terrains with increasing difficulty and is useful for training
-
-        # flat terrain
-        # self.scene.terrain.terrain_type = "plane"
-        # self.scene.terrain.terrain_generator = None
-        # self.curriculum.terrain_levels = None
-        self.rewards.flat_orientation_l2.weight = -2.5
-        self.rewards.feet_air_time.weight = 0.2
-
-        # update sensor period
-        self.scene.contact_forces.update_period = self.sim.dt
-        self.scene.height_scanner.update_period = self.sim.dt * self.decimation
-
-        if getattr(self.curriculum, "terrain_levels", None) is not None:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = True
-        else:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = False
-
-
-@configclass
-class Go2BaseRoughEnvCfg_PLAY(Go2BaseRoughEnvCfg):
-    def __post_init__(self):
-        # post init of parent
-        super().__post_init__()
-
-        # make a smaller scene for play
-        self.scene.num_envs = 50
-        self.scene.env_spacing = 2.5
-        # spawn the robot randomly in the grid (instead of their terrain levels)
-        self.scene.terrain.max_init_terrain_level = None
-        # reduce the number of terrains to save memory
-        if self.scene.terrain.terrain_generator is not None:
-            self.scene.terrain.terrain_generator.num_rows = 5
-            self.scene.terrain.terrain_generator.num_cols = 5
-            self.scene.terrain.terrain_generator.curriculum = False
-
-        # disable randomization for play
-        self.observations.policy.enable_corruption = False
-        # remove random pushing event
-        self.events.base_external_force_torque = None
-        self.events.push_robot = None
-        self.events.actuator_gains = None
-
-        # Commands
-        self.commands.base_velocity.ranges.lin_vel_x = (-1.0, -0.5)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (0.0, 0.0)
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_low_base_nav_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_low_base_nav_cfg.py
deleted file mode 100644
index b07cd19..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_low_base_nav_cfg.py
+++ /dev/null
@@ -1,189 +0,0 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import math
-
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.utils import configclass
-from omni.isaac.lab.utils.assets import ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import ActionsCfg, CurriculumCfg, RewardsCfg, EventCfg, TerminationsCfg
-
-import omni.isaac.lab_tasks.manager_based.navigation.mdp as mdp
-from omni.isaac.viplanner.config.go2 import Go2VisionRoughEnvCfg
-
-
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_uniform,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (-0.0, 0.0),
-                "y": (-0.0, 0.0),
-                "z": (-0.0, 0.0),
-                "roll": (-0.0, 0.0),
-                "pitch": (-0.0, 0.0),
-                "yaw": (-0.0, 0.0),
-            },
-        },
-    )
-
-
-@configclass
-class ActionsCfg:
-    """Action terms for the MDP."""
-    joint_pos = mdp.JointPositionActionCfg(asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True)
-
-
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        pose_command = ObsTerm(func=mdp.generated_commands, params={"command_name": "pose_command"})
-
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-
-
-@configclass
-class RewardsCfg:
-    """Reward terms for the MDP."""
-    # navigation reward terms
-    termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    position_tracking = RewTerm(
-        func=mdp.position_command_error_tanh,
-        weight=0.5,
-        params={"std": 2.0, "command_name": "pose_command"},
-    )
-    position_tracking_fine_grained = RewTerm(
-        func=mdp.position_command_error_tanh,
-        weight=0.5,
-        params={"std": 0.2, "command_name": "pose_command"},
-    )
-    orientation_tracking = RewTerm(
-        func=mdp.heading_command_error_abs,
-        weight=-0.2,
-        params={"command_name": "pose_command"},
-    )
-
-    # regularization terms
-    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-2.0)
-    ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
-    dof_torques_l2 = RewTerm(func=mdp.joint_torques_l2, weight=-1.0e-5)
-    dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-2.5e-7)
-    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
-    feet_air_time = RewTerm(
-        func=mdp.feet_air_time,
-        weight=0.125,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*_foot"),
-            "command_name": "base_velocity",
-            "threshold": 0.5,
-        },
-    )
-    # collision = RewTerm(
-    #     func=mdp.undesired_contacts,
-    #     weight=-1.0,
-    #     params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*_thigh"), "threshold": 1.0},
-    # )
-    flat_orientation_l2 = RewTerm(func=mdp.flat_orientation_l2, weight=-2.5)
-    dof_pos_limits = RewTerm(func=mdp.joint_pos_limits, weight=0.2)
-    hip_deviation = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.4,
-        # params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_joint", ".*_thigh_joint", ".*_calf_joint"])},
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_joint"])},
-    )
-    joint_deviation = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.04,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_thigh_joint", ".*_calf_joint"])},
-    )
-    base_height = RewTerm(
-        func=mdp.base_height_l2,
-        weight=-5.0,
-        params={"target_height": 0.32},
-    )
-
-
-@configclass
-class CommandsCfg:
-    """Command terms for the MDP."""
-
-    pose_command = mdp.TerrainBasedPose2dCommandCfg(
-        asset_name="robot",
-        simple_heading=False,
-        resampling_time_range=(8.0, 8.0),
-        debug_vis=True,
-        ranges=mdp.UniformPose2dCommandCfg.Ranges(pos_x=(-3.0, 3.0), pos_y=(-3.0, 3.0), heading=(-math.pi, math.pi)),
-    )
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    illegal_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*"), "threshold": 1.0},
-    )
-
-
-@configclass
-class Go2BaseNavigationEnvCfg(ManagerBasedRLEnvCfg):
-    scene: SceneEntityCfg = Go2VisionRoughEnvCfg.scene
-    commands: CommandsCfg = CommandsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    observations: ObservationsCfg = ObservationsCfg()
-    rewards: RewardsCfg = RewardsCfg()
-    events: EventCfg = EventCfg()
-
-    curriculum: CurriculumCfg = Go2VisionRoughEnvCfg.CurriculumCfg()
-    terminations: TerminationsCfg = TerminationsCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-
-        self.sim.dt = Go2VisionRoughEnvCfg.sim.dt
-        self.sim.render_interval = Go2VisionRoughEnvCfg.decimation
-        self.decimation = Go2VisionRoughEnvCfg.decimation
-        self.episode_length_s = self.commands.pose_command.resampling_time_range[1]
-
-        if self.scene.height_scanner is not None:
-            self.scene.height_scanner.update_period = (
-                self.actions.pre_trained_policy_action.low_level_decimation * self.sim.dt
-            )
-        if self.scene.contact_forces is not None:
-            self.scene.contact_forces.update_period = self.sim.dt
-
-
-class Go2BaseNavigationEnvCfg_PLAY(Go2BaseNavigationEnvCfg):
-    def __post_init__(self) -> None:
-        # post init of parent
-        super().__post_init__()
-
-        # make a smaller scene for play
-        self.scene.num_envs = 50
-        self.scene.env_spacing = 2.5
-        # disable randomization for play
-        self.observations.policy.enable_corruption = False
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_low_vision_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_low_vision_cfg.py
deleted file mode 100644
index a315f2c..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_low_vision_cfg.py
+++ /dev/null
@@ -1,516 +0,0 @@
-import platform
-from omni.isaac.lab.utils import configclass
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import ArticulationCfg, AssetBaseCfg
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import LocomotionVelocityRoughEnvCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg, FlatPatchSamplingCfg
-import omni.isaac.lab.terrains as terrain_gen
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import ContactSensorCfg, RayCasterCfg, patterns, RayCasterCameraCfg
-from omni.isaac.lab.actuators import ImplicitActuatorCfg, DelayedPDActuatorCfg
-from omni.isaac.lab.utils.assets import ISAAC_NUCLEUS_DIR, ISAACLAB_NUCLEUS_DIR#, ROBOT_DIR
-from omni.isaac.lab.utils.noise import AdditiveUniformNoiseCfg as Unoise
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import ActionsCfg, CurriculumCfg, RewardsCfg, EventCfg, CommandsCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-##
-# Pre-defined configs
-##
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-
-
-from .go2_low_base_cfg import Go2BaseRoughEnvCfg
-
-@configclass
-class Go2VisionRoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 24
-    max_iterations = 5000
-    save_interval = 50
-    experiment_name = "go2_vision"
-    empirical_normalization = False
-    policy = RslRlPpoActorCriticCfg(
-        init_noise_std=1.0,
-        actor_hidden_dims=[512, 256, 128],
-        critic_hidden_dims=[512, 256, 128],
-        activation="elu",
-    )
-    algorithm = RslRlPpoAlgorithmCfg(
-        value_loss_coef=1.0,
-        use_clipped_value_loss=True,
-        clip_param=0.2,
-        entropy_coef=0.01,
-        num_learning_epochs=5,
-        num_mini_batches=4,
-        learning_rate=1.0e-3,
-        schedule="adaptive",
-        gamma=0.99,
-        lam=0.95,
-        desired_kl=0.01,
-        max_grad_norm=1.0,
-    )
-
-
-UNITREE_GO2_CFG = ArticulationCfg(
-    spawn=sim_utils.UsdFileCfg(
-        # usd_path=f"/home/yji/humanoid_nav/h1-training-isaaclab/robots/go2.usd",
-        usd_path=f"{ISAACLAB_NUCLEUS_DIR}/Robots/Unitree/Go2/go2.usd",
-        activate_contact_sensors=True,
-        rigid_props=sim_utils.RigidBodyPropertiesCfg(
-            disable_gravity=False,
-            retain_accelerations=False,
-            linear_damping=0.0,
-            angular_damping=0.0,
-            max_linear_velocity=1000.0,
-            max_angular_velocity=1000.0,
-            max_depenetration_velocity=1.0,
-        ),
-        articulation_props=sim_utils.ArticulationRootPropertiesCfg(
-            enabled_self_collisions=False, solver_position_iteration_count=4, solver_velocity_iteration_count=0
-        ),
-    ),
-    init_state=ArticulationCfg.InitialStateCfg(
-        pos=(0.0, 0.0, 0.4),
-        joint_pos={
-            ".*L_hip_joint": 0.1,
-            ".*R_hip_joint": -0.1,
-            "F[L,R]_thigh_joint": 0.8,
-            "R[L,R]_thigh_joint": 1.0,
-            ".*_calf_joint": -1.5,
-        },
-        joint_vel={".*": 0.0},
-    ),
-    soft_joint_pos_limit_factor=0.9,
-    actuators={
-        "base_legs": DelayedPDActuatorCfg(
-            joint_names_expr=[".*_hip_joint", ".*_thigh_joint", ".*_calf_joint"],
-            effort_limit=40.0,
-            velocity_limit=30.0,
-            stiffness=40.0,
-            damping=1.0,
-            friction=0.0,
-            min_delay=4,
-            max_delay=4,
-        )
-    }
-)
-
-
-##
-# Configuration for custom terrains.
-##
-Go2_Vision_TERRAINS_CFG = TerrainGeneratorCfg(
-    size=(8.0, 8.0),
-    border_width=20.0,
-    num_rows=10,
-    num_cols=20,
-    horizontal_scale=0.1,
-    vertical_scale=0.005,
-    slope_threshold=0.75,
-    use_cache=False,
-    sub_terrains={
-        # "pyramid_stairs": terrain_gen.MeshPyramidStairsTerrainCfg(
-        #     proportion=0.2,
-        #     step_height_range=(0.05, 0.17),
-        #     step_width=0.3,
-        #     platform_width=3.0,
-        #     border_width=1.0,
-        #     holes=False,
-        # ),
-        # "pyramid_stairs_inv": terrain_gen.MeshInvertedPyramidStairsTerrainCfg(
-        #     proportion=0.2,
-        #     step_height_range=(0.05, 0.17),
-        #     step_width=0.3,
-        #     platform_width=3.0,
-        #     border_width=1.0,
-        #     holes=False,
-        # ),
-        # "boxes": terrain_gen.MeshRandomGridTerrainCfg(
-        #     proportion=0.2, grid_width=0.45, grid_height_range=(0.05, 0.1), platform_width=2.0
-        # ),
-        "flat": terrain_gen.MeshPlaneTerrainCfg(proportion=0.1),
-        "random_rough": terrain_gen.HfRandomUniformTerrainCfg(
-            proportion=0.1, noise_range=(0.02, 0.05), noise_step=0.02, border_width=0.25
-        ),
-        # "gaps": terrain_gen.MeshGapTerrainCfg(
-        #     proportion=0.1, gap_width_range=(0.5, 1.0), platform_width=2.0
-        # ),
-        "hf_pyramid_slope": terrain_gen.HfPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.2), platform_width=2.0, border_width=0.25
-        ),
-        "hf_pyramid_slope_inv": terrain_gen.HfInvertedPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.2), platform_width=2.0, border_width=0.25 
-        ),
-        "init_pos": terrain_gen.HfDiscreteObstaclesTerrainCfg(
-            proportion=0.3, 
-            num_obstacles=8,
-            obstacle_height_mode="fixed",
-            obstacle_height_range=(0.3, 2.0), obstacle_width_range=(0.25, 1.0), 
-            platform_width=0.0
-        ),
-        # "cylinder": terrain_gen.MeshRepeatedCylindersTerrainCfg(
-        #     proportion=0.2,
-        #     platform_width=0.0,
-        #     object_type="cylinder",
-        #     object_params_start=terrain_gen.MeshRepeatedCylindersTerrainCfg.ObjectCfg(
-        #         num_objects=4,
-        #         height=1.0,
-        #         radius=0.5
-        #     ),
-        #     object_params_end=terrain_gen.MeshRepeatedCylindersTerrainCfg.ObjectCfg(
-        #         num_objects=8,
-        #         height=0.6,
-        #         radius=0.2
-        #     ),
-        # )
-    },
-)
-for sub_terrain_name, sub_terrain_cfg in Go2_Vision_TERRAINS_CFG.sub_terrains.items():
-    sub_terrain_cfg.flat_patch_sampling = {
-        sub_terrain_name: FlatPatchSamplingCfg(num_patches=2, patch_radius=[0.01,0.1,0.2, 0.3, 0.5,0.6, 0.7, 1.0, 1.2, 1.5], max_height_diff=0.3)
-    }
-
-
-##
-# Scene definition
-##
-@configclass
-class Go2VisionSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="generator",
-        terrain_generator=Go2_Vision_TERRAINS_CFG,
-        max_init_terrain_level=5,
-        collision_group=-1,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        visual_material=sim_utils.MdlFileCfg(
-            mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
-            project_uvw=True,
-            texture_scale=(0.25, 0.25),
-        ),
-        debug_vis=False,
-    )
-
-    # robots
-    robot: ArticulationCfg = UNITREE_GO2_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-
-    # sensors
-    height_scanner = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 20.0)),
-        attach_yaw_only=True,
-        pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[3.0, 2.0]),
-        debug_vis=False,
-        mesh_prim_paths=["/World/ground"],
-    )
-    lidar_sensor = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/Head_lower",
-        # offset=RayCasterCfg.OffsetCfg(pos=(0.28945, 0.0, -0.046), rot=(0., -0.991,0.0,-0.131)),
-        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, -0.0), rot=(0., -0.991,0.0,-0.131)),
-        attach_yaw_only=False,
-        pattern_cfg=patterns.LidarPatternCfg(
-            channels=32, vertical_fov_range=(0.0, 90.0), horizontal_fov_range=(-180, 180.0), horizontal_res=4.0
-        ),
-        debug_vis=True,
-        mesh_prim_paths=["/World/ground"],
-    )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True)
-
-    # lights
-    sky_light = AssetBaseCfg(
-        prim_path="/World/skyLight",
-        spawn=sim_utils.DomeLightCfg(
-            intensity=750.0,
-            texture_file=f"{ISAAC_NUCLEUS_DIR}/Materials/Textures/Skies/PolyHaven/kloofendal_43d_clear_puresky_4k.hdr",
-        ),
-    )
-
-##
-# Rewards
-##
-@configclass 
-class CustomGo2RewardsCfg(RewardsCfg):
-    termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    hip_deviation = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.4,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_joint"])},
-    )
-    joint_deviation = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.04,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_thigh_joint", ".*_calf_joint"])},
-    )
-    base_height = RewTerm(
-        func=mdp.base_height_l2,
-        weight=-5.0,
-        params={"target_height": 0.32},
-    )
-    stand_still_penalty = RewTerm(
-        func=mdp.stand_still_penalty,
-        weight=-1.0,
-        params={
-            "command_name": "base_velocity",
-            "asset_cfg": SceneEntityCfg("robot", joint_names=[".*"])
-        },
-    )
-    action_smoothness = RewTerm(
-        func=mdp.action_smoothness_penalty,
-        weight=-0.02,
-    )
-    joint_power = RewTerm(
-        func=mdp.power_penalty,
-        weight=-2e-5,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=".*")},
-    )
-    feet_slide = RewTerm(
-        func=mdp.feet_slide,
-        weight=-0.05,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*foot"),
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*foot"),
-        },
-    )
-    # feet_stumble = RewTerm(
-    #     func=mdp.feet_stumble,
-    #     weight=-0.1,
-    #     params={
-    #         "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*foot"),
-    #     },
-    # )
-
-    collision = RewTerm(
-        func=mdp.collision_penalty,
-        weight=-5.0,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=["Head.*", ".*_hip", ".*_thigh", ".*_calf"]),
-            "threshold": 0.1,
-        },
-    )
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=["base"]), "threshold": 1.0},
-    )
-    hip_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=[".*_hip"]), "threshold": 1.0},
-    )
-    head_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=["Head.*"]), "threshold": 1.0},
-    )
-    leg_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=[".*_calf"]), "threshold": 1.0},
-    )
-    bad_orientation = DoneTerm(
-        func=mdp.bad_orientation,
-        params={"limit_angle": 1.0},
-    )
-
-##
-# Observations
-##
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        # base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        # projected_gravity = ObsTerm(
-        #     func=mdp.projected_gravity,
-        #     noise=Unoise(n_min=-0.05, n_max=0.05),
-        # )
-        base_rpy = ObsTerm(func=mdp.base_rpy, noise=Unoise(n_min=-0.1, n_max=0.1))
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-
-        height_scan = ObsTerm(
-            func=mdp.height_map_lidar,
-            params={"sensor_cfg": SceneEntityCfg("lidar_sensor"), "offset": 0.0},
-            clip=(-10.0, 10.0),
-            noise=Unoise(n_min=-0.02, n_max=0.02),
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    
-    @configclass
-    class ProprioCfg(ObsGroup):
-        """Observations for proprioceptive group."""
-
-        # observation terms
-        # base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        # projected_gravity = ObsTerm(
-        #     func=mdp.projected_gravity,
-        #     noise=Unoise(n_min=-0.05, n_max=0.05),
-        # )
-        base_rpy = ObsTerm(func=mdp.base_rpy, noise=Unoise(n_min=-0.1, n_max=0.1))
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-    
-    @configclass
-    class CriticObsCfg(ObsGroup):
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        projected_gravity = ObsTerm(
-            func=mdp.projected_gravity,
-            noise=Unoise(n_min=-0.05, n_max=0.05),
-        )
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-        height_scan = ObsTerm(
-            func=mdp.height_scan,
-            params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-            clip=(-1.0, 1.0),
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = False
-            self.concatenate_terms = True
-
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-    proprio: ProprioCfg = ProprioCfg()
-    critic: CriticObsCfg = CriticObsCfg()
-    # vision: VisionCfg = VisionCfg()
-
-@configclass
-class Go2VisionRoughEnvCfg(Go2BaseRoughEnvCfg):
-    """Configuration for the Go2 locomotion velocity-tracking environment."""
-
-    scene: Go2VisionSceneCfg = Go2VisionSceneCfg(num_envs=4096, env_spacing=2.5)
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # MDP settings
-    rewards: RewardsCfg = CustomGo2RewardsCfg()
-    terminations: TerminationsCfg = TerminationsCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-        super().__post_init__()
-        
-        # scale down the terrains because the robot is small
-        # self.scene.terrain.terrain_generator.sub_terrains["boxes"].grid_height_range = (0.025, 0.1)
-        self.scene.terrain.terrain_generator.sub_terrains["random_rough"].noise_range = (0.01, 0.04)
-        self.scene.terrain.terrain_generator.sub_terrains["random_rough"].noise_step = 0.01
-
-        # update sensor period
-        self.scene.height_scanner.update_period = self.sim.dt * self.decimation
-        self.scene.lidar_sensor.update_period = self.sim.dt * self.decimation
-        self.commands.base_velocity.ranges.lin_vel_x = (-0.0, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        # self.events.reset_base.params = {
-        #     "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-        #     "velocity_range": {
-        #         "x": (0.0, 0.0),
-        #         "y": (0.0, 0.0),
-        #         "z": (0.0, 0.0),
-        #         "roll": (-0.2, 0.2),
-        #         "pitch": (-0.2, 0.2),
-        #         "yaw": (-0.2, 0.2),
-        #     },
-        # }
-        self.events.reset_base = EventTerm(
-            func=mdp.reset_root_state_from_terrain,
-            mode="reset",
-            params={
-                "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-3.14, 3.14)},
-                "velocity_range": {
-                    "x": (-0.5, 0.5),
-                    "y": (0.0, 0.0),
-                    "z": (0.0, 0.0),
-                    "roll": (-0.5, 0.5),
-                    "pitch": (-0.5, 0.5),
-                    "yaw": (-0.5, 0.5),
-                },
-            },
-        )
-        # import pdb; pdb.set_trace()
-        # self.commands.base_velocity.ranges.ang_vel_z = (-0.5, 0.5)
-        self.rewards.feet_air_time.weight = 0.02
-        self.rewards.action_rate_l2.weight = -0.02
-        self.rewards.track_ang_vel_z_exp.weight = 1.5
-
-
-@configclass
-class Go2VisionRoughEnvCfg_PLAY(Go2VisionRoughEnvCfg):
-    def __post_init__(self):
-        # post init of parent
-        super().__post_init__()
-
-        # make a smaller scene for play
-        self.scene.num_envs = 50
-        self.scene.env_spacing = 2.5
-        # spawn the robot randomly in the grid (instead of their terrain levels)
-        self.scene.terrain.max_init_terrain_level = None
-        # reduce the number of terrains to save memory
-        if self.scene.terrain.terrain_generator is not None:
-            self.scene.terrain.terrain_generator.num_rows = 5
-            self.scene.terrain.terrain_generator.num_cols = 5
-            self.scene.terrain.terrain_generator.curriculum = False
-
-        # disable randomization for play
-        self.observations.policy.enable_corruption = True
-        # remove random pushing event
-        self.events.base_external_force_torque = None
-        self.events.push_robot = None
-        self.events.actuator_gains = None
-
-        # Commands
-        self.commands.base_velocity.ranges.lin_vel_x = (0.5, 0.5)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (0.0, 0.0)
-        # self.commands.base_velocity.heading_commands = False
-
-        # self.events.base_external_force_torque=None
-        # self.events.reset_robot_joints=None
-        # self.events.reset_base=None
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_matterport_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_matterport_cfg.py
deleted file mode 100644
index c8d70db..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/go2/go2_matterport_cfg.py
+++ /dev/null
@@ -1,407 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import CurriculumTermCfg as CurrTerm
-# from omni.isaac.lab.managers import RandomizationTermCfg as RandTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.utils import configclass
-# from omni.isaac.lab.utils.assets import LOCAL_ISAAC_DIR
-from omni.isaac.lab.utils.assets import ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.utils.noise import AdditiveUniformNoiseCfg as Unoise
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg
-import omni.isaac.lab.terrains as terrain_gen
-import omni.isaac.lab.terrains.height_field as hf_gen
-from omni.isaac.matterport.config import MatterportImporterCfg
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import AssetBaseCfg
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import CameraCfg, ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.viplanner.utils import UnRealImporterCfg, MatterportRayCasterCameraCfg
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg, patterns
-# from .h1_low_cfg import EventCfg
-# from omni.isaac.lab.terrains.config.rough import ROUGH_TERRAINS_CFG
-
-
-from omni.isaac.lab_assets import H1_MINIMAL_CFG  # isort: skip
-
-import math
-
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-
-from .go2_low_base_cfg import UNITREE_GO2_CFG
-
-
-##
-# MDP settings
-##
-
-@configclass
-class RewardsCfg:
-    # -- task
-    termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    track_lin_vel_xy_exp = RewTerm(
-        func=mdp.track_lin_vel_xy_exp, weight=2.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    )
-    track_ang_vel_z_exp = RewTerm(
-        func=mdp.track_ang_vel_z_exp, weight=1.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    )
-    # -- penalties
-    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-2.0)
-    ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
-    dof_torques_l2 = RewTerm(func=mdp.joint_torques_l2, weight=-5.0e-6)
-    dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-5e-8)
-    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
-    # feet_air_time = RewTerm(
-    #     func=mdp.feet_air_time_positive_biped,
-    #     weight=2.0,
-    #     params={
-    #         "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-    #         "command_name": "base_velocity",
-    #         "threshold": 0.3,
-    #     },
-    # )
-    # feet_slide = RewTerm(
-    #     func=mdp.feet_slide,
-    #     weight=-0.1,
-    #     params={
-    #         "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-    #         "asset_cfg": SceneEntityCfg("robot", body_names=".*ankle_link"),
-    #     },
-    # )
-    # Penalize ankle joint limits
-    # dof_pos_limits = RewTerm(
-    #     func=mdp.joint_pos_limits, weight=-1.0, params={"asset_cfg": SceneEntityCfg("robot", joint_names=".*_ankle")}
-    # )
-    # joint_deviation_hip = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.2,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*hip.*"])},
-    # )
-    # joint_deviation_toes = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.1,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*ankle.*"])},
-    # )
-    # joint_deviation_arms = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.2,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_shoulder_.*", ".*_elbow"])},
-    # )
-    # joint_deviation_torso = RewTerm(
-    #     func=mdp.joint_deviation_l1, weight=-0.1, params={"asset_cfg": SceneEntityCfg("robot", joint_names="torso")}
-    # )
-
-
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    joint_pos = mdp.JointPositionActionCfg(asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True)
-
-
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        # base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        # projected_gravity = ObsTerm(
-        #     func=mdp.projected_gravity,
-        #     noise=Unoise(n_min=-0.05, n_max=0.05),
-        # )
-        base_rpy = ObsTerm(func=mdp.base_rpy, noise=Unoise(n_min=-0.1, n_max=0.1))
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-
-        height_scan = ObsTerm(
-            func=mdp.height_map_lidar,
-            params={"sensor_cfg": SceneEntityCfg("lidar_sensor"), "offset": 0.0},
-            clip=(-10.0, 10.0),
-            noise=Unoise(n_min=-0.02, n_max=0.02),
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = False
-            self.concatenate_terms = True
-
-    
-    @configclass
-    class ProprioCfg(ObsGroup):
-        """Observations for proprioceptive group."""
-
-        # observation terms
-        # base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        # projected_gravity = ObsTerm(
-        #     func=mdp.projected_gravity,
-        #     noise=Unoise(n_min=-0.05, n_max=0.05),
-        # )
-        base_rpy = ObsTerm(func=mdp.base_rpy, noise=Unoise(n_min=-0.1, n_max=0.1))
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-
-
-        def __post_init__(self):
-            self.enable_corruption = False
-            self.concatenate_terms = True
-    
-    @configclass
-    class CriticObsCfg(ObsGroup):
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        # projected_gravity = ObsTerm(
-        #     func=mdp.projected_gravity,
-        #     noise=Unoise(n_min=-0.05, n_max=0.05),
-        # )
-        base_rpy = ObsTerm(func=mdp.base_rpy, noise=Unoise(n_min=-0.1, n_max=0.1))
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-        height_scan = ObsTerm(
-            func=mdp.height_scan,
-            params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-            clip=(-1.0, 1.0),
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = False
-            self.concatenate_terms = True
-
-
-    @configclass
-    class CameraObsCfg(ObsGroup):
-        """Observations for camera group."""
-
-        # observation terms (order preserved)
-        # depth_measurement = ObsTerm(
-        #     func=mdp.process_depth_image,
-        #     params={"sensor_cfg": SceneEntityCfg("lidar_sensor"), "data_type": "distance_to_image_plane"},
-        # )
-        rgb_measurement = ObsTerm(
-            func=mdp.isaac_camera_data,
-            params={"sensor_cfg": SceneEntityCfg("rgb_camera"), "data_type": "rgb"},
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = False
-            self.concatenate_terms = True
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-    proprio: ProprioCfg = ProprioCfg()
-    critic: CriticObsCfg = CriticObsCfg()
-    camera_obs: CameraObsCfg = CameraObsCfg()
-
-@configclass
-class CurriculumCfg:
-    """Curriculum terms for the MDP."""
-
-    terrain_levels = CurrTerm(func=mdp.terrain_levels_vel)
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="base"), "threshold": 1.0},
-    )
-    # root_height = DoneTerm(
-    #     func=mdp.root_height_below_minimum,
-    #     params={"asset_cfg": SceneEntityCfg("robot", body_names="pelvis"), "minimum_height": 0.5},
-    # )
-    bad_orientation = DoneTerm(
-        func=mdp.bad_orientation,
-        params={"limit_angle": 0.8},
-    )
-
-
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    base_velocity = mdp.UniformVelocityCommandCfg(
-        asset_name="robot",
-        resampling_time_range=(10.0, 10.0),
-        rel_standing_envs=0.02,
-        rel_heading_envs=1.0,
-        heading_command=True,
-        heading_control_stiffness=0.5,
-        debug_vis=False,
-        ranges=mdp.UniformVelocityCommandCfg.Ranges(
-            lin_vel_x=(0.0, 0.0), lin_vel_y=(0.0, 0.0), ang_vel_z=(0.0, 0.0), heading=(-math.pi, math.pi)
-        ),
-    )
-
-
-
-@configclass
-class TerrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = MatterportImporterCfg(
-        prim_path="/World/matterport",
-        terrain_type="matterport",
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        # obj_filepath="/home/zhaojing/h1-training-isaaclab/assets/matterport/29hnd4uzFmX/29hnd4uzFmX/matterport_mesh/04eb2788768d40a38d35d876a02e9624/04eb2788768d40a38d35d876a02e9624.usd",
-        obj_filepath="/home/zhaojing/h1-training-isaaclab/assets/matterport/5q7pvUzZiYa/matterport_mesh/d7a2911178dd48e89d6a23afb09cbc11/d7a2911178dd48e89d6a23afb09cbc11.usd",
-        groundplane=False,
-    )
-
-    # robots
-    robot = UNITREE_GO2_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-
-    robot.init_state.pos = (8.5, 3.0, 0.35)
-    # robot.init_state.rot = (1.0, 0.0, 0.0, 0.)
-
-    # sensors
-    height_scanner = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 20.0)),
-        attach_yaw_only=True,
-        pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[3.0, 2.0]),
-        debug_vis=False,
-        mesh_prim_paths=["/World/matterport"],
-    )
-    lidar_sensor = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/Head_lower",
-        # offset=RayCasterCfg.OffsetCfg(pos=(0.28945, 0.0, -0.046), rot=(0., -0.991,0.0,-0.131)),
-        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, -0.0), rot=(0., -0.991,0.0,-0.131)),
-        attach_yaw_only=False,
-        pattern_cfg=patterns.LidarPatternCfg(
-            channels=32, vertical_fov_range=(0.0, 90.0), horizontal_fov_range=(-180, 180.0), horizontal_res=4.0
-        ),
-        debug_vis=False,
-        mesh_prim_paths=["/World/matterport"],
-    )
-    # # camera
-    rgb_camera = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/Head_upper/rgb_camera",
-        offset=CameraCfg.OffsetCfg(pos=(0.0, 0.0, 0.5), rot=(0.7, 0.0, 0.7, 0.0)),
-        spawn=sim_utils.PinholeCameraCfg(horizontal_aperture=87.0),
-        width=480,
-        height=640,
-        data_types=["rgb"],
-    )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    disk_1 = AssetBaseCfg(
-        prim_path="/World/disk_1",
-        spawn=sim_utils.DiskLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=10000.0,
-            radius=30.0,
-        ),
-    )
-    disk_1.init_state.pos = (0, 0, 2.0)
-
-##
-# Environment configuration
-##
-
-@configclass
-class Go2MatterportCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # managers
-    terminations: TerminationsCfg = TerminationsCfg()
-    # events: EventCfg = EventCfg()
-    rewards: RewardsCfg = RewardsCfg()
-    scene: TerrainSceneCfg = TerrainSceneCfg(num_envs=4096, env_spacing=2.5)
-    curriculum: CurriculumCfg = CurriculumCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 4  # 20->10 Hz, 4->50 Hz
-        self.episode_length_s = 200000.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.actions.joint_pos.scale = 0.25
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        self.scene.height_scanner.update_period = 4 * self.sim.dt  # should we low-level decimation
-        # self.scene.lidar_sensor.update_period = 4*self.sim.dt
-        self.scene.contact_forces.update_period = self.sim.dt
-        # self.scene.terrain.terrain_type = "plane"
-        # self.scene.terrain.terrain_generator = None
-        # self.rewards.flat_orientation_l2.weight = -5.0
-        # self.rewards.dof_torques_l2.weight = -2.5e-5
-        # self.rewards.feet_air_time.weight = 0.5
-        # self.scene.height_scanner = None
-        # self.observations.policy.height_scan = None
-        self.curriculum.terrain_levels = None
-        self.events.reset_base = EventTerm(
-        func=mdp.reset_root_state_uniform,
-        mode="reset",
-        params={
-            "pose_range": {"x": (0.0, 0.0), "y": (0.0, 0.0), "yaw": (0.0, 0.0)},
-            "velocity_range": {
-                "x": (0.0, 0.0),
-                "y": (0.0, 0.0),
-                "z": (0.0, 0.0),
-                "roll": (0.0, 0.0),
-                "pitch": (0.0, 0.0),
-                "yaw": (0.0, 0.0),
-            },
-        },
-    )
-
-        self.commands.base_velocity.ranges.lin_vel_x = (0.5, 0.5)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (0.0, 0.0)
-
-        self.viewer.eye = (5, 12, 5)
-        self.viewer.lookat = (5, 0, 0.0)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/__init__.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/__init__.py
deleted file mode 100644
index f1d5025..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/__init__.py
+++ /dev/null
@@ -1,110 +0,0 @@
-import gymnasium as gym
-
-from .h1_low_base_cfg import H1BaseRoughEnvCfg, H1BaseRoughEnvCfg_PLAY, H1RoughPPORunnerCfg
-from .h1_low_cfg import H1RoughEnvCfg, H1RoughEnvCfg_PLAY
-from .h1_low_vision_cfg import H1VisionRoughEnvCfg, H1VisionRoughEnvCfg_PLAY, H1VisionRoughPPORunnerCfg
-from .h1_low_2dof_cfg import H12DoFRoughEnvCfg, H12DoFRoughEnvCfg_PLAY, H12DoFRoughPPORunnerCfg
-from .test_h1_matterport_cfg import TestH1MatterportCfg
-
-##
-# Register Gym environments.
-##
-
-
-gym.register(
-    id="h1_base",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": H1BaseRoughEnvCfg,
-        "rsl_rl_cfg_entry_point": H1RoughPPORunnerCfg,
-    },
-)
-
-
-gym.register(
-    id="h1_base_play",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": H1BaseRoughEnvCfg_PLAY,
-        "rsl_rl_cfg_entry_point": H1RoughPPORunnerCfg,
-    },
-)
-
-
-gym.register(
-    id="h1",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": H1RoughEnvCfg,
-        "rsl_rl_cfg_entry_point": H1RoughPPORunnerCfg,
-    },
-)
-
-
-gym.register(
-    id="h1_play",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": H1RoughEnvCfg_PLAY,
-        "rsl_rl_cfg_entry_point": H1RoughPPORunnerCfg,
-    },
-)
-
-
-gym.register(
-    id="h1_vision",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": H1VisionRoughEnvCfg,
-        "rsl_rl_cfg_entry_point": H1VisionRoughPPORunnerCfg,
-    },
-)
-
-
-gym.register(
-    id="h1_vision_play",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": H1VisionRoughEnvCfg_PLAY,
-        "rsl_rl_cfg_entry_point": H1VisionRoughPPORunnerCfg,
-    },
-)
-
-
-gym.register(
-    id="h1_2dof_base",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": H12DoFRoughEnvCfg,
-        "rsl_rl_cfg_entry_point": H12DoFRoughPPORunnerCfg
-    },
-)
-
-
-gym.register(
-    id="h1_2dof_base_play",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": H12DoFRoughEnvCfg_PLAY,
-        "rsl_rl_cfg_entry_point": H12DoFRoughPPORunnerCfg
-    },
-)
-
-
-gym.register(
-    id="h1_matterport",
-    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
-    disable_env_checker=True,
-    kwargs={
-        "env_cfg_entry_point": TestH1MatterportCfg,
-        "rsl_rl_cfg_entry_point": H1RoughPPORunnerCfg
-    },
-)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_2dof_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_2dof_cfg.py
deleted file mode 100644
index 9e712a6..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_2dof_cfg.py
+++ /dev/null
@@ -1,720 +0,0 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import math
-from dataclasses import MISSING
-
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import ArticulationCfg, AssetBaseCfg
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import CurriculumTermCfg as CurrTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import ContactSensorCfg, RayCasterCfg
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg, FlatPatchSamplingCfg
-import omni.isaac.lab.terrains as terrain_gen
-from omni.isaac.lab.utils import configclass
-from omni.isaac.lab.utils.assets import ISAAC_NUCLEUS_DIR, ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.utils.noise import AdditiveUniformNoiseCfg as Unoise
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg, patterns
-from omni.isaac.lab.assets.articulation import ArticulationCfg
-from omni.isaac.lab.actuators import ImplicitActuatorCfg
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import ActionsCfg, CurriculumCfg, RewardsCfg, EventCfg, LocomotionVelocityRoughEnvCfg
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.config.h1.rough_env_cfg import H1Rewards
-# from omni.isaac.lab_tasks.manager_based.locomotion.velocity.config.h1.rough_env_cfg import H1RoughEnvCfg as OriH1RoughEnvCfg
-
-##
-# Pre-defined configs
-##
-from omni.isaac.lab.terrains.config.rough import ROUGH_TERRAINS_CFG  # isort: skip
-from omni.isaac.lab_assets import H1_MINIMAL_CFG, H1_CFG  # isort: skip
-
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-
-
-LOCAL_H1_2DoF_USD_FILE = "/home/zhaojing/IsaacLab/h1_2dof_model/h1_2dof.usd"
-
-"""2-DoF H1 robot configuration with arms fixed
-
-Joints: ['left_hip_yaw_joint', 'right_hip_yaw_joint', 'left_hip_roll_joint', 
-'right_hip_roll_joint', 'left_hip_pitch_joint', 'right_hip_pitch_joint', 
-'left_knee_joint', 'right_knee_joint', 'left_ankle_pitch_joint', 
-'right_ankle_pitch_joint', 'left_ankle_roll_joint', 'right_ankle_roll_joint']
-"""
-H1_2DoF_CFG = ArticulationCfg(
-    prim_path="{ENV_REGEX_NS}/Robot",
-    spawn=sim_utils.UsdFileCfg(
-        usd_path=LOCAL_H1_2DoF_USD_FILE,
-        activate_contact_sensors=True,
-        rigid_props=sim_utils.RigidBodyPropertiesCfg(
-            disable_gravity=False,
-            retain_accelerations=False,
-            linear_damping=0.0,
-            angular_damping=0.0,
-            max_linear_velocity=1000.0,
-            max_angular_velocity=1000.0,
-            max_depenetration_velocity=1.0,
-        ),
-        articulation_props=sim_utils.ArticulationRootPropertiesCfg(
-            enabled_self_collisions=True, solver_position_iteration_count=4, solver_velocity_iteration_count=0
-        ),
-    ),
-    init_state=ArticulationCfg.InitialStateCfg(
-        pos=(0.0, 0.0, 1.1),
-        joint_pos={
-            ".*hip_yaw.*" : 0. ,
-            ".*hip_roll.*" : 0. ,
-            ".*hip_pitch.*": -0.4,
-            ".*knee.*": 0.8,
-            ".*ankle.*": 0.0,
-            # torso.*: 0.0,
-        },
-        joint_vel={".*": 0.0},
-    ),
-    soft_joint_pos_limit_factor=0.9,
-    actuators={
-        "legs": ImplicitActuatorCfg(
-            joint_names_expr=[".*"],
-            effort_limit={
-                ".*hip.*": 200,
-                ".*knee.*": 200,
-                ".*ankle.*": 19,
-                #  .*torso.*: 200,
-            },
-            # saturation_effort=200.0,
-            velocity_limit={
-                ".*hip.*": 23,
-                ".*knee.*": 14,
-                ".*ankle.*": 9,
-                #  .*torso.*: 9,
-            },
-            # friction=0.0,
-            stiffness = {".*hip.*": 200,
-                     ".*knee.*": 200,
-                     ".*ankle.*": 40,
-                    #  .*torso.*: 300,
-                     },  # [N*m/rad]
-            damping = {".*hip.*": 5,
-                     ".*knee.*": 5,
-                     ".*ankle.*": 2,
-                    #  .*torso.*: 6,
-                     }  # [N*m/rad]  # [N*m*s/rad]
-        ),
-        
-    },
-)
-
-
-@configclass
-class H12DoFRoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 32
-    max_iterations = 50000
-    save_interval = 500
-    experiment_name = "h1_2dof_rough"
-    empirical_normalization = False
-    policy = RslRlPpoActorCriticCfg(
-        init_noise_std=1.0,
-        # actor_hidden_dims=[512, 256, 128],
-        # critic_hidden_dims=[512, 256, 128],
-        actor_hidden_dims=[256, 128, 64],
-        critic_hidden_dims=[256, 128, 64],
-        activation="elu",
-        class_name="ActorCritic",
-    )
-    algorithm = RslRlPpoAlgorithmCfg(
-        value_loss_coef=1.0,
-        use_clipped_value_loss=True,
-        clip_param=0.2,
-        entropy_coef=0.005,
-        num_learning_epochs=5,
-        num_mini_batches=4,
-        learning_rate=1.0e-3,
-        schedule="adaptive",
-        gamma=0.99,
-        lam=0.95,
-        desired_kl=0.01,
-        max_grad_norm=1.0,
-    )
-
-
-"""Configuration for custom terrains."""
-ROUGH_TERRAINS_CFG = TerrainGeneratorCfg(
-    size=(8.0, 8.0),
-    border_width=20.0,
-    num_rows=10,
-    num_cols=20,
-    horizontal_scale=0.1,
-    vertical_scale=0.005,
-    slope_threshold=0.75,
-    use_cache=False,
-    sub_terrains={
-        "pyramid_stairs": terrain_gen.MeshPyramidStairsTerrainCfg(
-            proportion=0.2,
-            step_height_range=(0.05, 0.23),
-            step_width=0.3,
-            platform_width=3.0,
-            border_width=1.0,
-            holes=False,
-        ),
-        "pyramid_stairs_inv": terrain_gen.MeshInvertedPyramidStairsTerrainCfg(
-            proportion=0.2,
-            step_height_range=(0.05, 0.23),
-            step_width=0.3,
-            platform_width=3.0,
-            border_width=1.0,
-            holes=False,
-        ),
-        "boxes": terrain_gen.MeshRandomGridTerrainCfg(
-            proportion=0.2, grid_width=0.45, grid_height_range=(0.05, 0.2), platform_width=2.0
-        ),
-        "random_rough": terrain_gen.HfRandomUniformTerrainCfg(
-            proportion=0.2, noise_range=(0.02, 0.10), noise_step=0.02, border_width=0.25
-        ),
-        "hf_pyramid_slope": terrain_gen.HfPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25
-        ),
-        "hf_pyramid_slope_inv": terrain_gen.HfInvertedPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25 ),
-        # "init_pos": terrain_gen.HfDiscreteObstaclesTerrainCfg(
-        #     proportion=1.0, 
-        #     num_obstacles=10,
-        #     obstacle_height_mode="choice",
-        #     obstacle_height_range=(3.0, 3.0), obstacle_width_range=(0.5, 1.5), 
-        #     platform_width=2.0
-        # ),
-    },
-)
-# for sub_terrain_name, sub_terrain_cfg in ROUGH_TERRAINS_CFG.sub_terrains.items():
-#             sub_terrain_cfg.flat_patch_sampling = {
-#                 sub_terrain_name: FlatPatchSamplingCfg(num_patches=2, patch_radius=[0.01,0.1,0.5,1.0], max_height_diff=0.5)
-#             }
-"""Rough terrains configuration."""
-
-##
-# Scene definition
-##
-@configclass
-class TrainSceneCfg_2DoF(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="generator",
-        terrain_generator=ROUGH_TERRAINS_CFG,
-        max_init_terrain_level=5, # TRY 9 AS WELL
-        collision_group=-1,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        visual_material=sim_utils.MdlFileCfg(
-            mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
-            project_uvw=True,
-            texture_scale=(0.25, 0.25),
-        ),
-        debug_vis=False,
-    )
-    # robots
-    robot = H1_2DoF_CFG
-    # sensors
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    
-    height_scanner = None
-    # height_scanner = RayCasterCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/base",
-    #     offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 20.0)),
-    #     attach_yaw_only=True,
-    #     pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-    #     debug_vis=False,
-    #     mesh_prim_paths=["/World/ground"],
-    # )
-    # height_scanner.prim_path = "{ENV_REGEX_NS}/Robot/torso_link"
-    # camera
-    lidar_sensor = None
-    lidar_sensor = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-        # mesh_prim_paths=["/home/zhaojing/terrain-generator/results/generated_terrain/mesh_1/mesh.ply"],
-        mesh_prim_paths=["/World/ground"],
-        update_period=0.1,
-        attach_yaw_only=False,
-        # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.60, 0.0, 0.0), rot=(-0.5, 0.5, -0.5, 0.5)),
-        # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(0.579, -0.579, 0.406, -0.406)),
-        offset=RayCasterCfg.OffsetCfg(pos=(0.047, 0.0, 0.3), rot=(-0.119, 0.0,0.993,0.0)),
-        # data_types=["distance_to_image_plane"],
-        debug_vis=True,
-        pattern_cfg=patterns.BpearlPatternCfg(
-            vertical_ray_angles=[
-                 51.125, 48.0, 45.0, 42.0, 39.0, 36, 33, 30,27,24,20,17,14,11,8,5,2, -1
-    ]
-        ),
-        max_distance=10
-    )
-
-##
-# Observations
-##
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        projected_gravity = ObsTerm(
-            func=mdp.projected_gravity,
-            noise=Unoise(n_min=-0.05, n_max=0.05),
-        )
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-        lidar_measurement = ObsTerm(
-            func=mdp.process_lidar,
-            params={"sensor_cfg": SceneEntityCfg("lidar_sensor")},
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-
-## 
-# Actions
-##
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    joint_pos = mdp.JointPositionActionCfg(asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True)
-
-##
-# Events (for domain randomization)
-##
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    # startup
-    physics_material = EventTerm(
-        func=mdp.randomize_rigid_body_material,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-            "static_friction_range": (0.8, 0.8),
-            "dynamic_friction_range": (0.6, 0.6),
-            "restitution_range": (0.0, 0.0),
-            "num_buckets": 64,
-        },
-    )
-
-    add_base_mass = EventTerm(
-        func=mdp.randomize_rigid_body_mass,
-        mode="startup",
-        params={"asset_cfg": SceneEntityCfg("robot", body_names="pelvis"), "mass_distribution_params": (-5.0, 5.0), "operation": "add"},
-    )
-
-    # reset
-    base_external_force_torque = EventTerm(
-        func=mdp.apply_external_force_torque,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="pelvis"),
-            "force_range": (0.0, 0.0),
-            "torque_range": (-0.0, 0.0),
-        },
-    )
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_uniform,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (-0.5, 0.5),
-                "y": (-0.5, 0.5),
-                "z": (-0.5, 0.5),
-                "roll": (-0.5, 0.5),
-                "pitch": (-0.5, 0.5),
-                "yaw": (-0.5, 0.5),
-            },
-        },
-    )
-
-    # reset_base = EventTerm(
-    #     func=mdp.reset_root_state_from_terrain,
-    #     mode="reset",
-    #     params={
-    #         "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-3.14, 3.14)},
-    #         "velocity_range": {
-    #             "x": (-0.5, 0.5),
-    #             "y": (-0.5, 0.5),
-    #             "z": (-0.5, 0.5),
-    #             "roll": (-0.5, 0.5),
-    #             "pitch": (-0.5, 0.5),
-    #             "yaw": (-0.5, 0.5),
-    #         },
-    #     },
-    # )
-
-    reset_robot_joints = EventTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "position_range": (0.5, 1.5),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-    # # interval
-    # push_robot = EventTerm(
-    #     func=mdp.push_by_setting_velocity,
-    #     mode="interval",
-    #     interval_range_s=(10.0, 15.0),
-    #     params={"velocity_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5)}},
-    # )
-
-
-##
-# Rewards
-#
-@configclass
-class RewardsCfg2DoF:
-    """
-    Rewards configuration for the 2-DoF H1 (arm fixed) robot.
-    """
-    # -- task
-    termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    track_lin_vel_xy_exp = RewTerm(
-        func=mdp.track_lin_vel_xy_exp, weight=2.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    )
-    track_ang_vel_z_exp = RewTerm(
-        func=mdp.track_ang_vel_z_exp, weight=1.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    )
-    # -- penalties
-    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-2.0)
-    ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
-    dof_torques_l2 = RewTerm(func=mdp.joint_torques_l2, weight=-5.0e-6)
-    dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-5e-8)
-    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
-    feet_air_time = RewTerm(
-        func=mdp.feet_air_time_positive_biped,
-        weight=2.0,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_roll_link"),
-            "command_name": "base_velocity",
-            "threshold": 0.3,
-        },
-    )
-    feet_slide = RewTerm(
-        func=mdp.feet_slide,
-        weight=-0.1,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_roll_link"),
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*ankle_roll_link"),
-        },
-    )
-    # Penalize ankle joint limits
-    # dof_pos_limits = RewTerm(
-    #     func=mdp.joint_pos_limits, weight=-1.0, params={"asset_cfg": SceneEntityCfg("robot", joint_names=".*_ankle")}
-    # )
-    joint_deviation_hip = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.2,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*hip.*"])},
-    )
-    joint_deviation_toes = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.1,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*ankle.*"])},
-    )
-    # joint_deviation_arms = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.2,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_shoulder_.*", ".*_elbow"])},
-    # )
-    # joint_deviation_torso = RewTerm(
-    #     func=mdp.joint_deviation_l1, weight=-0.1, params={"asset_cfg": SceneEntityCfg("robot", joint_names="torso")}
-    # )
-    # termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    # lin_vel_z_l2 = None
-    # track_lin_vel_xy_exp = RewTerm(
-    #     func=mdp.track_lin_vel_xy_yaw_frame_exp,
-    #     weight=1.0,
-    #     params={"command_name": "base_velocity", "std": 0.5},
-    # )
-    # track_ang_vel_z_exp = RewTerm(
-    #     func=mdp.track_ang_vel_z_world_exp, weight=1.0, params={"command_name": "base_velocity", "std": 0.5}
-    # )
-    # feet_air_time = RewTerm(
-    #     func=mdp.feet_air_time_positive_biped,
-    #     weight=0.5,
-    #     params={
-    #         "command_name": "base_velocity",
-    #         "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-    #         "threshold": 0.4,
-    #     },
-    # )
-    # # feet_slide = RewTerm(
-    # #     func=mdp.feet_slide,
-    # #     weight=-0.25,
-    # #     params={
-    # #         "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-    # #         "asset_cfg": SceneEntityCfg("robot", body_names=".*ankle_link"),
-    # #     },
-    # # )
-    # # Penalize ankle joint limits
-    # dof_pos_limits = RewTerm(
-    #     func=mdp.joint_pos_limits, weight=-1.0, params={"asset_cfg": SceneEntityCfg("robot", joint_names=".*_ankle")}
-    # )
-    # # Penalize deviation from default of the joints that are not essential for locomotion
-    # joint_deviation_hip = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.2,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_yaw", ".*_hip_roll"])},
-    # )
-    # joint_deviation_arms = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.2,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_shoulder_.*", ".*_elbow"])},
-    # )
-    # joint_deviation_torso = RewTerm(
-    #     func=mdp.joint_deviation_l1, weight=-0.1, params={"asset_cfg": SceneEntityCfg("robot", joint_names="torso")}
-    # )
-
-
-@configclass
-class CustomH1Rewards(H1Rewards):
-    feet_stumble = RewTerm(
-        func=mdp.feet_stumble,
-        weight=-0.5,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-        },
-    )
-
-##
-# Commands
-##
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    base_velocity = mdp.UniformVelocityCommandCfg(
-        asset_name="robot",
-        resampling_time_range=(10.0, 10.0),
-        rel_standing_envs=0.02,
-        rel_heading_envs=1.0,
-        heading_command=True,
-        heading_control_stiffness=0.5,
-        debug_vis=True,
-        ranges=mdp.UniformVelocityCommandCfg.Ranges(
-            lin_vel_x=(-1.0, 1.0), lin_vel_y=(-1.0, 1.0), ang_vel_z=(-1.0, 1.0), heading=(-math.pi, math.pi)
-        ),
-    )
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*torso_link"), "threshold": 1.0},
-    )
-
-
-# @configclass
-# class H1RoughEnvCfg(LocomotionVelocityRoughEnvCfg):
-#     rewards: H1Rewards = H1Rewards()
-#     terminations: TerminationsCfg = TerminationsCfg()
-#     scene = MySceneCfg(num_envs=4096, env_spacing=2.5)
-#     observations: ObservationsCfg = ObservationsCfg()
-
-#     def __post_init__(self):
-#         # post init of parent
-#         super().__post_init__()
-#         # Scene
-#         self.scene.robot = H1_MINIMAL_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-#         # self.scene.height_scanner.prim_path = "{ENV_REGEX_NS}/Robot/torso_link"
-
-#         # Randomization
-#         self.events.push_robot = None
-#         self.events.add_base_mass = None
-#         self.events.reset_robot_joints.params["position_range"] = (1.0, 1.0)
-#         self.events.base_external_force_torque.params["asset_cfg"].body_names = [".*torso_link"]
-#         self.events.reset_base.params = {
-#             "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-#             "velocity_range": {
-#                 "x": (0.0, 0.0),
-#                 "y": (0.0, 0.0),
-#                 "z": (0.0, 0.0),
-#                 "roll": (0.0, 0.0),
-#                 "pitch": (0.0, 0.0),
-#                 "yaw": (0.0, 0.0),
-#             },
-#         }
-
-#         # Terminations
-#         self.terminations.base_contact.params["sensor_cfg"].body_names = [".*torso_link"]
-
-#         # Rewards
-#         self.rewards.undesired_contacts = None
-#         self.rewards.flat_orientation_l2.weight = -1.0
-#         self.rewards.dof_torques_l2.weight = 0.0
-#         self.rewards.action_rate_l2.weight = -0.005
-#         self.rewards.dof_acc_l2.weight = -1.25e-7
-
-#         # Commands
-#         self.commands.base_velocity.ranges.lin_vel_x = (0.0, 1.0)
-#         self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-#         self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-
-@configclass
-class H12DoFRoughEnvCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Scene settings
-    scene: InteractiveSceneCfg = TrainSceneCfg_2DoF(num_envs=4096, env_spacing=2.5)
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # MDP settings
-    rewards: RewardsCfg = RewardsCfg2DoF()
-    # rewards: RewardsCfg = H1Rewards()
-    # rewards: RewardsCfg = CustomH1Rewards()
-    terminations: TerminationsCfg = TerminationsCfg()
-    events: EventCfg = EventCfg()
-    curriculum: CurriculumCfg = CurriculumCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 4
-        self.episode_length_s = 20.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material = self.scene.terrain.physics_material
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-
-        # Randomization
-        self.events.push_robot = None
-        self.events.add_base_mass = None
-        self.events.reset_robot_joints.params["position_range"] = (1.0, 1.0)
-        self.events.base_external_force_torque.params["asset_cfg"].body_names = [".*torso_link"]
-        self.events.reset_base.params = {
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (0.0, 0.0),
-                "y": (0.0, 0.0),
-                "z": (0.0, 0.0),
-                "roll": (0.0, 0.0),
-                "pitch": (0.0, 0.0),
-                "yaw": (0.0, 0.0),
-            },
-        }
-
-        # Rewards
-        # self.rewards.undesired_contacts = None
-        # self.rewards.flat_orientation_l2.weight = -1.0
-        # self.rewards.dof_torques_l2.weight = 0.0
-        # self.rewards.action_rate_l2.weight = -0.005
-        # self.rewards.dof_acc_l2.weight = -1.25e-7
-        # self.rewards.feet_air_time.weight = 0.5
-
-        # Commands
-        self.commands.base_velocity.ranges.lin_vel_x = (0.0, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        # self.scene.lidar_sensor = None
-        if self.scene.lidar_sensor is not None:
-            self.scene.lidar_sensor.update_period = 4 * self.sim.dt
-        self.scene.contact_forces.update_period = self.sim.dt
-        # if self.scene.height_scanner is not None:
-        #     self.scene.height_scanner.update_period = self.decimation * self.sim.dt
-        if self.scene.contact_forces is not None:
-            self.scene.contact_forces.update_period = self.sim.dt
-
-        # check if terrain levels curriculum is enabled - if so, enable curriculum for terrain generator
-        # this generates terrains with increasing difficulty and is useful for training
-        if getattr(self.curriculum, "terrain_levels", None) is not None:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = True
-        else:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = False
-        # self.curriculum.terrain_levels = None
-
-
-@configclass
-class H12DoFRoughEnvCfg_PLAY(H12DoFRoughEnvCfg):
-    def __post_init__(self):
-        # post init of parent
-        super().__post_init__()
-
-        # make a smaller scene for play
-        self.scene.num_envs = 40
-        self.scene.env_spacing = 2.5
-        self.episode_length_s = 40.0
-        # spawn the robot randomly in the grid (instead of their terrain levels)
-        self.scene.terrain.max_init_terrain_level = None
-        # reduce the number of terrains to save memory
-        if self.scene.terrain.terrain_generator is not None:
-            self.scene.terrain.terrain_generator.num_rows = 5
-            self.scene.terrain.terrain_generator.num_cols = 5
-            self.scene.terrain.terrain_generator.curriculum = False
-
-        self.commands.base_velocity.ranges.lin_vel_x = (0.5, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-        self.commands.base_velocity.ranges.heading = (0.0, 0.0)
-        self.commands.base_velocity.heading_command = True
-        # disable randomization for play
-        self.observations.policy.enable_corruption = False
-        # remove random pushing
-        self.events.base_external_force_torque = None
-        self.events.push_robot = None
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_base_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_base_cfg.py
deleted file mode 100644
index 9b8f7bc..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_base_cfg.py
+++ /dev/null
@@ -1,440 +0,0 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import math
-from dataclasses import MISSING
-
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import ArticulationCfg, AssetBaseCfg
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import CurriculumTermCfg as CurrTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import ContactSensorCfg
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg, FlatPatchSamplingCfg
-import omni.isaac.lab.terrains as terrain_gen
-from omni.isaac.lab.utils import configclass
-from omni.isaac.lab.utils.assets import ISAAC_NUCLEUS_DIR, ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.utils.noise import AdditiveUniformNoiseCfg as Unoise
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg, patterns
-from omni.isaac.lab.assets.articulation import ArticulationCfg
-from omni.isaac.lab.actuators import ImplicitActuatorCfg
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import ActionsCfg, CurriculumCfg, RewardsCfg, EventCfg, LocomotionVelocityRoughEnvCfg
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.config.h1.rough_env_cfg import H1Rewards
-# from omni.isaac.lab_tasks.manager_based.locomotion.velocity.config.h1.rough_env_cfg import H1RoughEnvCfg as OriH1RoughEnvCfg
-
-##
-# Pre-defined configs
-##
-from omni.isaac.lab.terrains.config.rough import ROUGH_TERRAINS_CFG  # isort: skip
-from omni.isaac.lab_assets import H1_MINIMAL_CFG  # isort: skip
-
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-
-
-@configclass
-class H1RoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 32
-    max_iterations = 50000
-    save_interval = 500
-    experiment_name = "h1_base_rough"
-    empirical_normalization = False
-    policy = RslRlPpoActorCriticCfg(
-        init_noise_std=1.0,
-        actor_hidden_dims=[512, 256, 128],
-        critic_hidden_dims=[512, 256, 128],
-        activation="elu",
-        class_name="ActorCritic",
-    )
-    algorithm = RslRlPpoAlgorithmCfg(
-        value_loss_coef=1.0,
-        use_clipped_value_loss=True,
-        clip_param=0.2,
-        entropy_coef=0.005,
-        num_learning_epochs=5,
-        num_mini_batches=4,
-        learning_rate=1.0e-3,
-        schedule="adaptive",
-        gamma=0.99,
-        lam=0.95,
-        desired_kl=0.01,
-        max_grad_norm=1.0,
-    )
-
-
-"""Configuration for custom terrains."""
-BASE_TERRAIN_CFG = TerrainGeneratorCfg(
-    size=(8.0, 8.0),
-    border_width=20.0,
-    num_rows=10,
-    num_cols=20,
-    horizontal_scale=0.1,
-    vertical_scale=0.005,
-    slope_threshold=0.75,
-    use_cache=False,
-    sub_terrains={
-        # "boxes": terrain_gen.MeshRandomGridTerrainCfg(
-        #     proportion=0.2, grid_width=0.45, grid_height_range=(0.05, 0.2), platform_width=2.0
-        # ),
-        "flat": terrain_gen.MeshPlaneTerrainCfg(
-            proportion=0.5
-        ),
-        "random_rough": terrain_gen.HfRandomUniformTerrainCfg(
-            proportion=0.5, noise_range=(0.02, 0.10), noise_step=0.02, border_width=0.25
-        ),
-    },
-)
-
-##
-# Scene definition
-##
-@configclass
-class BaseSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="generator",
-        terrain_generator=BASE_TERRAIN_CFG,
-        max_init_terrain_level=5, # TRY 9 AS WELL
-        collision_group=-1,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        visual_material=sim_utils.MdlFileCfg(
-            mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
-            project_uvw=True,
-            texture_scale=(0.25, 0.25),
-        ),
-        debug_vis=False,
-    )
-    # robots
-    robot = H1_MINIMAL_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = H1_2DoF_CFG
-    # sensors
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    
-    height_scanner = None
-    # camera
-    lidar_sensor = None
-
-
-##
-# Observations
-##
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        # base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        projected_gravity = ObsTerm(
-            func=mdp.projected_gravity,
-            noise=Unoise(n_min=-0.05, n_max=0.05),
-        )
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-
-## 
-# Actions
-##
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    joint_pos = mdp.JointPositionActionCfg(asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True)
-
-##
-# Events (for domain randomization)
-##
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    # startup
-    physics_material = EventTerm(
-        func=mdp.randomize_rigid_body_material,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-            "static_friction_range": (0.8, 0.8),
-            "dynamic_friction_range": (0.6, 0.6),
-            "restitution_range": (0.0, 0.0),
-            "num_buckets": 64,
-        },
-    )
-
-    add_base_mass = EventTerm(
-        func=mdp.randomize_rigid_body_mass,
-        mode="startup",
-        params={"asset_cfg": SceneEntityCfg("robot", body_names="pelvis"), "mass_distribution_params": (-5.0, 5.0), "operation": "add"},
-    )
-
-    # reset
-    base_external_force_torque = EventTerm(
-        func=mdp.apply_external_force_torque,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="pelvis"),
-            "force_range": (0.0, 0.0),
-            "torque_range": (-0.0, 0.0),
-        },
-    )
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_uniform,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (-0.5, 0.5),
-                "y": (-0.5, 0.5),
-                "z": (-0.5, 0.5),
-                "roll": (-0.5, 0.5),
-                "pitch": (-0.5, 0.5),
-                "yaw": (-0.5, 0.5),
-            },
-        },
-    )
-
-    reset_robot_joints = EventTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "position_range": (0.5, 1.5),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-
-@configclass
-class RewardsCfg:
-    # -- task
-    termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    track_lin_vel_xy_exp = RewTerm(
-            func=mdp.track_lin_vel_xy_exp, weight=1.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-        )
-    track_ang_vel_z_exp = RewTerm(
-        func=mdp.track_ang_vel_z_exp, weight=1.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    )
-
-    # -- penalties
-    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-2.0)
-    ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
-    dof_torques_l2 = RewTerm(func=mdp.joint_torques_l2, weight=-5.0e-6)
-    dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-5e-8)
-    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
-    feet_air_time = RewTerm(
-        func=mdp.feet_air_time_positive_biped,
-        weight=2.0,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-            "command_name": "base_velocity",
-            "threshold": 0.3,
-        },
-    )
-    joint_deviation_hip = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.4,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*hip.*"])},
-    )
-    # joint_deviation_toes = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.01,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*ankle.*"])},
-    # )
-    joint_deviation_arms = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.5,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_shoulder_.*", ".*_elbow"])},
-    )
-    joint_deviation_torso = RewTerm(
-        func=mdp.joint_deviation_l1, weight=-0.1, params={"asset_cfg": SceneEntityCfg("robot", joint_names="torso")}
-    )
-    # -- optional penalties
-    flat_orientation_l2 = RewTerm(func=mdp.flat_orientation_l2, weight=-1.0)
-
-##
-# Commands
-##
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    base_velocity = mdp.UniformVelocityCommandCfg(
-        asset_name="robot",
-        resampling_time_range=(10.0, 10.0),
-        rel_standing_envs=0.02,
-        rel_heading_envs=1.0,
-        heading_command=True,
-        heading_control_stiffness=0.5,
-        debug_vis=True,
-        ranges=mdp.UniformVelocityCommandCfg.Ranges(
-            lin_vel_x=(-1.0, 1.0), lin_vel_y=(-1.0, 1.0), ang_vel_z=(-1.0, 1.0), heading=(-math.pi, math.pi)
-        ),
-    )
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*torso_link"), "threshold": 1.0},
-    )
-
-
-@configclass
-class H1BaseRoughEnvCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Scene settings
-    scene: InteractiveSceneCfg = BaseSceneCfg(num_envs=4096, env_spacing=2.5)
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # MDP settings
-    # rewards: RewardsCfg = RewardsCfg2DoF()
-    # rewards: RewardsCfg = H1Rewards()
-    rewards: RewardsCfg = H1Rewards()
-    terminations: TerminationsCfg = TerminationsCfg()
-    events: EventCfg = EventCfg()
-    curriculum: CurriculumCfg = CurriculumCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 4
-        self.episode_length_s = 20.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material = self.scene.terrain.physics_material
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-
-        # Randomization
-        self.events.push_robot = None
-        self.events.add_base_mass = None
-        self.events.reset_robot_joints.params["position_range"] = (1.0, 1.0)
-        self.events.base_external_force_torque.params["asset_cfg"].body_names = [".*torso_link"]
-        self.events.reset_base.params = {
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (0.0, 0.0),
-                "y": (0.0, 0.0),
-                "z": (0.0, 0.0),
-                "roll": (0.0, 0.0),
-                "pitch": (0.0, 0.0),
-                "yaw": (0.0, 0.0),
-            },
-        }
-
-        # Rewards
-        self.rewards.undesired_contacts = None
-        self.rewards.flat_orientation_l2.weight = -1.0
-        self.rewards.dof_torques_l2.weight = 0.0
-        self.rewards.action_rate_l2.weight = -0.005
-        self.rewards.dof_acc_l2.weight = -1.25e-7
-        self.rewards.feet_air_time.weight = 1.0
-
-        # Commands
-        self.commands.base_velocity.ranges.lin_vel_x = (0.0, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        # self.scene.lidar_sensor = None
-        if self.scene.lidar_sensor is not None:
-            self.scene.lidar_sensor.update_period = 4 * self.sim.dt
-        self.scene.contact_forces.update_period = self.sim.dt
-        # if self.scene.height_scanner is not None:
-        #     self.scene.height_scanner.update_period = self.decimation * self.sim.dt
-        if self.scene.contact_forces is not None:
-            self.scene.contact_forces.update_period = self.sim.dt
-
-        # check if terrain levels curriculum is enabled - if so, enable curriculum for terrain generator
-        # this generates terrains with increasing difficulty and is useful for training
-        if getattr(self.curriculum, "terrain_levels", None) is not None:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = True
-        else:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = False
-        # self.curriculum.terrain_levels = None
-
-
-@configclass
-class H1BaseRoughEnvCfg_PLAY(H1BaseRoughEnvCfg):
-    def __post_init__(self):
-        # post init of parent
-        super().__post_init__()
-
-        # make a smaller scene for play
-        self.scene.num_envs = 40
-        self.scene.env_spacing = 2.5
-        self.episode_length_s = 40.0
-        # spawn the robot randomly in the grid (instead of their terrain levels)
-        self.scene.terrain.max_init_terrain_level = None
-        # reduce the number of terrains to save memory
-        if self.scene.terrain.terrain_generator is not None:
-            self.scene.terrain.terrain_generator.num_rows = 5
-            self.scene.terrain.terrain_generator.num_cols = 5
-            self.scene.terrain.terrain_generator.curriculum = False
-
-        self.commands.base_velocity.ranges.lin_vel_x = (0.1, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-        self.commands.base_velocity.ranges.heading = (0.0, 0.0)
-        self.commands.base_velocity.heading_command = True
-        # disable randomization for play
-        self.observations.policy.enable_corruption = False
-        # remove random pushing
-        self.events.base_external_force_torque = None
-        self.events.push_robot = None
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_cfg.py
deleted file mode 100644
index 53f5228..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_cfg.py
+++ /dev/null
@@ -1,816 +0,0 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import math
-from dataclasses import MISSING
-
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import ArticulationCfg, AssetBaseCfg
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import CurriculumTermCfg as CurrTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import ContactSensorCfg, RayCasterCfg
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg, FlatPatchSamplingCfg
-import omni.isaac.lab.terrains as terrain_gen
-from omni.isaac.lab.utils import configclass
-from omni.isaac.lab.utils.assets import ISAAC_NUCLEUS_DIR, ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.utils.noise import AdditiveUniformNoiseCfg as Unoise
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg, patterns
-from omni.isaac.lab.assets.articulation import ArticulationCfg
-from omni.isaac.lab.actuators import ImplicitActuatorCfg
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import ActionsCfg, CurriculumCfg, RewardsCfg, EventCfg, LocomotionVelocityRoughEnvCfg
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.config.h1.rough_env_cfg import H1Rewards
-# from omni.isaac.lab_tasks.manager_based.locomotion.velocity.config.h1.rough_env_cfg import H1RoughEnvCfg as OriH1RoughEnvCfg
-
-##
-# Pre-defined configs
-##
-from omni.isaac.lab.terrains.config.rough import ROUGH_TERRAINS_CFG  # isort: skip
-from omni.isaac.lab_assets import H1_MINIMAL_CFG  # isort: skip
-
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-
-
-LOCAL_H1_USD_FILE = "/home/zhaojing/h1-training-isaaclab/h1_description/h1_2dof_model/h1_2dof_minimal.usd"
-
-"""2-DoF H1 robot configuration with arms fixed
-
-Joints: ['left_hip_yaw_joint', 'right_hip_yaw_joint', 'left_hip_roll_joint', 
-'right_hip_roll_joint', 'left_hip_pitch_joint', 'right_hip_pitch_joint', 
-'left_knee_joint', 'right_knee_joint', 'left_ankle_pitch_joint', 
-'right_ankle_pitch_joint', 'left_ankle_roll_joint', 'right_ankle_roll_joint']
-"""
-H1_2DoF_CFG = ArticulationCfg(
-    spawn=sim_utils.UsdFileCfg(
-        usd_path=f"{ISAACLAB_NUCLEUS_DIR}/Robots/Unitree/H1/h1.usd",
-        activate_contact_sensors=True,
-        rigid_props=sim_utils.RigidBodyPropertiesCfg(
-            disable_gravity=False,
-            retain_accelerations=False,
-            linear_damping=0.0,
-            angular_damping=0.0,
-            max_linear_velocity=1000.0,
-            max_angular_velocity=1000.0,
-            max_depenetration_velocity=1.0,
-        ),
-        articulation_props=sim_utils.ArticulationRootPropertiesCfg(
-            enabled_self_collisions=False, solver_position_iteration_count=4, solver_velocity_iteration_count=4
-        ),
-    ),
-    init_state=ArticulationCfg.InitialStateCfg(
-        pos=(0.0, 0.0, 1.05),
-        joint_pos={
-            ".*_hip_yaw": 0.0,
-            ".*_hip_roll": 0.0,
-            ".*_hip_pitch": -0.28,  # -16 degrees
-            ".*_knee": 0.79,  # 45 degrees
-            ".*_ankle": -0.52,  # -30 degrees
-            "torso": 0.0,
-            ".*_shoulder_pitch": 0.28,
-            ".*_shoulder_roll": 0.0,
-            ".*_shoulder_yaw": 0.0,
-            ".*_elbow": 0.52,
-        },
-        joint_vel={".*": 0.0},
-    ),
-    soft_joint_pos_limit_factor=0.9,
-    actuators={
-        "legs": ImplicitActuatorCfg(
-            joint_names_expr=[".*_hip_yaw.*", ".*_hip_roll.*", ".*_hip_pitch.*", ".*_knee", "torso_joint"],
-            effort_limit=300,
-            velocity_limit=100.0,
-            stiffness={
-                ".*_hip_yaw.*": 150.0,
-                ".*_hip_roll.*": 150.0,
-                ".*_hip_pitch.*": 200.0,
-                ".*_knee": 200.0,
-                "torso": 200.0,
-            },
-            damping={
-                ".*_hip_yaw.*": 5.0,
-                ".*_hip_roll.*": 5.0,
-                ".*_hip_pitch.*": 5.0,
-                ".*_knee.*": 5.0,
-                "torso_joint": 5.0,
-            },
-        ),
-        "feet": ImplicitActuatorCfg(
-            joint_names_expr=[".*_ankle_roll_*", ".*_ankle_pitch_*"],
-            effort_limit=100,
-            velocity_limit=100.0,
-            stiffness={
-                ".*_ankle_roll_*": 20.0,
-                ".*_ankle_pitch_*": 20.0,
-            },
-            damping={".*_ankle": 4.0},
-        ),
-        "arms": ImplicitActuatorCfg(
-            joint_names_expr=[".*_shoulder_pitch_*", ".*_shoulder_roll_*", ".*_shoulder_yaw_*", ".*_elbow_*"],
-            effort_limit=300,
-            velocity_limit=100.0,
-            stiffness={
-                ".*_shoulder_pitch_*": 40.0,
-                ".*_shoulder_roll_*": 40.0,
-                ".*_shoulder_yaw_*": 40.0,
-                ".*_elbow_*": 40.0,
-            },
-            damping={
-                ".*_shoulder_pitch_*": 10.0,
-                ".*_shoulder_roll_*": 10.0,
-                ".*_shoulder_yaw_*": 10.0,
-                ".*_elbow_*": 10.0,
-            },
-        ),
-    },
-)
-# H1_2DoF_CFG = ArticulationCfg(
-#     prim_path="{ENV_REGEX_NS}/Robot",
-#     spawn=sim_utils.UsdFileCfg(
-#         usd_path=LOCAL_H1_USD_FILE,
-#         activate_contact_sensors=True,
-#         rigid_props=sim_utils.RigidBodyPropertiesCfg(
-#             disable_gravity=False,
-#             retain_accelerations=False,
-#             linear_damping=0.0,
-#             angular_damping=0.0,
-#             max_linear_velocity=1000.0,
-#             max_angular_velocity=1000.0,
-#             max_depenetration_velocity=1.0,
-#         ),
-#         articulation_props=sim_utils.ArticulationRootPropertiesCfg(
-#             enabled_self_collisions=True, solver_position_iteration_count=4, solver_velocity_iteration_count=0
-#         ),
-#     ),
-#     init_state=ArticulationCfg.InitialStateCfg(
-#         pos=(0.0, 0.0, 1.1),
-#         joint_pos={
-#             ".*hip_yaw.*" : 0. ,
-#             ".*hip_roll.*" : 0. ,
-#             ".*hip_pitch.*": -0.4,
-#             ".*knee.*": 0.8,
-#             ".*ankle.*": 0.0,
-#             # torso.*: 0.0,
-#         },
-#         joint_vel={".*": 0.0},
-#     ),
-#     soft_joint_pos_limit_factor=0.9,
-#     actuators={
-#         "base_legs": ImplicitActuatorCfg(
-#             joint_names_expr=[".*"],
-#             effort_limit={
-#                 ".*hip.*": 200,
-#                 ".*knee.*": 200,
-#                 ".*ankle.*": 19,
-#                 #  .*torso.*: 200,
-#             },
-#             # saturation_effort=200.0,
-#             velocity_limit={
-#                 ".*hip.*": 23,
-#                 ".*knee.*": 14,
-#                 ".*ankle.*": 9,
-#                 #  .*torso.*: 9,
-#             },
-#             # friction=0.0,
-#             stiffness = {".*hip.*": 200,
-#                      ".*knee.*": 200,
-#                      ".*ankle.*": 40,
-#                     #  .*torso.*: 300,
-#                      },  # [N*m/rad]
-#             damping = {".*hip.*": 5,
-#                      ".*knee.*": 5,
-#                      ".*ankle.*": 2,
-#                     #  .*torso.*: 6,
-#                      }  # [N*m/rad]  # [N*m*s/rad]
-#         ),
-#     },
-# )
-
-
-@configclass
-class H1RoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 32
-    max_iterations = 50000
-    save_interval = 500
-    experiment_name = "h1_rough"
-    empirical_normalization = False
-    policy = RslRlPpoActorCriticCfg(
-        init_noise_std=1.0,
-        # actor_hidden_dims=[512, 256, 128],
-        # critic_hidden_dims=[512, 256, 128],
-        actor_hidden_dims=[128, 128, 64],
-        critic_hidden_dims=[128, 128, 64],
-        activation="elu",
-        class_name="ActorCritic",
-    )
-    algorithm = RslRlPpoAlgorithmCfg(
-        value_loss_coef=1.0,
-        use_clipped_value_loss=True,
-        clip_param=0.2,
-        entropy_coef=0.005,
-        num_learning_epochs=5,
-        num_mini_batches=4,
-        learning_rate=1.0e-3,
-        schedule="adaptive",
-        gamma=0.99,
-        lam=0.95,
-        desired_kl=0.01,
-        max_grad_norm=1.0,
-    )
-    
-
-"""Configuration for custom terrains."""
-ROUGH_TERRAINS_CFG = TerrainGeneratorCfg(
-    size=(8.0, 8.0),
-    border_width=20.0,
-    num_rows=10,
-    num_cols=20,
-    horizontal_scale=0.1,
-    vertical_scale=0.005,
-    slope_threshold=0.75,
-    use_cache=False,
-    sub_terrains={
-        "pyramid_stairs": terrain_gen.MeshPyramidStairsTerrainCfg(
-            proportion=0.2,
-            step_height_range=(0.05, 0.23),
-            step_width=0.3,
-            platform_width=3.0,
-            border_width=1.0,
-            holes=False,
-        ),
-        "pyramid_stairs_inv": terrain_gen.MeshInvertedPyramidStairsTerrainCfg(
-            proportion=0.2,
-            step_height_range=(0.05, 0.23),
-            step_width=0.3,
-            platform_width=3.0,
-            border_width=1.0,
-            holes=False,
-        ),
-        "boxes": terrain_gen.MeshRandomGridTerrainCfg(
-            proportion=0.2, grid_width=0.45, grid_height_range=(0.05, 0.2), platform_width=2.0
-        ),
-        "random_rough": terrain_gen.HfRandomUniformTerrainCfg(
-            proportion=0.2, noise_range=(0.02, 0.10), noise_step=0.02, border_width=0.25
-        ),
-        "hf_pyramid_slope": terrain_gen.HfPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25
-        ),
-        "hf_pyramid_slope_inv": terrain_gen.HfInvertedPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25 ),
-        "init_pos": terrain_gen.HfDiscreteObstaclesTerrainCfg(
-            proportion=1.0, 
-            num_obstacles=10,
-            obstacle_height_mode="choice",
-            obstacle_height_range=(3.0, 3.0), obstacle_width_range=(0.5, 1.5), 
-            platform_width=2.0
-        ),
-    },
-)
-for sub_terrain_name, sub_terrain_cfg in ROUGH_TERRAINS_CFG.sub_terrains.items():
-            sub_terrain_cfg.flat_patch_sampling = {
-                sub_terrain_name: FlatPatchSamplingCfg(num_patches=2, patch_radius=[0.01,0.1,0.5,1.0], max_height_diff=0.5)
-            }
-"""Rough terrains configuration."""
-
-##
-# Scene definition
-##
-@configclass
-class TrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="generator",
-        terrain_generator=ROUGH_TERRAINS_CFG,
-        max_init_terrain_level=5, # TRY 9 AS WELL
-        collision_group=-1,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        visual_material=sim_utils.MdlFileCfg(
-            mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
-            project_uvw=True,
-            texture_scale=(0.25, 0.25),
-        ),
-        debug_vis=False,
-    )
-    # robots
-    robot = H1_MINIMAL_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = H1_2DoF_CFG
-    # sensors
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    
-    height_scanner = None
-    # height_scanner = RayCasterCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/base",
-    #     offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 20.0)),
-    #     attach_yaw_only=True,
-    #     pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-    #     debug_vis=False,
-    #     mesh_prim_paths=["/World/ground"],
-    # )
-    # height_scanner.prim_path = "{ENV_REGEX_NS}/Robot/torso_link"
-    # camera
-    lidar_sensor = None
-    lidar_sensor = RayCasterCameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-        # mesh_prim_paths=["/home/zhaojing/terrain-generator/results/generated_terrain/mesh_1/mesh.ply"],
-        mesh_prim_paths=["/World/ground"],
-        update_period=0.1,
-        # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.60, 0.0, 0.0), rot=(-0.5, 0.5, -0.5, 0.5)),
-        # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(0.579, -0.579, 0.406, -0.406)),
-        offset=RayCasterCameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(0,1.0,0.0,0.0)),
-        data_types=["distance_to_image_plane"],
-        debug_vis=True,
-    #     pattern_cfg=patterns.BpearlPatternCfg(
-    #         vertical_ray_angles=[
-    #     89.5, 86.6875, 83.875, 81.0625, 78.25, 75.4375, 72.625, 69.8125, 67.0, 64.1875, 61.375,
-    #     58.5625, 55.75, 52.9375, 50.125, 47.3125, 44.5, 41.6875, 38.875
-    # ]
-    #     ),
-        pattern_cfg=patterns.PinholeCameraPatternCfg(
-            focal_length=24.0,  #24.0
-            horizontal_aperture=46.0,#87,#20.955,
-            height=15,
-            width=15,
-        ),
-        # pattern_cfg=patterns.PinholeCameraPatternCfg(
-        #     focal_length=24.0,  #24.0
-        #     horizontal_aperture=46.0,#87,#20.955,
-        #     height=15,
-        #     width=25,
-        # ),
-        max_distance=10,
-    )
-
-
-##
-# Observations
-##
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.1, n_max=0.1))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        projected_gravity = ObsTerm(
-            func=mdp.projected_gravity,
-            noise=Unoise(n_min=-0.05, n_max=0.05),
-        )
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-        lidar_measurement = ObsTerm(
-            func=mdp.process_depth_image,
-            params={"sensor_cfg": SceneEntityCfg("lidar_sensor"), "data_type": "distance_to_image_plane"},
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-
-## 
-# Actions
-##
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    joint_pos = mdp.JointPositionActionCfg(asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True)
-
-##
-# Events (for domain randomization)
-##
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    # startup
-    physics_material = EventTerm(
-        func=mdp.randomize_rigid_body_material,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-            "static_friction_range": (0.8, 0.8),
-            "dynamic_friction_range": (0.6, 0.6),
-            "restitution_range": (0.0, 0.0),
-            "num_buckets": 64,
-        },
-    )
-
-    add_base_mass = EventTerm(
-        func=mdp.randomize_rigid_body_mass,
-        mode="startup",
-        params={"asset_cfg": SceneEntityCfg("robot", body_names="pelvis"), "mass_distribution_params": (-5.0, 5.0), "operation": "add"},
-    )
-
-    # reset
-    base_external_force_torque = EventTerm(
-        func=mdp.apply_external_force_torque,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="pelvis"),
-            "force_range": (0.0, 0.0),
-            "torque_range": (-0.0, 0.0),
-        },
-    )
-
-    # reset_base = EventTerm(
-    #     func=mdp.reset_root_state_uniform,
-    #     mode="reset",
-    #     params={
-    #         "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-3.14, 3.14)},
-    #         "velocity_range": {
-    #             "x": (-0.5, 0.5),
-    #             "y": (-0.5, 0.5),
-    #             "z": (-0.5, 0.5),
-    #             "roll": (-0.5, 0.5),
-    #             "pitch": (-0.5, 0.5),
-    #             "yaw": (-0.5, 0.5),
-    #         },
-    #     },
-    # )
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_from_terrain,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (-0.5, 0.5),
-                "y": (-0.5, 0.5),
-                "z": (-0.5, 0.5),
-                "roll": (-0.5, 0.5),
-                "pitch": (-0.5, 0.5),
-                "yaw": (-0.5, 0.5),
-            },
-        },
-    )
-
-    reset_robot_joints = EventTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "position_range": (0.5, 1.5),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-    # # interval
-    # push_robot = EventTerm(
-    #     func=mdp.push_by_setting_velocity,
-    #     mode="interval",
-    #     interval_range_s=(10.0, 15.0),
-    #     params={"velocity_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5)}},
-    # )
-
-
-##
-# Rewards
-#
-@configclass
-class RewardsCfg2DoF:
-    """
-    Rewards configuration for the 2-DoF H1 (arm fixed) robot.
-    """
-    # -- task
-    termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    track_lin_vel_xy_exp = RewTerm(
-        func=mdp.track_lin_vel_xy_exp, weight=2.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    )
-    track_ang_vel_z_exp = RewTerm(
-        func=mdp.track_ang_vel_z_exp, weight=1.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    )
-    # -- penalties
-    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-2.0)
-    ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
-    dof_torques_l2 = RewTerm(func=mdp.joint_torques_l2, weight=-5.0e-6)
-    dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-5e-8)
-    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
-    feet_air_time = RewTerm(
-        func=mdp.feet_air_time_positive_biped,
-        weight=2.0,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-            "command_name": "base_velocity",
-            "threshold": 0.3,
-        },
-    )
-    feet_slide = RewTerm(
-        func=mdp.feet_slide,
-        weight=-0.1,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*ankle_link"),
-        },
-    )
-    # Penalize ankle joint limits
-    dof_pos_limits = RewTerm(
-        func=mdp.joint_pos_limits, weight=-1.0, params={"asset_cfg": SceneEntityCfg("robot", joint_names=".*_ankle")}
-    )
-    joint_deviation_hip = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.2,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*hip.*"])},
-    )
-    joint_deviation_toes = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.1,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*ankle.*"])},
-    )
-    joint_deviation_arms = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.2,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_shoulder_.*", ".*_elbow"])},
-    )
-    joint_deviation_torso = RewTerm(
-        func=mdp.joint_deviation_l1, weight=-0.1, params={"asset_cfg": SceneEntityCfg("robot", joint_names="torso")}
-    )
-    # termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    # lin_vel_z_l2 = None
-    # track_lin_vel_xy_exp = RewTerm(
-    #     func=mdp.track_lin_vel_xy_yaw_frame_exp,
-    #     weight=1.0,
-    #     params={"command_name": "base_velocity", "std": 0.5},
-    # )
-    # track_ang_vel_z_exp = RewTerm(
-    #     func=mdp.track_ang_vel_z_world_exp, weight=1.0, params={"command_name": "base_velocity", "std": 0.5}
-    # )
-    # feet_air_time = RewTerm(
-    #     func=mdp.feet_air_time_positive_biped,
-    #     weight=0.5,
-    #     params={
-    #         "command_name": "base_velocity",
-    #         "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-    #         "threshold": 0.4,
-    #     },
-    # )
-    # # feet_slide = RewTerm(
-    # #     func=mdp.feet_slide,
-    # #     weight=-0.25,
-    # #     params={
-    # #         "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-    # #         "asset_cfg": SceneEntityCfg("robot", body_names=".*ankle_link"),
-    # #     },
-    # # )
-    # # Penalize ankle joint limits
-    # dof_pos_limits = RewTerm(
-    #     func=mdp.joint_pos_limits, weight=-1.0, params={"asset_cfg": SceneEntityCfg("robot", joint_names=".*_ankle")}
-    # )
-    # # Penalize deviation from default of the joints that are not essential for locomotion
-    # joint_deviation_hip = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.2,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_yaw", ".*_hip_roll"])},
-    # )
-    # joint_deviation_arms = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.2,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_shoulder_.*", ".*_elbow"])},
-    # )
-    # joint_deviation_torso = RewTerm(
-    #     func=mdp.joint_deviation_l1, weight=-0.1, params={"asset_cfg": SceneEntityCfg("robot", joint_names="torso")}
-    # )
-
-
-@configclass
-class CustomH1Rewards(H1Rewards):
-    feet_stumble = RewTerm(
-        func=mdp.feet_stumble,
-        weight=-0.5,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-        },
-    )
-
-##
-# Commands
-##
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    base_velocity = mdp.UniformVelocityCommandCfg(
-        asset_name="robot",
-        resampling_time_range=(10.0, 10.0),
-        rel_standing_envs=0.02,
-        rel_heading_envs=1.0,
-        heading_command=True,
-        heading_control_stiffness=0.5,
-        debug_vis=True,
-        ranges=mdp.UniformVelocityCommandCfg.Ranges(
-            lin_vel_x=(-1.0, 1.0), lin_vel_y=(-1.0, 1.0), ang_vel_z=(-1.0, 1.0), heading=(-math.pi, math.pi)
-        ),
-    )
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*torso_link"), "threshold": 1.0},
-    )
-
-
-# @configclass
-# class H1RoughEnvCfg(LocomotionVelocityRoughEnvCfg):
-#     rewards: H1Rewards = H1Rewards()
-#     terminations: TerminationsCfg = TerminationsCfg()
-#     scene = MySceneCfg(num_envs=4096, env_spacing=2.5)
-#     observations: ObservationsCfg = ObservationsCfg()
-
-#     def __post_init__(self):
-#         # post init of parent
-#         super().__post_init__()
-#         # Scene
-#         self.scene.robot = H1_MINIMAL_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-#         # self.scene.height_scanner.prim_path = "{ENV_REGEX_NS}/Robot/torso_link"
-
-#         # Randomization
-#         self.events.push_robot = None
-#         self.events.add_base_mass = None
-#         self.events.reset_robot_joints.params["position_range"] = (1.0, 1.0)
-#         self.events.base_external_force_torque.params["asset_cfg"].body_names = [".*torso_link"]
-#         self.events.reset_base.params = {
-#             "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-#             "velocity_range": {
-#                 "x": (0.0, 0.0),
-#                 "y": (0.0, 0.0),
-#                 "z": (0.0, 0.0),
-#                 "roll": (0.0, 0.0),
-#                 "pitch": (0.0, 0.0),
-#                 "yaw": (0.0, 0.0),
-#             },
-#         }
-
-#         # Terminations
-#         self.terminations.base_contact.params["sensor_cfg"].body_names = [".*torso_link"]
-
-#         # Rewards
-#         self.rewards.undesired_contacts = None
-#         self.rewards.flat_orientation_l2.weight = -1.0
-#         self.rewards.dof_torques_l2.weight = 0.0
-#         self.rewards.action_rate_l2.weight = -0.005
-#         self.rewards.dof_acc_l2.weight = -1.25e-7
-
-#         # Commands
-#         self.commands.base_velocity.ranges.lin_vel_x = (0.0, 1.0)
-#         self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-#         self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-
-@configclass
-class H1RoughEnvCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Scene settings
-    scene: TrainSceneCfg = TrainSceneCfg(num_envs=4096, env_spacing=2.5)
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # MDP settings
-    # rewards: RewardsCfg = RewardsCfg2DoF()
-    # rewards: RewardsCfg = H1Rewards()
-    rewards: RewardsCfg = CustomH1Rewards()
-    terminations: TerminationsCfg = TerminationsCfg()
-    events: EventCfg = EventCfg()
-    curriculum: CurriculumCfg = CurriculumCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 4
-        self.episode_length_s = 20.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material = self.scene.terrain.physics_material
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-
-        # Randomization
-        self.events.push_robot = None
-        self.events.add_base_mass = None
-        self.events.reset_robot_joints.params["position_range"] = (1.0, 1.0)
-        self.events.base_external_force_torque.params["asset_cfg"].body_names = [".*torso_link"]
-        self.events.reset_base.params = {
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (0.0, 0.0),
-                "y": (0.0, 0.0),
-                "z": (0.0, 0.0),
-                "roll": (0.0, 0.0),
-                "pitch": (0.0, 0.0),
-                "yaw": (0.0, 0.0),
-            },
-        }
-
-        # Rewards
-        self.rewards.undesired_contacts = None
-        self.rewards.flat_orientation_l2.weight = -1.0
-        self.rewards.dof_torques_l2.weight = 0.0
-        self.rewards.action_rate_l2.weight = -0.005
-        self.rewards.dof_acc_l2.weight = -1.25e-7
-        self.rewards.feet_air_time.weight = 0.5
-
-        # Commands
-        self.commands.base_velocity.ranges.lin_vel_x = (0.0, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        # self.scene.lidar_sensor = None
-        if self.scene.lidar_sensor is not None:
-            self.scene.lidar_sensor.update_period = 4 * self.sim.dt
-        self.scene.contact_forces.update_period = self.sim.dt
-        # if self.scene.height_scanner is not None:
-        #     self.scene.height_scanner.update_period = self.decimation * self.sim.dt
-        if self.scene.contact_forces is not None:
-            self.scene.contact_forces.update_period = self.sim.dt
-
-        # check if terrain levels curriculum is enabled - if so, enable curriculum for terrain generator
-        # this generates terrains with increasing difficulty and is useful for training
-        if getattr(self.curriculum, "terrain_levels", None) is not None:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = True
-        else:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = False
-        # self.curriculum.terrain_levels = None
-
-
-@configclass
-class H1RoughEnvCfg_PLAY(H1RoughEnvCfg):
-    def __post_init__(self):
-        # post init of parent
-        super().__post_init__()
-
-        # make a smaller scene for play
-        self.scene.num_envs = 40
-        self.scene.env_spacing = 2.5
-        self.episode_length_s = 40.0
-        # spawn the robot randomly in the grid (instead of their terrain levels)
-        self.scene.terrain.max_init_terrain_level = None
-        # reduce the number of terrains to save memory
-        if self.scene.terrain.terrain_generator is not None:
-            self.scene.terrain.terrain_generator.num_rows = 5
-            self.scene.terrain.terrain_generator.num_cols = 5
-            self.scene.terrain.terrain_generator.curriculum = False
-
-        self.commands.base_velocity.ranges.lin_vel_x = (0.5, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-        self.commands.base_velocity.ranges.heading = (0.0, 0.0)
-        self.commands.base_velocity.heading_command = True
-        # disable randomization for play
-        self.observations.policy.enable_corruption = False
-        # remove random pushing
-        self.events.base_external_force_torque = None
-        self.events.push_robot = None
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_vision_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_vision_cfg.py
deleted file mode 100644
index 4db7d06..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_low_vision_cfg.py
+++ /dev/null
@@ -1,679 +0,0 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import math
-from dataclasses import MISSING
-
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import ArticulationCfg, AssetBaseCfg
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import CurriculumTermCfg as CurrTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg, FlatPatchSamplingCfg
-import omni.isaac.lab.terrains as terrain_gen
-from omni.isaac.lab.utils import configclass
-from omni.isaac.lab.utils.assets import ISAAC_NUCLEUS_DIR, ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.utils.noise import AdditiveUniformNoiseCfg as Unoise
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg, patterns
-from omni.isaac.lab.assets.articulation import ArticulationCfg
-from omni.isaac.lab.actuators import ImplicitActuatorCfg
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import ActionsCfg, CurriculumCfg, RewardsCfg, EventCfg, LocomotionVelocityRoughEnvCfg
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.config.h1.rough_env_cfg import H1Rewards
-# from omni.isaac.lab_tasks.manager_based.locomotion.velocity.config.h1.rough_env_cfg import H1RoughEnvCfg as OriH1RoughEnvCfg
-
-##
-# Pre-defined configs
-##
-from omni.isaac.lab.terrains.config.rough import ROUGH_TERRAINS_CFG  # isort: skip
-from omni.isaac.lab_assets import H1_MINIMAL_CFG  # isort: skip
-from .h1_low_base_cfg import H1RoughPPORunnerCfg
-
-
-@configclass
-class H1VisionRoughPPORunnerCfg(H1RoughPPORunnerCfg):
-    experiment_name = "h1_vision_rough"
-
-
-
-"""Configuration for custom terrains."""
-ROUGH_TERRAINS_CFG = TerrainGeneratorCfg(
-    size=(8.0, 8.0),
-    border_width=20.0,
-    num_rows=10,
-    num_cols=20,
-    horizontal_scale=0.1,
-    vertical_scale=0.005,
-    slope_threshold=0.75,
-    use_cache=False,
-    sub_terrains={
-        "pyramid_stairs": terrain_gen.MeshPyramidStairsTerrainCfg(
-            proportion=0.2,
-            step_height_range=(0.05, 0.3),
-            step_width=0.3,
-            platform_width=3.0,
-            border_width=1.0,
-            holes=False,
-        ),
-        "pyramid_stairs_inv": terrain_gen.MeshInvertedPyramidStairsTerrainCfg(
-            proportion=0.2,
-            step_height_range=(0.05, 0.3),
-            step_width=0.3,
-            platform_width=3.0,
-            border_width=1.0,
-            holes=False,
-        ),
-        "boxes": terrain_gen.MeshRandomGridTerrainCfg(
-            proportion=0.2, grid_width=0.45, grid_height_range=(0.05, 0.2), platform_width=2.0
-        ),
-        "random_rough": terrain_gen.HfRandomUniformTerrainCfg(
-            proportion=0.2, noise_range=(0.02, 0.10), noise_step=0.02, border_width=0.25
-        ),
-        "hf_pyramid_slope": terrain_gen.HfPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25
-        ),
-        "hf_pyramid_slope_inv": terrain_gen.HfInvertedPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25
-        ),
-        "init_pos": terrain_gen.HfDiscreteObstaclesTerrainCfg(
-            proportion=0.2, 
-            num_obstacles=10,
-            obstacle_height_mode="choice",
-            obstacle_height_range=(1.5, 1.5), obstacle_width_range=(0.35, 1.5), 
-            platform_width=0.25
-        ),
-    },
-)
-for sub_terrain_name, sub_terrain_cfg in ROUGH_TERRAINS_CFG.sub_terrains.items():
-            sub_terrain_cfg.flat_patch_sampling = {
-                sub_terrain_name: FlatPatchSamplingCfg(num_patches=2, patch_radius=[0.01,0.1,0.5,1.0], max_height_diff=0.5)
-            }
-"""Rough terrains configuration."""
-
-
-##
-# Scene definition
-##
-@configclass
-class TrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="generator",
-        terrain_generator=ROUGH_TERRAINS_CFG,
-        max_init_terrain_level=5, # TRY 9 AS WELL
-        collision_group=-1,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        visual_material=sim_utils.MdlFileCfg(
-            mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
-            project_uvw=True,
-            texture_scale=(0.25, 0.25),
-        ),
-        debug_vis=False,
-    )
-    # terrain = TerrainImporterCfg(
-    #     prim_path="/World/ground",
-    #     terrain_type="usd",
-    #     usd_path="/home/zhaojing/terrain-generator/results/generated_terrain/mesh_1/mesh.usd",
-    # )
-    # robots
-    robot = H1_MINIMAL_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # sensors
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    
-    height_scanner = None
-    # height_scanner = RayCasterCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/base",
-    #     offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 20.0)),
-    #     attach_yaw_only=True,
-    #     pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-    #     debug_vis=False,
-    #     mesh_prim_paths=["/World/ground"],
-    # )
-    # height_scanner.prim_path = "{ENV_REGEX_NS}/Robot/torso_link"
-    # camera
-    lidar_sensor = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-        # mesh_prim_paths=["/home/zhaojing/terrain-generator/results/generated_terrain/mesh_1/mesh.ply"],
-        mesh_prim_paths=["/World/ground"],
-        update_period=0.1,
-        attach_yaw_only=False,
-        # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.60, 0.0, 0.0), rot=(-0.5, 0.5, -0.5, 0.5)),
-        # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(0.579, -0.579, 0.406, -0.406)),
-        offset=RayCasterCfg.OffsetCfg(pos=(0.047, 0.0, 0.675), rot=(-0.119, 0.0,0.993,0.0)),
-        # data_types=["distance_to_image_plane"],
-        debug_vis=False,
-        pattern_cfg=patterns.BpearlPatternCfg(
-            vertical_ray_angles=[
-                 51.125, 48.0, 45.0, 42.0, 39.0, 36, 33, 30,27,24,20,17,14,11,8,5,2, -1
-    ]
-        ),
-        max_distance=10,
-        # pattern_cfg=patterns.LidarPatternCfg(
-        # channels=16, vertical_fov_range=(-15.0, 15.0), horizontal_fov_range=(-180.0, 180.0), horizontal_res=0.2
-        # ),
-        # debug_vis=True,
-        # max_distance=100,
-        # pattern_cfg=patterns.PinholeCameraPatternCfg(
-        #     focal_length=24.0,  #24.0
-        #     horizontal_aperture=46.0,#87,#20.955,
-        #     height=15,
-        #     width=15,
-        # ),
-        # pattern_cfg=patterns.PinholeCameraPatternCfg(
-        #     focal_length=24.0,  #24.0
-        #     horizontal_aperture=46.0,#87,#20.955,
-        #     height=15,
-        #     width=25,
-        # ),
-        # max_distance=10,
-    )
-
-    depth_sensor = RayCasterCameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-        mesh_prim_paths=["/World/ground"],
-        update_period=0.1,
-        offset=RayCasterCameraCfg.OffsetCfg(pos=(0.108, -0.0325, 0.693), rot=(0.336, 0.0, 0.942, 0.0)),
-        data_types=["distance_to_image_plane"],
-        debug_vis=False,
-        pattern_cfg=patterns.PinholeCameraPatternCfg(
-            # focal_length=24.0,  #24.0
-            # horizontal_aperture=46.0,#87,#20.955,
-            focal_length=1.93, horizontal_aperture=3.8,
-            height=9,
-            width=16,
-        ),
-        max_distance=10,
-    )
-
-##
-# Observations
-##
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel)
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel)
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel)
-        actions = ObsTerm(func=mdp.last_action)
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-        depth_measurement = ObsTerm(
-            func=mdp.process_lidar,
-            params={"sensor_cfg": SceneEntityCfg("lidar_sensor")},
-        )
-        realsense_depth_measurement = ObsTerm(
-            func=mdp.process_depth_image,
-            params={"sensor_cfg": SceneEntityCfg("depth_sensor"), "data_type": "distance_to_image_plane"},
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    @configclass
-    class ProprioCfg(ObsGroup):
-        """Observations for proprioceptive group."""
-
-        # observation terms
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel, noise=Unoise(n_min=-0.2, n_max=0.2))
-        projected_gravity = ObsTerm(
-            func=mdp.projected_gravity,
-            noise=Unoise(n_min=-0.05, n_max=0.05),
-        )
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel, noise=Unoise(n_min=-0.01, n_max=0.01))
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel, noise=Unoise(n_min=-1.5, n_max=1.5))
-        actions = ObsTerm(func=mdp.last_action)
-
-        def __post_init__(self):
-            self.concatenate_terms = True
-
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-    proprio: ProprioCfg = ProprioCfg()
-
-## 
-# Actions
-##
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    joint_pos = mdp.JointPositionActionCfg(asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True)
-
-##
-# Events (for domain randomization)
-##
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    # startup
-    physics_material = EventTerm(
-        func=mdp.randomize_rigid_body_material,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-            "static_friction_range": (0.8, 0.8),
-            "dynamic_friction_range": (0.6, 0.6),
-            "restitution_range": (0.0, 0.0),
-            "num_buckets": 64,
-        },
-    )
-
-    add_base_mass = EventTerm(
-        func=mdp.randomize_rigid_body_mass,
-        mode="startup",
-        params={"asset_cfg": SceneEntityCfg("robot", body_names="pelvis"), "mass_distribution_params": (-5.0, 5.0), "operation": "add"},
-    )
-
-    # reset
-    base_external_force_torque = EventTerm(
-        func=mdp.apply_external_force_torque,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="pelvis"),
-            "force_range": (0.0, 0.0),
-            "torque_range": (-0.0, 0.0),
-        },
-    )
-
-    # reset_base = EventTerm(
-    #     func=mdp.reset_root_state_uniform,
-    #     mode="reset",
-    #     params={
-    #         "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-3.14, 3.14)},
-    #         "velocity_range": {
-    #             "x": (-0.5, 0.5),
-    #             "y": (-0.5, 0.5),
-    #             "z": (-0.5, 0.5),
-    #             "roll": (-0.5, 0.5),
-    #             "pitch": (-0.5, 0.5),
-    #             "yaw": (-0.5, 0.5),
-    #         },
-    #     },
-    # )
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_from_terrain,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (-0.5, 0.5),
-                "y": (-0.5, 0.5),
-                "z": (-0.5, 0.5),
-                "roll": (-0.5, 0.5),
-                "pitch": (-0.5, 0.5),
-                "yaw": (-0.5, 0.5),
-            },
-        },
-    )
-
-    reset_robot_joints = EventTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "position_range": (0.5, 1.5),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-    # # interval
-    # push_robot = EventTerm(
-    #     func=mdp.push_by_setting_velocity,
-    #     mode="interval",
-    #     interval_range_s=(10.0, 15.0),
-    #     params={"velocity_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5)}},
-    # )
-
-
-##
-# Rewards
-#
-@configclass
-class RewardsCfg2DoF:
-    """
-    Rewards configuration for the 2-DoF H1 (arm fixed) robot.
-    """
-    # -- task
-    termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    track_lin_vel_xy_exp = RewTerm(
-        func=mdp.track_lin_vel_xy_exp, weight=2.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    )
-    track_ang_vel_z_exp = RewTerm(
-        func=mdp.track_ang_vel_z_exp, weight=1.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    )
-    # -- penalties
-    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-2.0)
-    ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
-    dof_torques_l2 = RewTerm(func=mdp.joint_torques_l2, weight=-5.0e-6)
-    dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-5e-8)
-    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
-    feet_air_time = RewTerm(
-        func=mdp.feet_air_time_positive_biped,
-        weight=2.0,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-            "command_name": "base_velocity",
-            "threshold": 0.3,
-        },
-    )
-    feet_slide = RewTerm(
-        func=mdp.feet_slide,
-        weight=-0.1,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*ankle_link"),
-        },
-    )
-    # Penalize ankle joint limits
-    dof_pos_limits = RewTerm(
-        func=mdp.joint_pos_limits, weight=-1.0, params={"asset_cfg": SceneEntityCfg("robot", joint_names=".*_ankle")}
-    )
-    joint_deviation_hip = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.2,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*hip.*"])},
-    )
-    joint_deviation_toes = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.1,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*ankle.*"])},
-    )
-    joint_deviation_arms = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.2,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_shoulder_.*", ".*_elbow"])},
-    )
-    joint_deviation_torso = RewTerm(
-        func=mdp.joint_deviation_l1, weight=-0.1, params={"asset_cfg": SceneEntityCfg("robot", joint_names="torso")}
-    )
-    # termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    # lin_vel_z_l2 = None
-    # track_lin_vel_xy_exp = RewTerm(
-    #     func=mdp.track_lin_vel_xy_yaw_frame_exp,
-    #     weight=1.0,
-    #     params={"command_name": "base_velocity", "std": 0.5},
-    # )
-    # track_ang_vel_z_exp = RewTerm(
-    #     func=mdp.track_ang_vel_z_world_exp, weight=1.0, params={"command_name": "base_velocity", "std": 0.5}
-    # )
-    # feet_air_time = RewTerm(
-    #     func=mdp.feet_air_time_positive_biped,
-    #     weight=0.5,
-    #     params={
-    #         "command_name": "base_velocity",
-    #         "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-    #         "threshold": 0.4,
-    #     },
-    # )
-    # # feet_slide = RewTerm(
-    # #     func=mdp.feet_slide,
-    # #     weight=-0.25,
-    # #     params={
-    # #         "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-    # #         "asset_cfg": SceneEntityCfg("robot", body_names=".*ankle_link"),
-    # #     },
-    # # )
-    # # Penalize ankle joint limits
-    # dof_pos_limits = RewTerm(
-    #     func=mdp.joint_pos_limits, weight=-1.0, params={"asset_cfg": SceneEntityCfg("robot", joint_names=".*_ankle")}
-    # )
-    # # Penalize deviation from default of the joints that are not essential for locomotion
-    # joint_deviation_hip = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.2,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_yaw", ".*_hip_roll"])},
-    # )
-    # joint_deviation_arms = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.2,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_shoulder_.*", ".*_elbow"])},
-    # )
-    # joint_deviation_torso = RewTerm(
-    #     func=mdp.joint_deviation_l1, weight=-0.1, params={"asset_cfg": SceneEntityCfg("robot", joint_names="torso")}
-    # )
-
-
-@configclass
-class CustomH1Rewards(H1Rewards):
-    feet_stumble = RewTerm(
-        func=mdp.feet_stumble,
-        weight=-0.5,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-        },
-    )
-##
-# Commands
-##
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    base_velocity = mdp.UniformVelocityCommandCfg(
-        asset_name="robot",
-        resampling_time_range=(10.0, 10.0),
-        rel_standing_envs=0.02,
-        rel_heading_envs=1.0,
-        heading_command=True,
-        heading_control_stiffness=0.5,
-        debug_vis=True,
-        ranges=mdp.UniformVelocityCommandCfg.Ranges(
-            lin_vel_x=(-1.0, 1.0), lin_vel_y=(-1.0, 1.0), ang_vel_z=(-1.0, 1.0), heading=(-math.pi, math.pi)
-        ),
-    )
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*torso_link"), "threshold": 1.0},
-    )
-
-
-# @configclass
-# class H1RoughEnvCfg(LocomotionVelocityRoughEnvCfg):
-#     rewards: H1Rewards = H1Rewards()
-#     terminations: TerminationsCfg = TerminationsCfg()
-#     scene = MySceneCfg(num_envs=4096, env_spacing=2.5)
-#     observations: ObservationsCfg = ObservationsCfg()
-
-#     def __post_init__(self):
-#         # post init of parent
-#         super().__post_init__()
-#         # Scene
-#         self.scene.robot = H1_MINIMAL_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-#         # self.scene.height_scanner.prim_path = "{ENV_REGEX_NS}/Robot/torso_link"
-
-#         # Randomization
-#         self.events.push_robot = None
-#         self.events.add_base_mass = None
-#         self.events.reset_robot_joints.params["position_range"] = (1.0, 1.0)
-#         self.events.base_external_force_torque.params["asset_cfg"].body_names = [".*torso_link"]
-#         self.events.reset_base.params = {
-#             "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-#             "velocity_range": {
-#                 "x": (0.0, 0.0),
-#                 "y": (0.0, 0.0),
-#                 "z": (0.0, 0.0),
-#                 "roll": (0.0, 0.0),
-#                 "pitch": (0.0, 0.0),
-#                 "yaw": (0.0, 0.0),
-#             },
-#         }
-
-#         # Terminations
-#         self.terminations.base_contact.params["sensor_cfg"].body_names = [".*torso_link"]
-
-#         # Rewards
-#         self.rewards.undesired_contacts = None
-#         self.rewards.flat_orientation_l2.weight = -1.0
-#         self.rewards.dof_torques_l2.weight = 0.0
-#         self.rewards.action_rate_l2.weight = -0.005
-#         self.rewards.dof_acc_l2.weight = -1.25e-7
-
-#         # Commands
-#         self.commands.base_velocity.ranges.lin_vel_x = (0.0, 1.0)
-#         self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-#         self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-
-@configclass
-class H1VisionRoughEnvCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Scene settings
-    scene: TrainSceneCfg = TrainSceneCfg(num_envs=4096, env_spacing=2.5)
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # MDP settings
-    # rewards: RewardsCfg = RewardsCfg2DoF()
-    rewards: RewardsCfg = H1Rewards()
-    # rewards: RewardsCfg = CustomH1Rewards()
-    terminations: TerminationsCfg = TerminationsCfg()
-    events: EventCfg = EventCfg()
-    curriculum: CurriculumCfg = CurriculumCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 4
-        self.episode_length_s = 20.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material = self.scene.terrain.physics_material
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-
-        # Randomization
-        self.events.push_robot = None
-        self.events.add_base_mass = None
-        self.events.reset_robot_joints.params["position_range"] = (1.0, 1.0)
-        self.events.base_external_force_torque.params["asset_cfg"].body_names = [".*torso_link"]
-        self.events.reset_base.params = {
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (0.0, 0.0),
-                "y": (0.0, 0.0),
-                "z": (0.0, 0.0),
-                "roll": (0.0, 0.0),
-                "pitch": (0.0, 0.0),
-                "yaw": (0.0, 0.0),
-            },
-        }
-
-        # Rewards
-        self.rewards.undesired_contacts = None
-        self.rewards.flat_orientation_l2.weight = -1.0
-        self.rewards.dof_torques_l2.weight = 0.0
-        self.rewards.action_rate_l2.weight = -0.005
-        self.rewards.dof_acc_l2.weight = -1.25e-7
-        self.rewards.feet_air_time.weight=0.25
-
-        # Commands
-        self.commands.base_velocity.ranges.lin_vel_x = (0.0, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        # self.scene.lidar_sensor = None
-        if self.scene.lidar_sensor is not None:
-            self.scene.lidar_sensor.update_period = 4 * self.sim.dt
-        if self.scene.depth_sensor is not None:
-            self.scene.depth_sensor.update_period = 4 * self.sim.dt
-        self.scene.contact_forces.update_period = self.sim.dt
-        # if self.scene.height_scanner is not None:
-        #     self.scene.height_scanner.update_period = self.decimation * self.sim.dt
-        if self.scene.contact_forces is not None:
-            self.scene.contact_forces.update_period = self.sim.dt
-
-        # check if terrain levels curriculum is enabled - if so, enable curriculum for terrain generator
-        # this generates terrains with increasing difficulty and is useful for training
-        if getattr(self.curriculum, "terrain_levels", None) is not None:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = True
-        else:
-            if self.scene.terrain.terrain_generator is not None:
-                self.scene.terrain.terrain_generator.curriculum = False
-        # self.curriculum.terrain_levels = None
-
-
-@configclass
-class H1VisionRoughEnvCfg_PLAY(H1VisionRoughEnvCfg):
-    def __post_init__(self):
-        # post init of parent
-        super().__post_init__()
-
-        # make a smaller scene for play
-        self.scene.num_envs = 40
-        self.scene.env_spacing = 2.5
-        self.episode_length_s = 40.0
-        # spawn the robot randomly in the grid (instead of their terrain levels)
-        self.scene.terrain.max_init_terrain_level = None
-        # reduce the number of terrains to save memory
-        if self.scene.terrain.terrain_generator is not None:
-            self.scene.terrain.terrain_generator.num_rows = 5
-            self.scene.terrain.terrain_generator.num_cols = 5
-            self.scene.terrain.terrain_generator.curriculum = False
-
-        self.commands.base_velocity.ranges.lin_vel_x = (1.0, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-        self.commands.base_velocity.ranges.heading = (0.0, 0.0)
-        # disable randomization for play
-        self.observations.policy.enable_corruption = False
-        # remove random pushing
-        self.events.base_external_force_torque = None
-        self.events.push_robot = None
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_matterport_deploy_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_matterport_deploy_cfg.py
deleted file mode 100644
index a000a8e..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_matterport_deploy_cfg.py
+++ /dev/null
@@ -1,302 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import CurriculumTermCfg as CurrTerm
-# from omni.isaac.lab.managers import RandomizationTermCfg as RandTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.utils import configclass
-# from omni.isaac.lab.utils.assets import LOCAL_ISAAC_DIR
-from omni.isaac.lab.utils.assets import ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg
-import omni.isaac.lab.terrains as terrain_gen
-import omni.isaac.lab.terrains.height_field as hf_gen
-from omni.isaac.matterport.config import MatterportImporterCfg
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import AssetBaseCfg
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import CameraCfg, ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.viplanner.utils import UnRealImporterCfg, MatterportRayCasterCameraCfg
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg, patterns
-# from omni.isaac.lab.terrains.config.rough import ROUGH_TERRAINS_CFG
-
-
-from omni.isaac.lab_assets.anymal import ANYMAL_C_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_H1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_G1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_GO2_CFG
-
-import math
-
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-
-##
-# MDP settings
-##
-
-@configclass
-class RewardsCfg:
-        # -- task
-    track_lin_vel_xy_exp = RewTerm(
-        func=mdp.track_lin_vel_xy_exp, weight=2.0, params={"command_name": "robot_vel_command", "std": math.sqrt(0.25)}
-    )
-    track_ang_vel_z_exp = RewTerm(
-        func=mdp.track_ang_vel_z_exp, weight=1.0, params={"command_name": "robot_vel_command", "std": math.sqrt(0.25)}
-    )
-    # -- penalties
-    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-2.0)
-    ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
-    dof_torques_l2 = RewTerm(func=mdp.joint_torques_l2, weight=-5.0e-6)
-    dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-5e-8)
-    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
-    feet_air_time = RewTerm(
-        func=mdp.feet_air_time_positive_biped,
-        weight=2.5,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_roll_link"),
-            "command_name": "robot_vel_command",
-            "threshold": 0.3,
-        },
-    )
-    joint_deviation_hip = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.2,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*hip.*"])},
-    )
-    joint_deviation_toes = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.2,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*ankle.*"])},
-    )
-
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-    vlm_actions = mdp.VLMActionsCfg(
-        asset_name="robot",
-        low_level_decimation=4,
-        low_level_action=mdp.JointPositionActionCfg(
-            asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True
-        ),
-        low_level_policy_file=os.path.join(LOCAL_ISAAC_DIR, "low_level_policy", "h1_policy.jit"),
-    )
-
-
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel)
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "robot_vel_command"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel)
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel)
-        actions = ObsTerm(func=mdp.last_low_level_actions_llava)
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-        depth_measurement = ObsTerm(
-            func=mdp.process_lidar,
-            params={"sensor_cfg": SceneEntityCfg("lidar_sensor"), "data_type": "distance_to_image_plane"},
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    @configclass
-    class CameraObsCfg(ObsGroup):
-        """Observations for camera group."""
-
-        # observation terms (order preserved)
-        # depth_measurement = ObsTerm(
-        #     func=mdp.process_depth_image,
-        #     params={"sensor_cfg": SceneEntityCfg("lidar_sensor"), "data_type": "distance_to_image_plane"},
-        # )
-        rgb_measurement = ObsTerm(
-            func=mdp.isaac_camera_data,
-            params={"sensor_cfg": SceneEntityCfg("rgb_camera"), "data_type": "rgb"},
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = False
-            self.concatenate_terms = False
-    # observation groups
-    low_level_policy: PolicyCfg = PolicyCfg()
-    camera_obs: CameraObsCfg = CameraObsCfg()
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="torso_link"), "threshold": 1.0},
-    )
-    # root_height = DoneTerm(
-    #     func=mdp.root_height_below_minimum,
-    #     params={"asset_cfg": SceneEntityCfg("robot", body_names="pelvis"), "minimum_height": 0.5},
-    # )
-    bad_orientation = DoneTerm(
-        func=mdp.bad_orientation,
-        params={"limit_angle": 0.8},
-    )
-
-
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-    robot_vel_command: mdp.RobotVelCommandGeneratorCfg = mdp.RobotVelCommandGeneratorCfg(
-        robot_attr="robot",
-        debug_vis=True,
-    )
-
-
-@configclass
-class TerrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = MatterportImporterCfg(
-        prim_path="/World/matterport",
-        terrain_type="matterport",
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        obj_filepath="/home/yji/Biped/biped_vision/viplanner/assets/matterport/raw_data/v1/scans/17DRP5sb8fy/matterport_mesh/17DRP5sb8fy/matterport_mesh/bed1a77d92d64f5cbbaaae4feed64ec1/matterport.usd",
-        groundplane=True,
-    )
-
-    # robots
-    # robot = ANYMAL_C_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot = UNITREE_H1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_G1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_GO2_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot.init_state.pos = (0.0, 0.0, 1.1)
-    robot.init_state.rot = (0.7, 0.0, 0.0, 0.7)
-
-    # sensors
-    # height_scanner = RayCasterCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-    #     offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.5)),
-    #     attach_yaw_only=True,
-    #     pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-    #     debug_vis=True,
-    #     mesh_prim_paths=["/World/Custom"],
-    # )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    disk_1 = AssetBaseCfg(
-        prim_path="/World/disk_1",
-        spawn=sim_utils.DiskLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=10000.0,
-            radius=30.0,
-        ),
-    )
-    disk_1.init_state.pos = (0, 0, 2.0)
-    # # camera
-    lidar_sensor = MatterportRayCasterCameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-        offset=MatterportRayCasterCameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(0,1.0,0.0,0.0)),
-        data_types=["distance_to_image_plane"],
-        debug_vis=True,
-        pattern_cfg=patterns.PinholeCameraPatternCfg(
-            focal_length=24.0,  #24.0
-            horizontal_aperture=46.0,#87,#20.955,
-            height=15,
-            width=15,
-        ),
-        # max_distance=10,
-        mesh_prim_paths=["/home/yji/Biped/biped_vision/viplanner/assets/matterport/raw_data/v1/scans/17DRP5sb8fy/matterport_mesh/17DRP5sb8fy/matterport_mesh/bed1a77d92d64f5cbbaaae4feed64ec1/matterport.ply"]
-    )
-    rgb_camera = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/pelvis/semantic_camera",
-        offset=CameraCfg.OffsetCfg(pos=(0.15, 0.0, 0.3), rot=(-0.5, 0.5, -0.5, 0.5)),
-        spawn=sim_utils.PinholeCameraCfg(horizontal_aperture=87.0),
-        width=848,
-        height=480,
-        data_types=["rgb"],
-    )
-
-##
-# Environment configuration
-##
-
-@configclass
-class H1MatterportDeployCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # managers
-    terminations: TerminationsCfg = TerminationsCfg()
-    rewards: RewardsCfg = RewardsCfg()
-    scene: TerrainSceneCfg = TerrainSceneCfg(num_envs=1, env_spacing=2.5)
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 4  # 20->10 Hz, 4->50 Hz
-        self.episode_length_s = 200000.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        # self.scene.height_scanner.update_period = 4 * self.sim.dt  # should we low-level decimation
-        self.scene.lidar_sensor.update_period = 4*self.sim.dt
-        self.scene.contact_forces.update_period = self.sim.dt
-        # self.scene.terrain.terrain_type = "plane"
-        # self.scene.terrain.terrain_generator = None
-        # self.rewards.flat_orientation_l2.weight = -5.0
-        # self.rewards.dof_torques_l2.weight = -2.5e-5
-        # self.rewards.feet_air_time.weight = 0.5
-        self.scene.height_scanner = None
-        # self.observations.policy.height_scan = None
-        # self.curriculum.terrain_levels = None
-
-        self.viewer.eye = (5, 12, 5)
-        self.viewer.lookat = (5, 0, 0.0)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_matterport_deploy_gpt_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_matterport_deploy_gpt_cfg.py
deleted file mode 100644
index d7d0be7..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/h1_matterport_deploy_gpt_cfg.py
+++ /dev/null
@@ -1,367 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import CurriculumTermCfg as CurrTerm
-# from omni.isaac.lab.managers import RandomizationTermCfg as RandTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.utils import configclass
-# from omni.isaac.lab.utils.assets import LOCAL_ISAAC_DIR
-from omni.isaac.lab.utils.assets import ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg
-import omni.isaac.lab.terrains as terrain_gen
-import omni.isaac.lab.terrains.height_field as hf_gen
-from omni.isaac.matterport.config import MatterportImporterCfg
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import AssetBaseCfg
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import CameraCfg, ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.viplanner.utils import UnRealImporterCfg, MatterportRayCasterCameraCfg
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg, patterns
-# from omni.isaac.lab.terrains.config.rough import ROUGH_TERRAINS_CFG
-
-
-from omni.isaac.lab_assets.anymal import ANYMAL_C_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_H1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_G1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_GO2_CFG
-
-import math
-
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-
-@configclass
-class H1RoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 24
-    max_iterations = 30000
-    save_interval = 50
-    experiment_name = "h1_rough"
-    empirical_normalization = False
-    policy = RslRlPpoActorCriticCfg(
-        init_noise_std=1.0,
-        actor_hidden_dims=[512, 256, 128],
-        critic_hidden_dims=[512, 256, 128],
-        # actor_hidden_dims=[128, 128, 128],
-        # critic_hidden_dims=[128, 128, 128],
-        activation="elu",
-    )
-    algorithm = RslRlPpoAlgorithmCfg(
-        value_loss_coef=1.0,
-        use_clipped_value_loss=True,
-        clip_param=0.2,
-        entropy_coef=0.005,
-        num_learning_epochs=5,
-        num_mini_batches=4,
-        learning_rate=1.0e-3,
-        schedule="adaptive",
-        gamma=0.99,
-        lam=0.95,
-        desired_kl=0.01,
-        max_grad_norm=1.0,
-    )
-
-##
-# MDP settings
-##
-
-@configclass
-class RewardsCfg:
-        # -- task
-    track_lin_vel_xy_exp = RewTerm(
-        func=mdp.track_lin_vel_xy_exp, weight=2.0, params={"command_name": "robot_vel_command", "std": math.sqrt(0.25)}
-    )
-    track_ang_vel_z_exp = RewTerm(
-        func=mdp.track_ang_vel_z_exp, weight=1.0, params={"command_name": "robot_vel_command", "std": math.sqrt(0.25)}
-    )
-    # -- penalties
-    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-2.0)
-    ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
-    dof_torques_l2 = RewTerm(func=mdp.joint_torques_l2, weight=-5.0e-6)
-    dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-5e-8)
-    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
-    feet_air_time = RewTerm(
-        func=mdp.feet_air_time_positive_biped,
-        weight=2.5,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_roll_link"),
-            "command_name": "robot_vel_command",
-            "threshold": 0.3,
-        },
-    )
-    joint_deviation_hip = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.2,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*hip.*"])},
-    )
-    joint_deviation_toes = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.2,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*ankle.*"])},
-    )
-
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-    vlm_actions_gpt = mdp.VLMActionsGPTCfg(
-        asset_name="robot",
-        low_level_decimation=4,
-        low_level_action=mdp.JointPositionActionCfg(
-            asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True
-        ),
-        low_level_policy_file=os.path.join(LOCAL_ISAAC_DIR, "low_level_policy", "h1_policy.jit"),
-    )
-
-
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel)
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "robot_vel_command"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel)
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel)
-        actions = ObsTerm(func=mdp.last_low_level_actions_gpt)
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-        depth_measurement = ObsTerm(
-            func=mdp.process_lidar,
-            params={"sensor_cfg": SceneEntityCfg("lidar_sensor"), "data_type": "distance_to_image_plane"},
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    @configclass
-    class CameraObsCfg(ObsGroup):
-        """Observations for camera group."""
-
-        # observation terms (order preserved)
-        depth_measurement = ObsTerm(
-            func=mdp.isaac_camera_data,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera"), "data_type": "distance_to_image_plane"},
-        )
-        rgb_measurement = ObsTerm(
-            func=mdp.isaac_camera_data,
-            params={"sensor_cfg": SceneEntityCfg("rgb_camera"), "data_type": "rgb"},
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = False
-            self.concatenate_terms = False
-
-    @configclass
-    class PlannerTransformCfg(ObsGroup):
-        cam_position = ObsTerm(
-            func=mdp.cam_position,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-        cam_orientation = ObsTerm(
-            func=mdp.cam_orientation,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-        cam_orientation_ros = ObsTerm(
-            func=mdp.cam_orientation_ros,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-        cam_int_matrix = ObsTerm(
-            func=mdp.cam_int_matrix,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-
-        def __post_init__(self):
-            self.concatenate_terms = False
-            self.enable_corruption = False
-
-
-    # observation groups
-    low_level_policy: PolicyCfg = PolicyCfg()
-    camera_obs: CameraObsCfg = CameraObsCfg()
-    planner_transform: PlannerTransformCfg = PlannerTransformCfg()
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="torso_link"), "threshold": 1.0},
-    )
-    # root_height = DoneTerm(
-    #     func=mdp.root_height_below_minimum,
-    #     params={"asset_cfg": SceneEntityCfg("robot", body_names="pelvis"), "minimum_height": 0.5},
-    # )
-    bad_orientation = DoneTerm(
-        func=mdp.bad_orientation,
-        params={"limit_angle": 0.8},
-    )
-
-
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-    robot_vel_command: mdp.PathFollowerCommandGeneratorGPTCfg = mdp.PathFollowerCommandGeneratorGPTCfg(
-        robot_attr="robot",
-        debug_vis=True,
-    )
-
-
-@configclass
-class TerrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = MatterportImporterCfg(
-        prim_path="/World/matterport",
-        terrain_type="matterport",
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        obj_filepath="/home/yji/Biped/biped_vision/viplanner/assets/matterport/raw_data/v1/scans/17DRP5sb8fy/matterport_mesh/17DRP5sb8fy/matterport_mesh/bed1a77d92d64f5cbbaaae4feed64ec1/matterport.usd",
-        groundplane=True,
-    )
-
-    # robots
-    # robot = ANYMAL_C_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot = UNITREE_H1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_G1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_GO2_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot.init_state.pos = (0.0, 0.0, 1.1)
-    robot.init_state.rot = (0.7, 0.0, 0.0, 0.7)
-
-    # sensors
-    # height_scanner = RayCasterCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-    #     offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.5)),
-    #     attach_yaw_only=True,
-    #     pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-    #     debug_vis=True,
-    #     mesh_prim_paths=["/World/Custom"],
-    # )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    disk_1 = AssetBaseCfg(
-        prim_path="/World/disk_1",
-        spawn=sim_utils.DiskLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=10000.0,
-            radius=30.0,
-        ),
-    )
-    disk_1.init_state.pos = (0, 0, 2.0)
-    # # camera
-    lidar_sensor = MatterportRayCasterCameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-        offset=MatterportRayCasterCameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(0,1.0,0.0,0.0)),
-        data_types=["distance_to_image_plane"],
-        debug_vis=True,
-        pattern_cfg=patterns.PinholeCameraPatternCfg(
-            focal_length=24.0,  #24.0
-            horizontal_aperture=46.0,#87,#20.955,
-            height=15,
-            width=15,
-        ),
-        # max_distance=10,
-        mesh_prim_paths=["/home/yji/Biped/biped_vision/viplanner/assets/matterport/raw_data/v1/scans/17DRP5sb8fy/matterport_mesh/17DRP5sb8fy/matterport_mesh/bed1a77d92d64f5cbbaaae4feed64ec1/matterport.ply"]
-    )
-    depth_camera = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/pelvis/depth_camera",
-        offset=CameraCfg.OffsetCfg(pos=(0.15, 0.0, 0.3), rot=(-0.5, 0.5, -0.5, 0.5)),
-        spawn=sim_utils.PinholeCameraCfg(horizontal_aperture=87.0),
-        width=848,
-        height=480,
-        data_types=["distance_to_image_plane"],
-    )
-
-    rgb_camera = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/pelvis/semantic_camera",
-        offset=CameraCfg.OffsetCfg(pos=(0.15, 0.0, 0.3), rot=(-0.5, 0.5, -0.5, 0.5)),
-        spawn=sim_utils.PinholeCameraCfg(horizontal_aperture=87.0),
-        width=848,
-        height=480,
-        data_types=["rgb"],
-    )
-
-##
-# Environment configuration
-##
-
-@configclass
-class H1MatterportDeployGPTCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # managers
-    terminations: TerminationsCfg = TerminationsCfg()
-    rewards: RewardsCfg = RewardsCfg()
-    scene: TerrainSceneCfg = TerrainSceneCfg(num_envs=1, env_spacing=2.5)
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 4  # 20->10 Hz, 4->50 Hz
-        self.episode_length_s = 200000.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        # self.scene.height_scanner.update_period = 4 * self.sim.dt  # should we low-level decimation
-        self.scene.lidar_sensor.update_period = 4*self.sim.dt
-        self.scene.contact_forces.update_period = self.sim.dt
-        # self.scene.terrain.terrain_type = "plane"
-        # self.scene.terrain.terrain_generator = None
-        # self.rewards.flat_orientation_l2.weight = -5.0
-        # self.rewards.dof_torques_l2.weight = -2.5e-5
-        # self.rewards.feet_air_time.weight = 0.5
-        self.scene.height_scanner = None
-        # self.observations.policy.height_scan = None
-        # self.curriculum.terrain_levels = None
-
-        self.viewer.eye = (5, 12, 5)
-        self.viewer.lookat = (5, 0, 0.0)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/test_h1_low_terrain_generator_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/test_h1_low_terrain_generator_cfg.py
deleted file mode 100644
index 71df035..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/test_h1_low_terrain_generator_cfg.py
+++ /dev/null
@@ -1,603 +0,0 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import math
-from dataclasses import MISSING
-
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import ArticulationCfg, AssetBaseCfg
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import CurriculumTermCfg as CurrTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.lab.terrains import TerrainImporterCfg
-from omni.isaac.lab.utils import configclass
-from omni.isaac.lab.utils.assets import ISAAC_NUCLEUS_DIR, ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.utils.noise import AdditiveUniformNoiseCfg as Unoise
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg, patterns
-from omni.isaac.lab.assets.articulation import ArticulationCfg
-from omni.isaac.lab.actuators import ImplicitActuatorCfg
-from omni.isaac.viplanner.utils import UnRealImporterCfg
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import ActionsCfg, CurriculumCfg, RewardsCfg, EventCfg, LocomotionVelocityRoughEnvCfg
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.config.h1.rough_env_cfg import H1Rewards
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.config.h1.rough_env_cfg import H1RoughEnvCfg as OriH1RoughEnvCfg
-
-##
-# Pre-defined configs
-##
-from omni.isaac.lab.terrains.config.rough import ROUGH_TERRAINS_CFG  # isort: skip
-from omni.isaac.lab_assets import H1_MINIMAL_CFG  # isort: skip
-
-
-##
-# Scene definition
-##
-@configclass
-class MySceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="usd",
-        usd_path="/home/zhaojing/terrain-generator/results/generated_terrain/mesh_0/mesh_new.usd",
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        # visual_material=sim_utils.MdlFileCfg(
-        #     mdl_path=f"{ISAACLAB_NUCLEUS_DIR}/Materials/TilesMarbleSpiderWhiteBrickBondHoned/TilesMarbleSpiderWhiteBrickBondHoned.mdl",
-        #     project_uvw=True,
-        #     texture_scale=(0.25, 0.25),
-        # ),
-        debug_vis=False,
-        # groundplane=False,
-        # axis_up="Z",
-    )
-    # terrain = UnRealImporterCfg(
-    #     prim_path="/World/ground",
-    #     physics_material=sim_utils.RigidBodyMaterialCfg(
-    #         friction_combine_mode="multiply",
-    #         restitution_combine_mode="multiply",
-    #         static_friction=1.0,
-    #         dynamic_friction=1.0,
-    #     ),
-    #     usd_path="/home/zhaojing/terrain-generator/results/generated_terrain/mesh_0/mesh_new.usd",
-    #     groundplane=False,
-    #     # sem_mesh_to_class_map=os.path.join(DATA_DIR, "warehouse", "keyword_mapping.yml"),
-    #     # people_config_file=os.path.join(DATA_DIR, "warehouse", "people_cfg.yml"),
-    #     axis_up="Z",
-    # )
-
-    # robots
-    robot = H1_MINIMAL_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # sensors
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    
-    height_scanner = None
-    # height_scanner = RayCasterCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/base",
-    #     offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 20.0)),
-    #     attach_yaw_only=True,
-    #     pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-    #     debug_vis=False,
-    #     mesh_prim_paths=["/World/ground"],
-    # )
-    # height_scanner.prim_path = "{ENV_REGEX_NS}/Robot/torso_link"
-    # camera
-    lidar_sensor = RayCasterCameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-        # mesh_prim_paths=["/home/zhaojing/Downloads/mesh_static.usd"],
-        mesh_prim_paths=["/World/ground"],
-        update_period=0.1,
-        # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.60, 0.0, 0.0), rot=(-0.5, 0.5, -0.5, 0.5)),
-        # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(0.579, -0.579, 0.406, -0.406)),
-        offset=RayCasterCameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(0,1.0,0.0,0.0)),
-        data_types=["distance_to_image_plane"],
-        debug_vis=True,
-    #     pattern_cfg=patterns.BpearlPatternCfg(
-    #         vertical_ray_angles=[
-    #     89.5, 86.6875, 83.875, 81.0625, 78.25, 75.4375, 72.625, 69.8125, 67.0, 64.1875, 61.375,
-    #     58.5625, 55.75, 52.9375, 50.125, 47.3125, 44.5, 41.6875, 38.875
-    # ]
-    #     ),
-        pattern_cfg=patterns.PinholeCameraPatternCfg(
-            focal_length=24.0,  #24.0
-            horizontal_aperture=46.0,#87,#20.955,
-            height=15,
-            width=15,
-        ),
-        max_distance=10,
-    )
-
-
-##
-# Observations
-##
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel)
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel)
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel)
-        actions = ObsTerm(func=mdp.last_action)
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-        depth_measurement = ObsTerm(
-            func=mdp.process_depth_image,
-            params={"sensor_cfg": SceneEntityCfg("lidar_sensor"), "data_type": "distance_to_image_plane"},
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-
-## 
-# Actions
-##
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    joint_pos = mdp.JointPositionActionCfg(asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True)
-
-##
-# Events (for domain randomization)
-##
-# @configclass
-# class EventCfg:
-#     """Configuration for events."""
-
-#     # startup
-#     physics_material = EventTerm(
-#         func=mdp.randomize_rigid_body_material,
-#         mode="startup",
-#         params={
-#             "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-#             "static_friction_range": (0.8, 0.8),
-#             "dynamic_friction_range": (0.6, 0.6),
-#             "restitution_range": (0.0, 0.0),
-#             "num_buckets": 64,
-#         },
-#     )
-
-#     add_base_mass = EventTerm(
-#         func=mdp.randomize_rigid_body_mass,
-#         mode="startup",
-#         params={"asset_cfg": SceneEntityCfg("robot", body_names="pelvis"), "mass_distribution_params": (-5.0, 5.0), "operation": "add"},
-#     )
-
-#     # reset
-#     base_external_force_torque = EventTerm(
-#         func=mdp.apply_external_force_torque,
-#         mode="reset",
-#         params={
-#             "asset_cfg": SceneEntityCfg("robot", body_names="pelvis"),
-#             "force_range": (0.0, 0.0),
-#             "torque_range": (-0.0, 0.0),
-#         },
-#     )
-
-#     reset_base = EventTerm(
-#         func=mdp.reset_root_state_uniform,
-#         mode="reset",
-#         params={
-#             "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-3.14, 3.14)},
-#             "velocity_range": {
-#                 "x": (-0.5, 0.5),
-#                 "y": (-0.5, 0.5),
-#                 "z": (-0.5, 0.5),
-#                 "roll": (-0.5, 0.5),
-#                 "pitch": (-0.5, 0.5),
-#                 "yaw": (-0.5, 0.5),
-#             },
-#         },
-#     )
-
-#     # reset_base = EventTerm(
-#     #     func=mdp.reset_root_state_from_terrain,
-#     #     mode="reset",
-#     #     params={
-#     #         "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-3.14, 3.14)},
-#     #         "velocity_range": {
-#     #             "x": (-0.5, 0.5),
-#     #             "y": (-0.5, 0.5),
-#     #             "z": (-0.5, 0.5),
-#     #             "roll": (-0.5, 0.5),
-#     #             "pitch": (-0.5, 0.5),
-#     #             "yaw": (-0.5, 0.5),
-#     #         },
-#     #     },
-#     # )
-
-#     reset_robot_joints = EventTerm(
-#         func=mdp.reset_joints_by_scale,
-#         mode="reset",
-#         params={
-#             "position_range": (0.5, 1.5),
-#             "velocity_range": (0.0, 0.0),
-#         },
-#     )
-
-#     # # interval
-#     # push_robot = EventTerm(
-#     #     func=mdp.push_by_setting_velocity,
-#     #     mode="interval",
-#     #     interval_range_s=(10.0, 15.0),
-#     #     params={"velocity_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5)}},
-#     # )
-
-
-##
-# Rewards
-#
-@configclass
-class RewardsCfg2DoF:
-    """
-    Rewards configuration for the 2-DoF H1 (arm fixed) robot.
-    """
-    # -- task
-    termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    track_lin_vel_xy_exp = RewTerm(
-        func=mdp.track_lin_vel_xy_exp, weight=2.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    )
-    track_ang_vel_z_exp = RewTerm(
-        func=mdp.track_ang_vel_z_exp, weight=1.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    )
-    # -- penalties
-    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-2.0)
-    ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
-    dof_torques_l2 = RewTerm(func=mdp.joint_torques_l2, weight=-5.0e-6)
-    dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-5e-8)
-    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
-    feet_air_time = RewTerm(
-        func=mdp.feet_air_time_positive_biped,
-        weight=2.0,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-            "command_name": "base_velocity",
-            "threshold": 0.3,
-        },
-    )
-    feet_slide = RewTerm(
-        func=mdp.feet_slide,
-        weight=-0.1,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*ankle_link"),
-        },
-    )
-    # Penalize ankle joint limits
-    dof_pos_limits = RewTerm(
-        func=mdp.joint_pos_limits, weight=-1.0, params={"asset_cfg": SceneEntityCfg("robot", joint_names=".*_ankle")}
-    )
-    joint_deviation_hip = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.2,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*hip.*"])},
-    )
-    joint_deviation_toes = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.1,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*ankle.*"])},
-    )
-    joint_deviation_arms = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.2,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_shoulder_.*", ".*_elbow"])},
-    )
-    joint_deviation_torso = RewTerm(
-        func=mdp.joint_deviation_l1, weight=-0.1, params={"asset_cfg": SceneEntityCfg("robot", joint_names="torso")}
-    )
-    # termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    # lin_vel_z_l2 = None
-    # track_lin_vel_xy_exp = RewTerm(
-    #     func=mdp.track_lin_vel_xy_yaw_frame_exp,
-    #     weight=1.0,
-    #     params={"command_name": "base_velocity", "std": 0.5},
-    # )
-    # track_ang_vel_z_exp = RewTerm(
-    #     func=mdp.track_ang_vel_z_world_exp, weight=1.0, params={"command_name": "base_velocity", "std": 0.5}
-    # )
-    # feet_air_time = RewTerm(
-    #     func=mdp.feet_air_time_positive_biped,
-    #     weight=0.5,
-    #     params={
-    #         "command_name": "base_velocity",
-    #         "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-    #         "threshold": 0.4,
-    #     },
-    # )
-    # # feet_slide = RewTerm(
-    # #     func=mdp.feet_slide,
-    # #     weight=-0.25,
-    # #     params={
-    # #         "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-    # #         "asset_cfg": SceneEntityCfg("robot", body_names=".*ankle_link"),
-    # #     },
-    # # )
-    # # Penalize ankle joint limits
-    # dof_pos_limits = RewTerm(
-    #     func=mdp.joint_pos_limits, weight=-1.0, params={"asset_cfg": SceneEntityCfg("robot", joint_names=".*_ankle")}
-    # )
-    # # Penalize deviation from default of the joints that are not essential for locomotion
-    # joint_deviation_hip = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.2,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_yaw", ".*_hip_roll"])},
-    # )
-    # joint_deviation_arms = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.2,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_shoulder_.*", ".*_elbow"])},
-    # )
-    # joint_deviation_torso = RewTerm(
-    #     func=mdp.joint_deviation_l1, weight=-0.1, params={"asset_cfg": SceneEntityCfg("robot", joint_names="torso")}
-    # )
-
-
-@configclass
-class RewardsCfg:
-    termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    lin_vel_z_l2 = None
-    track_lin_vel_xy_exp = RewTerm(
-        func=mdp.track_lin_vel_xy_exp,
-        weight=1.0,
-        params={"command_name": "base_velocity", "std": 0.5},
-    )
-    track_ang_vel_z_exp = RewTerm(
-        func=mdp.track_ang_vel_z_exp, weight=1.0, params={"command_name": "base_velocity", "std": 0.5}
-    )
-    feet_air_time = RewTerm(
-        func=mdp.feet_air_time_positive_biped,
-        weight=0.25,
-        params={
-            "command_name": "base_velocity",
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-            "threshold": 0.4,
-        },
-    )
-    feet_slide = RewTerm(
-        func=mdp.feet_slide,
-        weight=-0.25,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*ankle_link"),
-        },
-    )
-    # Penalize ankle joint limits
-    dof_pos_limits = RewTerm(
-        func=mdp.joint_pos_limits, weight=-1.0, params={"asset_cfg": SceneEntityCfg("robot", joint_names=".*_ankle")}
-    )
-    # Penalize deviation from default of the joints that are not essential for locomotion
-    joint_deviation_hip = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.2,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_hip_yaw", ".*_hip_roll"])},
-    )
-    joint_deviation_torso = RewTerm(
-        func=mdp.joint_deviation_l1, weight=-0.1, params={"asset_cfg": SceneEntityCfg("robot", joint_names="torso")}
-    )
-    # -- task
-    # termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    # track_lin_vel_xy_exp = RewTerm(
-    #     func=mdp.track_lin_vel_xy_exp, weight=2.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    # )
-    # track_ang_vel_z_exp = RewTerm(
-    #     func=mdp.track_ang_vel_z_exp, weight=1.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    # )
-    # feet_air_time = RewTerm(
-    #     func=mdp.feet_air_time_positive_biped,
-    #     weight=2.5,
-    #     params={
-    #         "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-    #         "command_name": "base_velocity",
-    #         "threshold": 0.3,
-    #     },
-    # )
-
-    # joint_deviation_hip = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.05,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*hip.*"])},
-    # )
-    # joint_deviation_toes = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.05,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*ankle.*"])},
-    # )
-##
-# Commands
-##
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    base_velocity = mdp.UniformVelocityCommandCfg(
-        asset_name="robot",
-        resampling_time_range=(10.0, 10.0),
-        rel_standing_envs=0.02,
-        rel_heading_envs=1.0,
-        heading_command=True,
-        heading_control_stiffness=0.5,
-        debug_vis=True,
-        ranges=mdp.UniformVelocityCommandCfg.Ranges(
-            lin_vel_x=(-1.0, 1.0), lin_vel_y=(-1.0, 1.0), ang_vel_z=(-1.0, 1.0), heading=(-math.pi, math.pi)
-        ),
-    )
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*torso_link"), "threshold": 1.0},
-    )
-
-
-# @configclass
-# class H1RoughEnvCfg(LocomotionVelocityRoughEnvCfg):
-#     rewards: H1Rewards = H1Rewards()
-#     terminations: TerminationsCfg = TerminationsCfg()
-#     scene = MySceneCfg(num_envs=4096, env_spacing=2.5)
-#     observations: ObservationsCfg = ObservationsCfg()
-
-#     def __post_init__(self):
-#         # post init of parent
-#         super().__post_init__()
-#         # Scene
-#         self.scene.robot = H1_MINIMAL_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-#         # self.scene.height_scanner.prim_path = "{ENV_REGEX_NS}/Robot/torso_link"
-
-#         # Randomization
-#         self.events.push_robot = None
-#         self.events.add_base_mass = None
-#         self.events.reset_robot_joints.params["position_range"] = (1.0, 1.0)
-#         self.events.base_external_force_torque.params["asset_cfg"].body_names = [".*torso_link"]
-#         self.events.reset_base.params = {
-#             "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-#             "velocity_range": {
-#                 "x": (0.0, 0.0),
-#                 "y": (0.0, 0.0),
-#                 "z": (0.0, 0.0),
-#                 "roll": (0.0, 0.0),
-#                 "pitch": (0.0, 0.0),
-#                 "yaw": (0.0, 0.0),
-#             },
-#         }
-
-#         # Terminations
-#         self.terminations.base_contact.params["sensor_cfg"].body_names = [".*torso_link"]
-
-#         # Rewards
-#         self.rewards.undesired_contacts = None
-#         self.rewards.flat_orientation_l2.weight = -1.0
-#         self.rewards.dof_torques_l2.weight = 0.0
-#         self.rewards.action_rate_l2.weight = -0.005
-#         self.rewards.dof_acc_l2.weight = -1.25e-7
-
-#         # Commands
-#         self.commands.base_velocity.ranges.lin_vel_x = (0.0, 1.0)
-#         self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-#         self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-
-@configclass
-class H1TerrainGeneratorCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Scene settings
-    scene: MySceneCfg = MySceneCfg(num_envs=4096, env_spacing=5.0)
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # MDP settings
-    # rewards: RewardsCfg = RewardsCfg2DoF()
-    rewards: RewardsCfg = H1Rewards()
-    terminations: TerminationsCfg = TerminationsCfg()
-    events: EventCfg = EventCfg()
-    curriculum: CurriculumCfg = CurriculumCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 4
-        self.episode_length_s = 20.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        # self.sim.physics_material = self.scene.terrain.physics_material
-        # self.sim.physics_material.static_friction = 1.0
-        # self.sim.physics_material.dynamic_friction = 1.0
-        # self.sim.physics_material.friction_combine_mode = "max"
-        # self.sim.physics_material.restitution_combine_mode = "max"
-
-        # Randomization
-        self.events.push_robot = None
-        self.events.add_base_mass = None
-        self.events.reset_robot_joints.params["position_range"] = (1.0, 1.0)
-        self.events.base_external_force_torque.params["asset_cfg"].body_names = [".*torso_link"]
-        self.events.reset_base.params = {
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (0.0, 0.0),
-                "y": (0.0, 0.0),
-                "z": (0.0, 0.0),
-                "roll": (0.0, 0.0),
-                "pitch": (0.0, 0.0),
-                "yaw": (0.0, 0.0),
-            },
-        }
-
-        # Rewards
-        self.rewards.undesired_contacts = None
-        self.rewards.flat_orientation_l2.weight = -1.0
-        self.rewards.dof_torques_l2.weight = 0.0
-        self.rewards.action_rate_l2.weight = -0.005
-        self.rewards.dof_acc_l2.weight = -1.25e-7
-        self.rewards.feet_air_time.weight=0.25
-
-        # Commands
-        self.commands.base_velocity.ranges.lin_vel_x = (0.0, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        # self.scene.lidar_sensor = None
-        if self.scene.lidar_sensor is not None:
-            self.scene.lidar_sensor.update_period = 4 * self.sim.dt
-        self.scene.contact_forces.update_period = self.sim.dt
-        # if self.scene.height_scanner is not None:
-        #     self.scene.height_scanner.update_period = self.decimation * self.sim.dt
-        if self.scene.contact_forces is not None:
-            self.scene.contact_forces.update_period = self.sim.dt
-
-        # check if terrain levels curriculum is enabled - if so, enable curriculum for terrain generator
-        # this generates terrains with increasing difficulty and is useful for training
-        # if getattr(self.curriculum, "terrain_levels", None) is not None:
-        #     if self.scene.terrain.terrain_generator is not None:
-        #         self.scene.terrain.terrain_generator.curriculum = True
-        # else:
-        #     if self.scene.terrain.terrain_generator is not None:
-        #         self.scene.terrain.terrain_generator.curriculum = False
-        self.curriculum.terrain_levels = None
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/test_h1_matterport_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/test_h1_matterport_cfg.py
deleted file mode 100644
index 45685ce..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/test_h1_matterport_cfg.py
+++ /dev/null
@@ -1,401 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import CurriculumTermCfg as CurrTerm
-# from omni.isaac.lab.managers import RandomizationTermCfg as RandTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.utils import configclass
-# from omni.isaac.lab.utils.assets import LOCAL_ISAAC_DIR
-from omni.isaac.lab.utils.assets import ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg
-import omni.isaac.lab.terrains as terrain_gen
-import omni.isaac.lab.terrains.height_field as hf_gen
-from omni.isaac.matterport.config import MatterportImporterCfg
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import AssetBaseCfg
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import CameraCfg, ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.viplanner.utils import UnRealImporterCfg, MatterportRayCasterCameraCfg
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg, patterns
-# from .h1_low_cfg import EventCfg
-# from omni.isaac.lab.terrains.config.rough import ROUGH_TERRAINS_CFG
-
-
-from omni.isaac.lab_assets import H1_MINIMAL_CFG  # isort: skip
-
-import math
-
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-
-
-ROUGH_TERRAINS_CFG = TerrainGeneratorCfg(
-    size=(8.0, 8.0),
-    border_width=20.0,
-    num_rows=10,
-    num_cols=20,
-    horizontal_scale=0.1,
-    vertical_scale=0.005,
-    slope_threshold=0.75,
-    use_cache=False,
-    sub_terrains={
-        "pyramid_stairs": terrain_gen.MeshPyramidStairsTerrainCfg(
-            proportion=0.2,
-            step_height_range=(0.1, 0.3),
-            step_width=0.3,
-            platform_width=3.0,
-            border_width=1.0,
-            holes=False,
-        ),
-        "pyramid_stairs_inv": terrain_gen.MeshInvertedPyramidStairsTerrainCfg(
-            proportion=0.2,
-            step_height_range=(0.1, 0.3),
-            step_width=0.3,
-            platform_width=3.0,
-            border_width=1.0,
-            holes=False,
-        ),
-        "boxes": terrain_gen.MeshRandomGridTerrainCfg(
-            proportion=0.2, grid_width=0.45, grid_height_range=(0.05, 0.2), platform_width=2.0
-        ),
-        "random_rough": terrain_gen.HfRandomUniformTerrainCfg(
-            proportion=0.2, noise_range=(0.02, 0.10), noise_step=0.02, border_width=0.25
-        ),
-        "hf_pyramid_slope": terrain_gen.HfPyramidSlopedTerrainCfg(
-            proportion=0.05, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25
-        ),
-        "hf_pyramid_slope_inv": terrain_gen.HfInvertedPyramidSlopedTerrainCfg(
-            proportion=0.05, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25
-        ),
-        "obstacles": hf_gen.HfDiscreteObstaclesTerrainCfg(
-            proportion=0.1,
-            size=(8.0, 8.0),
-            horizontal_scale=0.1,
-            vertical_scale=0.005,
-            border_width=0.0,
-            num_obstacles=5,
-            obstacle_height_mode="fixed",
-            obstacle_width_range=(0.25, 0.75),
-            obstacle_height_range=(1.0, 2.0),
-            platform_width=1.5,
-    )
-    },
-)
-"""Rough terrains configuration."""
-
-
-
-##
-# MDP settings
-##
-
-@configclass
-class RewardsCfg:
-    # -- task
-    termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    track_lin_vel_xy_exp = RewTerm(
-        func=mdp.track_lin_vel_xy_exp, weight=2.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    )
-    track_ang_vel_z_exp = RewTerm(
-        func=mdp.track_ang_vel_z_exp, weight=1.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    )
-    # -- penalties
-    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-2.0)
-    ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
-    dof_torques_l2 = RewTerm(func=mdp.joint_torques_l2, weight=-5.0e-6)
-    dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-5e-8)
-    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
-    feet_air_time = RewTerm(
-        func=mdp.feet_air_time_positive_biped,
-        weight=2.0,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-            "command_name": "base_velocity",
-            "threshold": 0.3,
-        },
-    )
-    feet_slide = RewTerm(
-        func=mdp.feet_slide,
-        weight=-0.1,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*ankle_link"),
-        },
-    )
-    # Penalize ankle joint limits
-    dof_pos_limits = RewTerm(
-        func=mdp.joint_pos_limits, weight=-1.0, params={"asset_cfg": SceneEntityCfg("robot", joint_names=".*_ankle")}
-    )
-    joint_deviation_hip = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.2,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*hip.*"])},
-    )
-    joint_deviation_toes = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.1,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*ankle.*"])},
-    )
-    joint_deviation_arms = RewTerm(
-        func=mdp.joint_deviation_l1,
-        weight=-0.2,
-        params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_shoulder_.*", ".*_elbow"])},
-    )
-    joint_deviation_torso = RewTerm(
-        func=mdp.joint_deviation_l1, weight=-0.1, params={"asset_cfg": SceneEntityCfg("robot", joint_names="torso")}
-    )
-
-
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    joint_pos = mdp.JointPositionActionCfg(asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True)
-
-
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel)
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel)
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel)
-        actions = ObsTerm(func=mdp.last_action)
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-        depth_measurement = ObsTerm(
-            func=mdp.process_lidar,
-            params={"sensor_cfg": SceneEntityCfg("lidar_sensor"), "data_type": "distance_to_image_plane"},
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    # @configclass
-    # class CameraObsCfg(ObsGroup):
-    #     """Observations for camera group."""
-
-    #     # observation terms (order preserved)
-    #     # depth_measurement = ObsTerm(
-    #     #     func=mdp.process_depth_image,
-    #     #     params={"sensor_cfg": SceneEntityCfg("lidar_sensor"), "data_type": "distance_to_image_plane"},
-    #     # )
-    #     rgb_measurement = ObsTerm(
-    #         func=mdp.isaac_camera_data,
-    #         params={"sensor_cfg": SceneEntityCfg("rgb_camera"), "data_type": "rgb"},
-    #     )
-
-    #     def __post_init__(self):
-    #         self.enable_corruption = False
-    #         self.concatenate_terms = False
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-    # camera_obs: CameraObsCfg = CameraObsCfg()
-
-@configclass
-class CurriculumCfg:
-    """Curriculum terms for the MDP."""
-
-    terrain_levels = CurrTerm(func=mdp.terrain_levels_vel)
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="torso_link"), "threshold": 1.0},
-    )
-    # root_height = DoneTerm(
-    #     func=mdp.root_height_below_minimum,
-    #     params={"asset_cfg": SceneEntityCfg("robot", body_names="pelvis"), "minimum_height": 0.5},
-    # )
-    bad_orientation = DoneTerm(
-        func=mdp.bad_orientation,
-        params={"limit_angle": 0.8},
-    )
-
-
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    base_velocity = mdp.UniformVelocityCommandCfg(
-        asset_name="robot",
-        resampling_time_range=(10.0, 10.0),
-        rel_standing_envs=0.02,
-        rel_heading_envs=1.0,
-        heading_command=True,
-        heading_control_stiffness=0.5,
-        debug_vis=True,
-        ranges=mdp.UniformVelocityCommandCfg.Ranges(
-            lin_vel_x=(-1.0, 1.0), lin_vel_y=(-1.0, 1.0), ang_vel_z=(-1.0, 1.0), heading=(-math.pi, math.pi)
-        ),
-    )
-
-
-
-@configclass
-class TerrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = MatterportImporterCfg(
-        prim_path="/World/matterport",
-        terrain_type="matterport",
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        # obj_filepath="/home/zhaojing/h1-training-isaaclab/assets/matterport/29hnd4uzFmX/29hnd4uzFmX/matterport_mesh/04eb2788768d40a38d35d876a02e9624/04eb2788768d40a38d35d876a02e9624.usd",
-        obj_filepath="/home/zhaojing/h1-training-isaaclab/assets/matterport/5q7pvUzZiYa/matterport_mesh/d7a2911178dd48e89d6a23afb09cbc11/d7a2911178dd48e89d6a23afb09cbc11.usd",
-        groundplane=False,
-    )
-
-    # robots
-    # robot = ANYMAL_C_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot = H1_MINIMAL_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_G1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_GO2_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot.init_state.pos = (9.1, 3.8, 1.1)
-    robot.init_state.rot = (0.7, 0.0, 0.0, 0.)
-
-    # sensors
-    # height_scanner = RayCasterCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-    #     offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.5)),
-    #     attach_yaw_only=True,
-    #     pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-    #     debug_vis=True,
-    #     mesh_prim_paths=["/World/Custom"],
-    # )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    disk_1 = AssetBaseCfg(
-        prim_path="/World/disk_1",
-        spawn=sim_utils.DiskLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=10000.0,
-            radius=30.0,
-        ),
-    )
-    disk_1.init_state.pos = (0, 0, 2.0)
-    # # camera
-    lidar_sensor = MatterportRayCasterCameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-        # mesh_prim_paths=["/home/yji/Biped/biped_vision/viplanner/assets/bowl_toy_origin.ply"],
-        # mesh_prim_paths=["/World/ground"],
-        # update_period=0.1,
-        # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.60, 0.0, 0.0), rot=(-0.5, 0.5, -0.5, 0.5)),
-        # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(0.579, -0.579, 0.406, -0.406)),
-        offset=MatterportRayCasterCameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(0,1.0,0.0,0.0)),
-        data_types=["distance_to_image_plane"],
-        debug_vis=True,
-        pattern_cfg=patterns.PinholeCameraPatternCfg(
-            focal_length=24.0,  #24.0
-            horizontal_aperture=46.0,#87,#20.955,
-            height=15,
-            width=15,
-        ),
-        # max_distance=10,
-        # mesh_prim_paths=["/home/zhaojing/h1-training-isaaclab/assets/matterport/29hnd4uzFmX/29hnd4uzFmX/poisson_meshes/29hnd4uzFmX.ply"]
-        mesh_prim_paths=["/home/zhaojing/h1-training-isaaclab/assets/matterport/5q7pvUzZiYa/poisson_meshes/5q7pvUzZiYa.ply"]
-    )
-    # rgb_camera = CameraCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/pelvis/semantic_camera",
-    #     offset=CameraCfg.OffsetCfg(pos=(0.15, 0.0, 0.3), rot=(-0.5, 0.5, -0.5, 0.5)),
-    #     spawn=sim_utils.PinholeCameraCfg(horizontal_aperture=87.0),
-    #     width=848,
-    #     height=480,
-    #     data_types=["rgb"],
-    # )
-
-##
-# Environment configuration
-##
-
-@configclass
-class TestH1MatterportCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # managers
-    terminations: TerminationsCfg = TerminationsCfg()
-    # events: EventCfg = EventCfg()
-    rewards: RewardsCfg = RewardsCfg()
-    scene: TerrainSceneCfg = TerrainSceneCfg(num_envs=4096, env_spacing=2.5)
-    curriculum: CurriculumCfg = CurriculumCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 4  # 20->10 Hz, 4->50 Hz
-        self.episode_length_s = 200000.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        # self.scene.height_scanner.update_period = 4 * self.sim.dt  # should we low-level decimation
-        self.scene.lidar_sensor.update_period = 4*self.sim.dt
-        self.scene.contact_forces.update_period = self.sim.dt
-        # self.scene.terrain.terrain_type = "plane"
-        # self.scene.terrain.terrain_generator = None
-        # self.rewards.flat_orientation_l2.weight = -5.0
-        # self.rewards.dof_torques_l2.weight = -2.5e-5
-        # self.rewards.feet_air_time.weight = 0.5
-        self.scene.height_scanner = None
-        self.observations.policy.height_scan = None
-        self.curriculum.terrain_levels = None
-
-        self.commands.base_velocity.ranges.lin_vel_x = (0.0, 1.0)
-        self.commands.base_velocity.ranges.lin_vel_y = (0.0, 0.0)
-        self.commands.base_velocity.ranges.ang_vel_z = (-1.0, 1.0)
-
-        self.viewer.eye = (5, 12, 5)
-        self.viewer.lookat = (5, 0, 0.0)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/train_h1_matterport_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/train_h1_matterport_cfg.py
deleted file mode 100644
index 558c36b..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/h1/train_h1_matterport_cfg.py
+++ /dev/null
@@ -1,411 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import CurriculumTermCfg as CurrTerm
-# from omni.isaac.lab.managers import RandomizationTermCfg as RandTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.utils import configclass
-# from omni.isaac.lab.utils.assets import LOCAL_ISAAC_DIR
-from omni.isaac.lab.utils.assets import ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg
-import omni.isaac.lab.terrains as terrain_gen
-import omni.isaac.lab.terrains.height_field as hf_gen
-from omni.isaac.matterport.config import MatterportImporterCfg
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import AssetBaseCfg
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import CameraCfg, ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.viplanner.utils import UnRealImporterCfg, MatterportRayCasterCameraCfg
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg, patterns
-# from omni.isaac.lab.terrains.config.rough import ROUGH_TERRAINS_CFG
-
-
-from omni.isaac.lab_assets import H1_MINIMAL_CFG  # isort: skip
-
-import math
-
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-
-@configclass
-class H1RoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 24
-    max_iterations = 30000
-    save_interval = 50
-    experiment_name = "h1_rough"
-    empirical_normalization = False
-    policy = RslRlPpoActorCriticCfg(
-        init_noise_std=1.0,
-        actor_hidden_dims=[512, 256, 128],
-        critic_hidden_dims=[512, 256, 128],
-        # actor_hidden_dims=[128, 128, 128],
-        # critic_hidden_dims=[128, 128, 128],
-        activation="elu",
-    )
-    algorithm = RslRlPpoAlgorithmCfg(
-        value_loss_coef=1.0,
-        use_clipped_value_loss=True,
-        clip_param=0.2,
-        entropy_coef=0.005,
-        num_learning_epochs=5,
-        num_mini_batches=4,
-        learning_rate=1.0e-3,
-        schedule="adaptive",
-        gamma=0.99,
-        lam=0.95,
-        desired_kl=0.01,
-        max_grad_norm=1.0,
-    )
-
-##
-# MDP settings
-##
-
-@configclass
-class RewardsCfg:
-    # -- task
-    termination_penalty = RewTerm(func=mdp.is_terminated, weight=-200.0)
-    #  TODO: add the navigation reward
-    # track_lin_vel_xy_exp = RewTerm(
-    #         func=mdp.track_lin_vel_xy_exp, weight=2.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    #     )
-    # track_ang_vel_z_exp = RewTerm(
-    #     func=mdp.track_ang_vel_z_exp, weight=1.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    # )
-
-    # # -- penalties
-    # lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-2.0)
-    # ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
-    # dof_torques_l2 = RewTerm(func=mdp.joint_torques_l2, weight=-5.0e-6)
-    # dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-5e-8)
-    # action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
-    # feet_air_time = RewTerm(
-    #     func=mdp.feet_air_time_positive_biped,
-    #     weight=2.5,
-    #     params={
-    #         "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_link"),
-    #         "command_name": "base_velocity",
-    #         "threshold": 0.3,
-    #     },
-    # )
-    # joint_deviation_hip = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.1,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*hip.*"])},
-    # )
-    # joint_deviation_toes = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.01,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*ankle.*"])},
-    # )
-    # joint_deviation_arms = RewTerm(
-    #     func=mdp.joint_deviation_l1,
-    #     weight=-0.1,
-    #     params={"asset_cfg": SceneEntityCfg("robot", joint_names=[".*_shoulder_.*", ".*_elbow"])},
-    # )
-    # joint_deviation_torso = RewTerm(
-    #     func=mdp.joint_deviation_l1, weight=-0.1, params={"asset_cfg": SceneEntityCfg("robot", joint_names="torso")}
-    # )
-    # # -- optional penalties
-    # flat_orientation_l2 = RewTerm(func=mdp.flat_orientation_l2, weight=-1.0)
-
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    joint_pos = mdp.JointPositionActionCfg(asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True)
-
-
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel)
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "robot_vel_command"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel)
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel)
-        actions = ObsTerm(func=mdp.last_low_level_actions_llava)
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-        depth_measurement = ObsTerm(
-            func=mdp.process_lidar,
-            params={"sensor_cfg": SceneEntityCfg("lidar_sensor"), "data_type": "distance_to_image_plane"},
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel)
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel)
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel)
-        actions = ObsTerm(func=mdp.last_action)
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-        depth_measurement = ObsTerm(
-            func=mdp.process_lidar,
-            params={"sensor_cfg": SceneEntityCfg("lidar_sensor"), "data_type": "distance_to_image_plane"},
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    # @configclass
-    # class CameraObsCfg(ObsGroup):
-    #     """Observations for camera group."""
-
-    #     # observation terms (order preserved)
-    #     # depth_measurement = ObsTerm(
-    #     #     func=mdp.process_depth_image,
-    #     #     params={"sensor_cfg": SceneEntityCfg("lidar_sensor"), "data_type": "distance_to_image_plane"},
-    #     # )
-    #     rgb_measurement = ObsTerm(
-    #         func=mdp.isaac_camera_data,
-    #         params={"sensor_cfg": SceneEntityCfg("rgb_camera"), "data_type": "rgb"},
-    #     )
-
-    #     def __post_init__(self):
-    #         self.enable_corruption = False
-    #         self.concatenate_terms = False
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="torso_link"), "threshold": 1.0},
-    )
-    # root_height = DoneTerm(
-    #     func=mdp.root_height_below_minimum,
-    #     params={"asset_cfg": SceneEntityCfg("robot", body_names="pelvis"), "minimum_height": 0.5},
-    # )
-    # bad_orientation = DoneTerm(
-    #     func=mdp.bad_orientation,
-    #     params={"limit_angle": 0.8},
-    # )
-    # feet_contact = DoneTerm(
-    #     func=mdp.long_feet_contact,
-    #     params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*ankle_roll_link"), "threshold": 2.0},
-    # )
-
-
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    base_velocity = mdp.UniformVelocityCommandCfg(
-        asset_name="robot",
-        resampling_time_range=(10.0, 10.0),
-        rel_standing_envs=0.02,
-        rel_heading_envs=1.0,
-        heading_command=False,
-        heading_control_stiffness=0.5,
-        debug_vis=True,
-        ranges=mdp.UniformVelocityCommandCfg.Ranges(
-            lin_vel_x=(-1.0, 1.0), lin_vel_y=(-1.0, 1.0), ang_vel_z=(-1.0, 1.0), heading=(-math.pi, math.pi)
-        ),
-    )
-
-
-@configclass
-class TerrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = MatterportImporterCfg(
-        prim_path="/World/matterport",
-        terrain_type="matterport",
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        obj_filepath="/home/zhaojing/h1-training-isaaclab/assets/matterport/5q7pvUzZiYa/matterport_mesh/d7a2911178dd48e89d6a23afb09cbc11/d7a2911178dd48e89d6a23afb09cbc11.usd",
-        groundplane=False,
-    )
-
-    # robots
-    # robot = ANYMAL_C_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot = H1_MINIMAL_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_G1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_GO2_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot.init_state.pos = (0.0, 0.0, 1.1)
-    robot.init_state.rot = (0.7, 0.0, 0.0, 0.7)
-    # robot.init_state.rot = (1.0, 0.0, 0.0, 0.0)
-
-    # sensors
-    # height_scanner = RayCasterCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-    #     offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.5)),
-    #     attach_yaw_only=True,
-    #     pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-    #     debug_vis=True,
-    #     mesh_prim_paths=["/World/Custom"],
-    # )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    disk_1 = AssetBaseCfg(
-        prim_path="/World/disk_1",
-        spawn=sim_utils.DiskLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=10000.0,
-            radius=30.0,
-        ),
-    )
-    disk_1.init_state.pos = (0, 0, 2.0)
-    disk_2 = AssetBaseCfg(
-        prim_path="/World/disk_2",
-        spawn=sim_utils.DiskLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=10000.0,
-            radius=30.0,
-        ),
-    )
-    disk_2.init_state.pos = (0, 0, 4.0)
-    disk_3 = AssetBaseCfg(
-        prim_path="/World/disk_3",
-        spawn=sim_utils.DiskLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=10000.0,
-            radius=30.0,
-        ),
-    )
-    disk_3.init_state.pos = (0, 0, 6.0)
-    disk_4 = AssetBaseCfg(
-        prim_path="/World/disk_4",
-        spawn=sim_utils.DiskLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=10000.0,
-            radius=30.0,
-        ),
-    )
-    disk_4.init_state.pos = (0, 0, -0.5)
-    # # camera
-    
-    # rgb_camera = CameraCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/pelvis/rgb_camera",
-    #     offset=CameraCfg.OffsetCfg(pos=(0.15, 0.0, -0.1), rot=(-0.5, 0.5, -0.5, 0.5)),
-    #     spawn=sim_utils.PinholeCameraCfg(horizontal_aperture=87.0),
-    #     width=848,
-    #     height=480,
-    #     data_types=["rgb"],
-    # )
-    lidar_sensor = MatterportRayCasterCameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/pelvis",
-        offset=MatterportRayCasterCameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(0,1.0,0.0,0.0)),
-        data_types=["distance_to_image_plane"],
-        debug_vis=False,
-        pattern_cfg=patterns.PinholeCameraPatternCfg(
-            focal_length=24.0,  #24.0
-            horizontal_aperture=20.0,#87,#20.955,
-            height=15,
-            width=15,
-        ),
-        # max_distance=10,
-        mesh_prim_paths=["/home/zhaojing/h1-training-isaaclab/assets/matterport/5q7pvUzZiYa/poisson_meshes/5q7pvUzZiYa.ply"]
-    )
-    
-
-##
-# Environment configuration
-##
-
-@configclass
-class H1MatterportCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # managers
-    terminations: TerminationsCfg = TerminationsCfg()
-    rewards: RewardsCfg = RewardsCfg()
-    scene: TerrainSceneCfg = TerrainSceneCfg(num_envs=16, env_spacing=2.5)
-    
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 20  # 20->10 Hz, 4->50 Hz
-        self.episode_length_s = 200000.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        # self.scene.height_scanner.update_period = 4 * self.sim.dt  # should we low-level decimation
-        self.scene.lidar_sensor.update_period = 4*self.sim.dt
-        # self.scene.rgb_camera.update_period = 4*self.sim.dt
-        self.scene.contact_forces.update_period = self.sim.dt
-        # self.scene.terrain.terrain_type = "plane"
-        # self.scene.terrain.terrain_generator = None
-        # self.rewards.flat_orientation_l2.weight = -5.0
-        # self.rewards.dof_torques_l2.weight = -2.5e-5
-        # self.rewards.feet_air_time.weight = 0.5
-        self.scene.height_scanner = None
-        # self.observations.policy.height_scan = None
-        # self.curriculum.terrain_levels = None
-
-        self.viewer.eye = (5, 12, 5)
-        self.viewer.lookat = (5, 0, 0.0)
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/matterport_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/matterport_cfg.py
deleted file mode 100644
index 79d09d5..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/matterport_cfg.py
+++ /dev/null
@@ -1,178 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import omni.isaac.lab.sim as sim_utils
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.matterport.config import MatterportImporterCfg
-from omni.isaac.matterport.domains import MatterportRayCasterCfg
-from omni.isaac.lab.assets import AssetBaseCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import ContactSensorCfg, patterns
-from omni.isaac.lab.utils import configclass
-from omni.isaac.viplanner.utils import VIPlannerMatterportRayCasterCameraCfg
-
-from .base_cfg import ObservationsCfg, ViPlannerBaseCfg
-
-##
-# Pre-defined configs
-##
-# isort: off
-from omni.isaac.lab_assets.anymal import ANYMAL_C_CFG
-
-##
-# Scene definition
-##
-
-
-@configclass
-class TerrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = MatterportImporterCfg(
-        prim_path="/World/matterport",
-        terrain_type="matterport",
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        obj_filepath="/home/yji/Biped/biped_vision/viplanner/assets/matterport/raw_data/v1/scans/17DRP5sb8fy/matterport_mesh/17DRP5sb8fy/matterport_mesh/bed1a77d92d64f5cbbaaae4feed64ec1/matterport.usd",
-        groundplane=True,
-    )
-    # robots
-    robot = ANYMAL_C_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot.init_state.pos = (8.0, -0.5, 0.6)
-    robot.init_state.rot = (0.6126, 0.0327, 0.0136, -0.7896)
-
-    # sensors
-    height_scanner = MatterportRayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        offset=MatterportRayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.5)),
-        attach_yaw_only=True,
-        pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-        debug_vis=True,
-        mesh_prim_paths=["${USER_PATH_TO_USD}/matterport.ply"],
-    )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    sphere_1 = AssetBaseCfg(
-        prim_path="/World/sphere_1",
-        spawn=sim_utils.SphereLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=3000.0,
-        ),
-    )
-    sphere_1.init_state.pos = (8, 1, 2.0)
-    sphere_2 = AssetBaseCfg(
-        prim_path="/World/sphere_2",
-        spawn=sim_utils.SphereLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=30000.0,
-        ),
-    )
-    sphere_2.init_state.pos = (10.5, -5.5, 2.0)
-    sphere_3 = AssetBaseCfg(
-        prim_path="/World/sphere_3",
-        spawn=sim_utils.SphereLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=30000.0,
-        ),
-    )
-    sphere_3.init_state.pos = (6.0, -5.5, 2.0)
-    sphere_4 = AssetBaseCfg(
-        prim_path="/World/sphere_4",
-        spawn=sim_utils.SphereLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=30000.0,
-        ),
-    )
-    sphere_4.init_state.pos = (8.0, -12, 2.0)
-    # camera
-    depth_camera = VIPlannerMatterportRayCasterCameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        offset=VIPlannerMatterportRayCasterCameraCfg.OffsetCfg(
-            pos=(0.510, 0.0, 0.015), rot=(-0.5, 0.5, -0.5, 0.5)
-        ),  # FIXME: currently in ROS convention
-        pattern_cfg=patterns.PinholeCameraPatternCfg(
-            width=848,
-            height=480,
-            # intrinsics=(430.31607, 0.0, 428.28408, 0.0, 430.31607, 244.00695, 0.0, 0.0, 1.0),  # FIXME: intrinsics not supported yet
-        ),
-        debug_vis=False,
-        max_distance=10,
-        mesh_prim_paths=["${USER_PATH_TO_USD}/matterport.ply"],
-        data_types=["distance_to_image_plane"],
-    )
-    semantic_camera = VIPlannerMatterportRayCasterCameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        offset=VIPlannerMatterportRayCasterCameraCfg.OffsetCfg(
-            pos=(0.510, 0.0, 0.015), rot=(-0.5, 0.5, -0.5, 0.5)
-        ),  # FIXME: currently in ROS convention
-        pattern_cfg=patterns.PinholeCameraPatternCfg(
-            width=1280,
-            height=720,
-            # intrinsics=(644.15496, 0.0, 639.53125, 0.0, 643.49212, 366.30880, 0.0, 0.0, 1.0),  # FIXME: intrinsics not supported yet
-        ),
-        data_types=["semantic_segmentation"],
-        debug_vis=False,
-        mesh_prim_paths=["${USER_PATH_TO_USD}/matterport.ply"],
-    )
-
-
-@configclass
-class MatterportObservationsCfg(ObservationsCfg):
-    """Observations for locomotion and planner with adjustments for Matterport Environments"""
-
-    @configclass
-    class MatterportPlannerImageCfg(ObsGroup):
-        depth_measurement = ObsTerm(
-            func=mdp.matterport_raycast_camera_data,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera"), "data_type": "distance_to_image_plane"},
-        )
-        semantic_measurement = ObsTerm(
-            func=mdp.matterport_raycast_camera_data,
-            params={"sensor_cfg": SceneEntityCfg("semantic_camera"), "data_type": "semantic_segmentation"},
-        )
-
-        def __post_init__(self):
-            self.concatenate_terms = False
-            self.enable_corruption = False
-
-    planner_image: MatterportPlannerImageCfg = MatterportPlannerImageCfg()
-
-
-##
-# Environment configuration
-##
-
-
-@configclass
-class ViPlannerMatterportCfg(ViPlannerBaseCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Scene settings
-    scene: TerrainSceneCfg = TerrainSceneCfg(num_envs=1, env_spacing=1.0)
-    # adjust image observations
-    observations: MatterportObservationsCfg = MatterportObservationsCfg()
-
-    def __post_init__(self):
-        """Post initialization."""
-        super().__post_init__()
-        # adapt viewer
-        self.viewer.eye = (8.5, 3.0, 2.5)
-        self.viewer.lookat = (8.5, -4.0, 0.0)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/test_obj_nav_matterport_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/test_obj_nav_matterport_cfg.py
deleted file mode 100644
index 5f39735..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/test_obj_nav_matterport_cfg.py
+++ /dev/null
@@ -1,330 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.utils import configclass
-from omni.isaac.lab.utils.assets import LOCAL_ISAAC_DIR
-from omni.isaac.lab.utils.assets import ISAAC_lab_NUCLEUS_DIR
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg, FlatPatchSamplingCfg
-import omni.isaac.lab.terrains as terrain_gen
-from omni.isaac.matterport.config import MatterportImporterCfg
-
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import AssetBaseCfg
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import CameraCfg, ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg, patterns
-from omni.isaac.matterport.domains import MatterportRayCasterCfg, MatterportRayCasterCamera, MatterportRayCasterCfg
-from omni.isaac.viplanner.utils import UnRealImporterCfg, MatterportRayCasterCameraCfg
-# from omni.isaac.lab.terrains.config.rough import ROUGH_TERRAINS_CFG
-
-
-from omni.isaac.lab_assets.anymal import ANYMAL_C_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_H1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_G1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_GO2_CFG
-
-##
-# Scene definition
-##
-
-
-@configclass
-class TerrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = MatterportImporterCfg(
-        prim_path="/World/matterport",
-        terrain_type="matterport",
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        obj_filepath="/home/yji/Biped/biped_vision/viplanner/assets/matterport/raw_data/v1/scans/17DRP5sb8fy/matterport_mesh/17DRP5sb8fy/matterport_mesh/bed1a77d92d64f5cbbaaae4feed64ec1/matterport.usd",
-        groundplane=True,
-    )
-    # robots
-    robot = ANYMAL_C_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot.init_state.pos = (8.0, -0.5, 0.6)
-    robot.init_state.pos = (0.0, -0.0, 0.6)
-    robot.init_state.rot = (0.6126, 0.0327, 0.0136, -0.7896)
-
-    # # sensors
-    # height_scanner = MatterportRayCasterCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/base",
-    #     offset=MatterportRayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.5)),
-    #     attach_yaw_only=True,
-    #     pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-    #     debug_vis=True,
-    #     mesh_prim_paths=["${USER_PATH_TO_USD}/matterport.ply"],
-    # )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    disk_1 = AssetBaseCfg(
-        prim_path="/World/disk_1",
-        spawn=sim_utils.DiskLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=10000.0,
-            radius=30.0,
-        ),
-    )
-    disk_1.init_state.pos = (0, 0, 2.0)
-
-    # camera
-    depth_camera = MatterportRayCasterCameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        offset=MatterportRayCasterCameraCfg.OffsetCfg(pos=(0.60, 0.0, 0.0), rot=(-0.5, 0.5, -0.5, 0.5)),
-        pattern_cfg=patterns.PinholeCameraPatternCfg(
-            focal_length=24.0,
-            horizontal_aperture=46,
-            height=15,
-            width=25,
-        ),
-        debug_vis=False,
-        # max_distance=10,
-        mesh_prim_paths=["/home/yji/Biped/biped_vision/viplanner/assets/matterport/raw_data/v1/scans/17DRP5sb8fy/matterport_mesh/matterport.ply"],
-        # mesh_prim_paths=["/home/yji/Biped/biped_vision/viplanner/assets/bowl_toy_origin.ply"],
-        data_types=["distance_to_image_plane"],
-    )
-    semantic_camera = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base/semantic_camera",
-        offset=CameraCfg.OffsetCfg(pos=(0.60, 0.0, 0.0), rot=(-0.5, 0.5, -0.5, 0.5)),
-        spawn=sim_utils.PinholeCameraCfg(),
-        width=848,
-        height=480,
-        data_types=["rgb"],
-    )
-
-
-@configclass
-class RewardsCfg:
-    
-    goal_distance = RewTerm(func=mdp.goal_distance, weight=0.5, params={"command_name": "goal_command"})
-    goal_direction = RewTerm(func=mdp.goal_direction, weight=0.1, params={"command_name": "goal_command"})
-    robot_goal_velocity_projection = RewTerm(func=mdp.robot_goal_velocity_projection, weight=0.2, params={"command_name": "goal_command","asset_cfg": SceneEntityCfg("robot", body_names="base")})
-    collision_penalty = RewTerm(func=mdp.collision_penalty, weight=-1.0, params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="base"), "threshold": 0.1})
-    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.02)
-    stand_still_velocity_penalty = RewTerm(func=mdp.stand_still_velocity_penalty, weight=-4.0, params={"command_name": "goal_command","asset_cfg": SceneEntityCfg("robot", body_names="base")})
-
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    paths = mdp.NavigationActionCfg(
-        asset_name="robot",
-        low_level_decimation=4,
-        low_level_action=mdp.JointPositionActionCfg(
-            asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True
-        ),
-        low_level_policy_file=os.path.join(LOCAL_ISAAC_DIR, "low_level_policy", "policy.jit"),
-        image_size=(58, 87),
-
-    )
-
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel) #3
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel) #3
-        projected_gravity = ObsTerm(func=mdp.projected_gravity) #3
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "vel_command"}) #3
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel) #12
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel) #12
-        actions = ObsTerm(func=mdp.last_low_level_actions) #12
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    @configclass
-    class PlannerImageCfg(ObsGroup):
-
-        base_lin_accel = ObsTerm(func=mdp.base_lin_acc, params={"asset_cfg": SceneEntityCfg("robot", body_names="base")})
-        base_ang_accel = ObsTerm(func=mdp.base_ang_acc, params={"asset_cfg": SceneEntityCfg("robot", body_names="base")})
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        goal_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "goal_command"})
-        last_mid_actions = ObsTerm(func=mdp.last_mid_actions)
-
-        depth_measurement = ObsTerm(
-            func=mdp.process_depth_image,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera"), "data_type": "distance_to_image_plane"},
-        )
-
-        def __post_init__(self):
-            self.concatenate_terms = True
-            self.enable_corruption = True
-
-    # observation groups
-    low_level_policy: PolicyCfg = PolicyCfg()
-    policy: PlannerImageCfg = PlannerImageCfg()
-    # planner_transform: PlannerTransformCfg = PlannerTransformCfg()
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="base"), "threshold": 1.0},
-    )
-    root_height = DoneTerm(
-        func=mdp.root_height_below_minimum,
-        params={"asset_cfg": SceneEntityCfg("robot", body_names="base"), "minimum_height": 0.2},
-    )
-
-
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    # startup
-    physics_material = EventTerm(
-        func=mdp.randomize_rigid_body_material,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-            "static_friction_range": (0.8, 0.8),
-            "dynamic_friction_range": (0.6, 0.6),
-            "restitution_range": (0.0, 0.0),
-            "num_buckets": 64,
-        },
-    )
-
-    add_base_mass = EventTerm(
-        func=mdp.randomize_rigid_body_mass,
-        mode="startup",
-        params={"asset_cfg": SceneEntityCfg("robot", body_names="base"), "mass_range": (-5.0, 5.0), "operation": "add"},
-    )
-
-    # reset
-    base_external_force_torque = EventTerm(
-        func=mdp.apply_external_force_torque,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="base"),
-            "force_range": (0.0, 0.0),
-            "torque_range": (-0.0, 0.0),
-        },
-    )
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_uniform,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-0.0, 0.0)},
-            "velocity_range": {
-                "x": (-0.0, 0.0),
-                "y": (-0.0, 0.0),
-                "z": (-0.0, 0.0),
-                "roll": (-0.0, 0.0),
-                "pitch": (-0.0, 0.0),
-                "yaw": (-0.0, 0.0),
-            },
-        },
-    )
-
-    reset_robot_joints = EventTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "position_range": (0.5, 1.5),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-    # interval
-    push_robot = EventTerm(
-        func=mdp.push_by_setting_velocity,
-        mode="interval",
-        interval_range_s=(10.0, 15.0),
-        params={"velocity_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5)}},
-    )
-
-
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    goal_command = mdp.GoalCommandGeneratorCfg(
-        asset_name="robot",
-        resampling_time_range=(30.0, 30.0),
-        simple_heading=True,
-        ranges=mdp.GoalCommandGeneratorCfg.Ranges(
-            pos_x=(3.0, 10.0), pos_y=(-6.0, 6.0), heading=(-0, 0)
-        ),
-        # ranges=mdp.GoalCommandGeneratorCfg.Ranges(
-        #     pos_x=(-10.0, 10.0), pos_y=(-10.0, 10.0), heading=(-0, 0)
-        # ),
-    )
-    vel_command = mdp.RLCommandGeneratorCfg(
-        robot_attr="robot",
-        debug_vis=True,
-    )
-
-##
-# Environment configuration
-##
-
-@configclass
-class TestObjNavMatterportCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # managers
-    terminations: TerminationsCfg = TerminationsCfg()
-    events: EventCfg = EventCfg()
-    rewards: RewardsCfg = RewardsCfg()
-    scene: TerrainSceneCfg = TerrainSceneCfg(num_envs=1, env_spacing=1.0)
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 20  # 20->10 Hz, 4->50 Hz
-        self.episode_length_s = 60.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        # if self.scene.height_scanner is not None:
-        # self.scene.height_scanner.update_period = 4 * self.sim.dt  # should we low-level decimation
-        self.scene.contact_forces.update_period = self.sim.dt
-
-        self.viewer.eye = (5, 12, 5)
-        self.viewer.lookat = (5, 0, 0.0)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/train_obj_nav_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/train_obj_nav_cfg.py
deleted file mode 100644
index 941f88d..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/train_obj_nav_cfg.py
+++ /dev/null
@@ -1,474 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.utils import configclass
-# from omni.isaac.lab.utils.assets import LOCAL_ISAAC_DIR
-from omni.isaac.lab.utils.assets import ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg, FlatPatchSamplingCfg
-import omni.isaac.lab.terrains as terrain_gen
-
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import AssetBaseCfg
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import CameraCfg, ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg, patterns
-from omni.isaac.matterport.domains import MatterportRayCasterCfg, MatterportRayCasterCamera, MatterportRayCasterCfg
-from omni.isaac.viplanner.utils import UnRealImporterCfg, MatterportRayCasterCameraCfg
-# from omni.isaac.lab.terrains.config.rough import ROUGH_TERRAINS_CFG
-
-
-from omni.isaac.lab_assets.anymal import ANYMAL_C_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_H1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_G1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_GO2_CFG
-
-import math
-
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-from .train_obj_nav_low_cfg import AnymalCFlatPPORunnerCfg
-@configclass
-class AnymalCObjNavPPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 24
-    max_iterations = 15000
-    save_interval = 50
-    experiment_name = "anymal_c_objnav_depth"
-    empirical_normalization = False
-    policy = RslRlPpoActorCriticCfg(
-        init_noise_std=1.0,
-        actor_hidden_dims=[128, 128, 128],
-        critic_hidden_dims=[128, 128, 128],
-        activation="elu",
-    )
-    algorithm = RslRlPpoAlgorithmCfg(
-        value_loss_coef=1.0,
-        use_clipped_value_loss=True,
-        clip_param=0.2,
-        entropy_coef=0.005,
-        num_learning_epochs=5,
-        num_mini_batches=4,
-        learning_rate=1.0e-4,
-        schedule="adaptive",
-        gamma=0.99,
-        lam=0.95,
-        desired_kl=0.01,
-        max_grad_norm=1.0,
-    )
-
-
-ROUGH_TERRAINS_CFG = TerrainGeneratorCfg(
-    size=(8.0, 8.0),
-    border_width=10.0,
-    num_rows=10,
-    num_cols=20,
-    horizontal_scale=0.1,
-    vertical_scale=0.005,
-    slope_threshold=0.75,
-    use_cache=False,
-    sub_terrains={
-        "boxes": terrain_gen.MeshRepeatedBoxesTerrainCfg(
-            size=(8.0, 8.0),
-            platform_width=0.1,
-            max_height_noise=0.5,
-            object_params_start=terrain_gen.MeshRepeatedBoxesTerrainCfg.ObjectCfg(
-                num_objects=5, height=1.0, size=(0.6, 0.6), max_yx_angle=0.0, degrees=True
-            ),
-            object_params_end=terrain_gen.MeshRepeatedBoxesTerrainCfg.ObjectCfg(
-                num_objects=5, height=2.0, size=(0.6, 0.6), max_yx_angle=0.0, degrees=True
-            ),
-        )
-    },
-)
-for sub_terrain_name, sub_terrain_cfg in ROUGH_TERRAINS_CFG.sub_terrains.items():
-            sub_terrain_cfg.flat_patch_sampling = {
-                sub_terrain_name: FlatPatchSamplingCfg(num_patches=10, patch_radius=1.0, max_height_diff=1.0)
-            }
-"""Rough terrains configuration."""
-
-
-
-##
-# MDP settings
-##
-
-@configclass
-class RewardsCfg:
-    goal_distance = RewTerm(func=mdp.goal_distance, weight=0.5, params={"command_name": "goal_command"})
-    goal_direction = RewTerm(func=mdp.goal_direction, weight=0.1, params={"command_name": "goal_command"})
-    robot_goal_velocity_projection = RewTerm(func=mdp.robot_goal_velocity_projection, weight=0.2, params={"command_name": "goal_command","asset_cfg": SceneEntityCfg("robot", body_names="base")})
-    collision_penalty = RewTerm(func=mdp.collision_penalty, weight=-1.0, params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="base"), "threshold": 0.1})
-    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.02)
-    stand_still_velocity_penalty = RewTerm(func=mdp.stand_still_velocity_penalty, weight=-4.0, params={"command_name": "goal_command","asset_cfg": SceneEntityCfg("robot", body_names="base")})
-
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    paths = mdp.NavigationActionCfg(
-        asset_name="robot",
-        low_level_decimation=4,
-        low_level_action=mdp.JointPositionActionCfg(
-            asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True
-        ),
-        low_level_policy_file=os.path.join(LOCAL_ISAAC_DIR, "low_level_policy", "policy.jit"),
-        image_size=(58, 87),
-
-    )
-
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel) #3
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel) #3
-        projected_gravity = ObsTerm(func=mdp.projected_gravity) #3
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "vel_command"}) #3
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel) #12
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel) #12
-        actions = ObsTerm(func=mdp.last_low_level_actions) #12
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    @configclass
-    class PlannerImageCfg(ObsGroup):
-
-        base_lin_accel = ObsTerm(func=mdp.base_lin_acc, params={"asset_cfg": SceneEntityCfg("robot", body_names="base")})
-        base_ang_accel = ObsTerm(func=mdp.base_ang_acc, params={"asset_cfg": SceneEntityCfg("robot", body_names="base")})
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        goal_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "goal_command"})
-        last_mid_actions = ObsTerm(func=mdp.last_mid_actions)
-        # depth_latent = ObsTerm(
-        #     func=mdp.process_depth,
-        #     params={"sensor_cfg": SceneEntityCfg("depth_camera"), "data_type": "distance_to_image_plane"},
-        # )
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-
-
-        depth_measurement = ObsTerm(
-            func=mdp.process_depth_image,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera"), "data_type": "distance_to_image_plane"},
-        )
-        # semantic_measurement = ObsTerm(
-        #     func=mdp.isaac_camera_data,
-        #     params={"sensor_cfg": SceneEntityCfg("semantic_camera"), "data_type": "rgb"},
-        # )
-
-        def __post_init__(self):
-            self.concatenate_terms = True
-            self.enable_corruption = True
-
-    # @configclass
-    # class PlannerTransformCfg(ObsGroup):
-    #     cam_position = ObsTerm(
-    #         func=mdp.cam_position,
-    #         params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-    #     )
-    #     cam_orientation = ObsTerm(
-    #         func=mdp.cam_orientation,
-    #         params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-    #     )
-    #     cam_orientation_ros = ObsTerm(
-    #         func=mdp.cam_orientation_ros,
-    #         params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-    #     )
-    #     cam_int_matrix = ObsTerm(
-    #         func=mdp.cam_int_matrix,
-    #         params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-    #     )
-
-    #     def __post_init__(self):
-    #         self.concatenate_terms = False
-    #         self.enable_corruption = False
-
-    # observation groups
-    low_level_policy: PolicyCfg = PolicyCfg()
-    policy: PlannerImageCfg = PlannerImageCfg()
-    # planner_transform: PlannerTransformCfg = PlannerTransformCfg()
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="base"), "threshold": 1.0},
-    )
-    root_height = DoneTerm(
-        func=mdp.root_height_below_minimum,
-        params={"asset_cfg": SceneEntityCfg("robot", body_names="base"), "minimum_height": 0.2},
-    )
-
-
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    # startup
-    physics_material = EventTerm(
-        func=mdp.randomize_rigid_body_material,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-            "static_friction_range": (0.8, 0.8),
-            "dynamic_friction_range": (0.6, 0.6),
-            "restitution_range": (0.0, 0.0),
-            "num_buckets": 64,
-        },
-    )
-
-    add_base_mass = EventTerm(
-        func=mdp.randomize_rigid_body_mass,
-        mode="startup",
-        params={"asset_cfg": SceneEntityCfg("robot", body_names="base"), "mass_range": (-5.0, 5.0), "operation": "add"},
-    )
-
-    # reset
-    base_external_force_torque = EventTerm(
-        func=mdp.apply_external_force_torque,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="base"),
-            "force_range": (0.0, 0.0),
-            "torque_range": (-0.0, 0.0),
-        },
-    )
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_from_terrain,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.0, 0.0), "y": (-0.5, 0.5), "yaw": (-0.0, 0.0)},
-            "velocity_range": {
-                "x": (-0.5, 0.5),
-                "y": (-0.5, 0.5),
-                "z": (-0.5, 0.5),
-                "roll": (-0.5, 0.5),
-                "pitch": (-0.5, 0.5),
-                "yaw": (-0.5, 0.5),
-            },
-        },
-    )
-
-    reset_robot_joints = EventTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "position_range": (0.5, 1.5),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-    # interval
-    push_robot = EventTerm(
-        func=mdp.push_by_setting_velocity,
-        mode="interval",
-        interval_range_s=(10.0, 15.0),
-        params={"velocity_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5)}},
-    )
-
-
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    goal_command = mdp.GoalCommandGeneratorCfg(
-        asset_name="robot",
-        resampling_time_range=(30.0, 30.0),
-        simple_heading=True,
-        ranges=mdp.GoalCommandGeneratorCfg.Ranges(
-            pos_x=(3.0, 10.0), pos_y=(-6.0, 6.0), heading=(-0, 0)
-        ),
-        # ranges=mdp.GoalCommandGeneratorCfg.Ranges(
-        #     pos_x=(-10.0, 10.0), pos_y=(-10.0, 10.0), heading=(-0, 0)
-        # ),
-    )
-    vel_command = mdp.RLCommandGeneratorCfg(
-        robot_attr="robot",
-        debug_vis=True,
-    )
-
-    
-
-@configclass
-class TerrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # # ground terrain
-    terrain = TerrainImporterCfg(
-        prim_path="/World/Custom",
-        terrain_type="generator",
-        terrain_generator=ROUGH_TERRAINS_CFG,
-        max_init_terrain_level=5,
-        collision_group=-1,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        visual_material=sim_utils.MdlFileCfg(
-            mdl_path="{NVIDIA_NUCLEUS_DIR}/Materials/Base/Architecture/Shingles_01.mdl",
-            project_uvw=True,
-        ),
-        debug_vis=False,
-    )
-    # terrain = UnRealImporterCfg(
-    #     prim_path="/World/Custom",
-    #     physics_material=sim_utils.RigidBodyMaterialCfg(
-    #         friction_combine_mode="multiply",
-    #         restitution_combine_mode="multiply",
-    #         static_friction=1.0,
-    #         dynamic_friction=1.0,
-    #     ),
-    #     # usd_path="/home/yji/Biped/biped_vision/viplanner/assets/warehouse_new.usd",
-    #     # usd_path="/home/yji/Biped/biped_vision/viplanner/assets/Bowl_bleach.usdz",
-    #     usd_path="/home/yji/Biped/biped_vision/viplanner/assets/bowl_toy_w_plane.usd",
-    #     # usd_path="/home/yji/Biped/biped_vision/viplanner/assets/matterport/raw_data/v1/scans/17DRP5sb8fy/matterport_mesh/17DRP5sb8fy/matterport_mesh/bed1a77d92d64f5cbbaaae4feed64ec1/matterport.usd",
-    #     groundplane=True,
-    #     sem_mesh_to_class_map=None,
-    #     people_config_file=None,
-    #     axis_up="Z",
-    # )
-
-    # robots
-    robot = ANYMAL_C_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_H1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_G1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_GO2_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot.init_state.pos = (3.0, -1.6, 0.6)
-    robot.init_state.rot = (0.707, 0.0, 0.0, 0.707)
-
-    # # sensors
-    # height_scanner = RayCasterCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/base",
-    #     offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.5)),
-    #     attach_yaw_only=True,
-    #     pattern_cfg=patterns.GridPatternCfg(resolution=0.2, size=[3.0, 2.0]),
-    #     # pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-    #     debug_vis=True,
-    #     mesh_prim_paths=["/World/Custom"],
-    # )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    # # # # camera
-    # depth_camera = CameraCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/base/depth_camera",
-    #     offset=CameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(-0.5, 0.5, -0.5, 0.5)),
-    #     spawn=sim_utils.PinholeCameraCfg(),
-    #     # width=848,
-    #     # height=480,
-    #     width=25,
-    #     height=15,
-    #     data_types=["distance_to_image_plane"],
-    # )
-    depth_camera = RayCasterCameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        # mesh_prim_paths=["/home/yji/Biped/biped_vision/viplanner/assets/bowl_toy_origin.ply"],
-        mesh_prim_paths=["/World/Custom"],
-        update_period=0.1,
-        offset=RayCasterCameraCfg.OffsetCfg(pos=(0.60, 0.0, 0.0), rot=(-0.5, 0.5, -0.5, 0.5)),
-        # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(0.579, -0.579, 0.406, -0.406)),
-        # offset=RayCasterCameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(0,1.0,0.0,0.0)),
-        data_types=["distance_to_image_plane"],
-        debug_vis=True,
-        pattern_cfg=patterns.PinholeCameraPatternCfg(
-            focal_length=24.0,  #24.0
-            horizontal_aperture=46.0,#87,#20.955,
-            height=15,
-            width=25,
-        ),
-        # max_distance=10,
-    )
-    # depth_camera = CameraCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/base/depth_camera_resized",
-    #     offset=CameraCfg.OffsetCfg(pos=(0.6, 0.0, 0.0), rot=(-0.5, 0.5, -0.5, 0.5)),
-    #     spawn=sim_utils.PinholeCameraCfg(focal_length=24.0,
-    #                                      horizontal_aperture=46.0), #horizontal_aperture=87,
-    #     width=25,
-    #     height=15,
-    #     data_types=["distance_to_image_plane"],
-    # )
-    # import ipdb; ipdb.set_trace()
-    # semantic_camera = CameraCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/base/semantic_camera",
-    #     offset=CameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.5), rot=(-0.5, 0.5, -0.5, 0.5)),
-    #     spawn=sim_utils.PinholeCameraCfg(),
-    #     width=848,
-    #     height=480,
-    #     data_types=["rgb"],
-    # )
-
-##
-# Environment configuration
-##
-
-@configclass
-class TrainObjNavSceneCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # managers
-    terminations: TerminationsCfg = TerminationsCfg()
-    events: EventCfg = EventCfg()
-    rewards: RewardsCfg = RewardsCfg()
-    scene: TerrainSceneCfg = TerrainSceneCfg(num_envs=1, env_spacing=1.0)
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 20  # 20->10 Hz, 4->50 Hz
-        self.episode_length_s = 60.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        # if self.scene.height_scanner is not None:
-        # self.scene.height_scanner.update_period = 4 * self.sim.dt  # should we low-level decimation
-        self.scene.contact_forces.update_period = self.sim.dt
-
-        self.viewer.eye = (5, 12, 5)
-        self.viewer.lookat = (5, 0, 0.0)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/train_obj_nav_low_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/train_obj_nav_low_cfg.py
deleted file mode 100644
index 34aee0e..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/train_obj_nav_low_cfg.py
+++ /dev/null
@@ -1,414 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import RewardTermCfg as RewTerm
-# from omni.isaac.lab.managers import RandomizationTermCfg as RandTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.utils import configclass
-# from omni.isaac.lab.utils.assets import LOCAL_ISAAC_DIR
-from omni.isaac.lab.utils.assets import ISAACLAB_NUCLEUS_DIR
-from omni.isaac.lab.terrains import TerrainImporterCfg, TerrainGeneratorCfg
-import omni.isaac.lab.terrains as terrain_gen
-
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import AssetBaseCfg
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import CameraCfg, ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.viplanner.utils import UnRealImporterCfg
-# from omni.isaac.lab.terrains.config.rough import ROUGH_TERRAINS_CFG
-
-
-from omni.isaac.lab_assets.anymal import ANYMAL_C_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_H1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_G1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_GO2_CFG
-
-import math
-
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-    RslRlPpoActorCriticCfg,
-    RslRlPpoAlgorithmCfg,
-)
-
-@configclass
-class AnymalCRoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 24
-    max_iterations = 1500
-    save_interval = 50
-    experiment_name = "anymal_c_rough"
-    empirical_normalization = False
-    policy = RslRlPpoActorCriticCfg(
-        init_noise_std=1.0,
-        actor_hidden_dims=[512, 256, 128],
-        critic_hidden_dims=[512, 256, 128],
-        activation="elu",
-    )
-    algorithm = RslRlPpoAlgorithmCfg(
-        value_loss_coef=1.0,
-        use_clipped_value_loss=True,
-        clip_param=0.2,
-        entropy_coef=0.005,
-        num_learning_epochs=5,
-        num_mini_batches=4,
-        learning_rate=1.0e-3,
-        schedule="adaptive",
-        gamma=0.99,
-        lam=0.95,
-        desired_kl=0.01,
-        max_grad_norm=1.0,
-    )
-
-@configclass
-class AnymalCFlatPPORunnerCfg(AnymalCRoughPPORunnerCfg):
-    def __post_init__(self):
-        super().__post_init__()
-
-        self.max_iterations = 3000
-        self.experiment_name = "anymal_c_flat"
-        self.policy.actor_hidden_dims = [128, 128, 128]
-        self.policy.critic_hidden_dims = [128, 128, 128]
-
-
-ROUGH_TERRAINS_CFG = TerrainGeneratorCfg(
-    size=(8.0, 8.0),
-    border_width=20.0,
-    num_rows=10,
-    num_cols=20,
-    horizontal_scale=0.1,
-    vertical_scale=0.005,
-    slope_threshold=0.75,
-    use_cache=False,
-    sub_terrains={
-        "pyramid_stairs": terrain_gen.MeshPyramidStairsTerrainCfg(
-            proportion=0.2,
-            step_height_range=(0.05, 0.23),
-            step_width=0.3,
-            platform_width=3.0,
-            border_width=1.0,
-            holes=False,
-        ),
-        "pyramid_stairs_inv": terrain_gen.MeshInvertedPyramidStairsTerrainCfg(
-            proportion=0.2,
-            step_height_range=(0.05, 0.23),
-            step_width=0.3,
-            platform_width=3.0,
-            border_width=1.0,
-            holes=False,
-        ),
-        "boxes": terrain_gen.MeshRandomGridTerrainCfg(
-            proportion=0.2, grid_width=0.45, grid_height_range=(0.05, 0.2), platform_width=2.0
-        ),
-        "random_rough": terrain_gen.HfRandomUniformTerrainCfg(
-            proportion=0.2, noise_range=(0.02, 0.10), noise_step=0.02, border_width=0.25
-        ),
-        "hf_pyramid_slope": terrain_gen.HfPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25
-        ),
-        "hf_pyramid_slope_inv": terrain_gen.HfInvertedPyramidSlopedTerrainCfg(
-            proportion=0.1, slope_range=(0.0, 0.4), platform_width=2.0, border_width=0.25
-        ),
-    },
-)
-"""Rough terrains configuration."""
-
-
-
-##
-# MDP settings
-##
-
-@configclass
-class RewardsCfg:
-        # -- task
-    track_lin_vel_xy_exp = RewTerm(
-        func=mdp.track_lin_vel_xy_exp, weight=1.0, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    )
-    track_ang_vel_z_exp = RewTerm(
-        func=mdp.track_ang_vel_z_exp, weight=0.5, params={"command_name": "base_velocity", "std": math.sqrt(0.25)}
-    )
-    # -- penalties
-    lin_vel_z_l2 = RewTerm(func=mdp.lin_vel_z_l2, weight=-2.0)
-    ang_vel_xy_l2 = RewTerm(func=mdp.ang_vel_xy_l2, weight=-0.05)
-    dof_torques_l2 = RewTerm(func=mdp.joint_torques_l2, weight=-1.0e-5)
-    dof_acc_l2 = RewTerm(func=mdp.joint_acc_l2, weight=-2.5e-7)
-    action_rate_l2 = RewTerm(func=mdp.action_rate_l2, weight=-0.01)
-    feet_air_time = RewTerm(
-        func=mdp.feet_air_time,
-        weight=0.125,
-        params={
-            "sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*FOOT"),
-            "command_name": "base_velocity",
-            "threshold": 0.5,
-        },
-    )
-    undesired_contacts = RewTerm(
-        func=mdp.undesired_contacts,
-        weight=-1.0,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names=".*THIGH"), "threshold": 1.0},
-    )
-    # -- optional penalties
-    flat_orientation_l2 = RewTerm(func=mdp.flat_orientation_l2, weight=0.0)
-    dof_pos_limits = RewTerm(func=mdp.joint_pos_limits, weight=0.0)
-
-
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    joint_pos = mdp.JointPositionActionCfg(asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True)
-
-
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel)
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "base_velocity"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel)
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel)
-        actions = ObsTerm(func=mdp.last_action)
-        height_scan = ObsTerm(
-            func=mdp.height_scan,
-            params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-            clip=(-1.0, 1.0),
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    # observation groups
-    policy: PolicyCfg = PolicyCfg()
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="base"), "threshold": 1.0},
-    )
-
-
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    # startup
-    physics_material = EventTerm(
-        func=mdp.randomize_rigid_body_material,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-            "static_friction_range": (0.8, 0.8),
-            "dynamic_friction_range": (0.6, 0.6),
-            "restitution_range": (0.0, 0.0),
-            "num_buckets": 64,
-        },
-    )
-
-    add_base_mass = EventTerm(
-        func=mdp.randomize_rigid_body_mass,
-        mode="startup",
-        params={"asset_cfg": SceneEntityCfg("robot", body_names="base"), "mass_range": (-5.0, 5.0), "operation": "add"},
-    )
-
-    # reset
-    base_external_force_torque = EventTerm(
-        func=mdp.apply_external_force_torque,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="base"),
-            "force_range": (0.0, 0.0),
-            "torque_range": (-0.0, 0.0),
-        },
-    )
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_uniform,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5), "yaw": (-3.14, 3.14)},
-            "velocity_range": {
-                "x": (-0.5, 0.5),
-                "y": (-0.5, 0.5),
-                "z": (-0.5, 0.5),
-                "roll": (-0.5, 0.5),
-                "pitch": (-0.5, 0.5),
-                "yaw": (-0.5, 0.5),
-            },
-        },
-    )
-
-    reset_robot_joints = EventTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "position_range": (0.5, 1.5),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-    # interval
-    push_robot = EventTerm(
-        func=mdp.push_by_setting_velocity,
-        mode="interval",
-        interval_range_s=(10.0, 15.0),
-        params={"velocity_range": {"x": (-0.5, 0.5), "y": (-0.5, 0.5)}},
-    )
-
-
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    base_velocity = mdp.UniformVelocityCommandCfg(
-        asset_name="robot",
-        resampling_time_range=(10.0, 10.0),
-        rel_standing_envs=0.02,
-        rel_heading_envs=1.0,
-        heading_command=False,
-        heading_control_stiffness=0.5,
-        debug_vis=True,
-        ranges=mdp.UniformVelocityCommandCfg.Ranges(
-            lin_vel_x=(-1.0, 1.0), lin_vel_y=(-1.0, 1.0), ang_vel_z=(-1.0, 1.0), heading=(-math.pi, math.pi)
-        ),
-    )
-
-
-
-@configclass
-class TerrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = TerrainImporterCfg(
-        prim_path="/World/ground",
-        terrain_type="generator",
-        terrain_generator=ROUGH_TERRAINS_CFG,
-        max_init_terrain_level=5,
-        collision_group=-1,
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        visual_material=sim_utils.MdlFileCfg(
-            mdl_path="{NVIDIA_NUCLEUS_DIR}/Materials/Base/Architecture/Shingles_01.mdl",
-            project_uvw=True,
-        ),
-        debug_vis=False,
-    )
-
-    # robots
-    robot = ANYMAL_C_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_H1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_G1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_GO2_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot.init_state.pos = (3.0, -1.6, 0.7)
-    robot.init_state.rot = (0.7, 0.0, 0.0, 0.7)
-
-    # sensors
-    height_scanner = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.5)),
-        attach_yaw_only=True,
-        pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-        debug_vis=True,
-        mesh_prim_paths=["/World/Custom"],
-    )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, track_air_time=True, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    # # camera
-    # depth_camera = CameraCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/base/depth_camera",
-    #     offset=CameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.5), rot=(-0.5, 0.5, -0.5, 0.5)),
-    #     spawn=sim_utils.PinholeCameraCfg(),
-    #     width=848,
-    #     height=480,
-    #     data_types=["distance_to_image_plane"],
-    # )
-    # semantic_camera = CameraCfg(
-    #     prim_path="{ENV_REGEX_NS}/Robot/base/semantic_camera",
-    #     offset=CameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.5), rot=(-0.5, 0.5, -0.5, 0.5)),
-    #     spawn=sim_utils.PinholeCameraCfg(),
-    #     width=848,
-    #     height=480,
-    #     data_types=["rgb"],
-    # )
-
-##
-# Environment configuration
-##
-
-@configclass
-class TrainObjNavLowSceneCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # managers
-    terminations: TerminationsCfg = TerminationsCfg()
-    events: EventCfg = EventCfg()
-    rewards: RewardsCfg = RewardsCfg()
-    scene: TerrainSceneCfg = TerrainSceneCfg(num_envs=4096, env_spacing=2.5)
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 4  # 20->10 Hz, 4->50 Hz
-        self.episode_length_s = 20.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        # self.scene.height_scanner.update_period = 4 * self.sim.dt  # should we low-level decimation
-        self.scene.contact_forces.update_period = self.sim.dt
-        self.scene.terrain.terrain_type = "plane"
-        self.scene.terrain.terrain_generator = None
-        self.rewards.flat_orientation_l2.weight = -5.0
-        self.rewards.dof_torques_l2.weight = -2.5e-5
-        self.rewards.feet_air_time.weight = 0.5
-        self.scene.height_scanner = None
-        self.observations.policy.height_scan = None
-        self.curriculum.terrain_levels = None
-
-        self.viewer.eye = (5, 12, 5)
-        self.viewer.lookat = (5, 0, 0.0)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/warehouse_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/warehouse_cfg.py
deleted file mode 100644
index ab5dfad..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/warehouse_cfg.py
+++ /dev/null
@@ -1,110 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import AssetBaseCfg
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import CameraCfg, ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.lab.utils import configclass
-from omni.isaac.viplanner.utils import UnRealImporterCfg
-
-from ..viplanner import DATA_DIR
-from .base_cfg import ViPlannerBaseCfg
-
-##
-# Pre-defined configs
-##
-# isort: off
-from omni.isaac.lab_assets.anymal import ANYMAL_C_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_H1_CFG
-
-##
-# Scene definition
-##
-
-
-@configclass
-class TerrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = UnRealImporterCfg(
-        prim_path="/World/Warehouse",
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        usd_path="/home/yji/Biped/biped_vision/viplanner/assets/warehouse_new.usd",
-        groundplane=True,
-        sem_mesh_to_class_map=os.path.join(DATA_DIR, "warehouse", "keyword_mapping.yml"),
-        people_config_file=os.path.join(DATA_DIR, "warehouse", "people_cfg.yml"),
-        axis_up="Z",
-    )
-
-    # robots
-    robot = ANYMAL_C_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot.init_state.pos = (5.0, 10.5, 0.6)
-    robot.init_state.rot = (0.7, 0.0, 0.0, -0.7)
-
-    # sensors
-    height_scanner = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.5)),
-        attach_yaw_only=True,
-        pattern_cfg=patterns.GridPatternCfg(resolution=0.1, size=[1.6, 1.0]),
-        debug_vis=True,
-        mesh_prim_paths=["/World/Warehouse"],
-    )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    # camera
-    depth_camera = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base/depth_camera",
-        offset=CameraCfg.OffsetCfg(pos=(0.510, 0.0, 0.015), rot=(-0.5, 0.5, -0.5, 0.5)),
-        spawn=sim_utils.PinholeCameraCfg(),
-        width=848,
-        height=480,
-        data_types=["distance_to_image_plane"],
-    )
-    semantic_camera = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base/semantic_camera",
-        offset=CameraCfg.OffsetCfg(pos=(0.510, 0.0, 0.015), rot=(-0.5, 0.5, -0.5, 0.5)),
-        spawn=sim_utils.PinholeCameraCfg(),
-        width=848,
-        height=480,
-        data_types=["rgb"],
-    )
-
-
-##
-# Environment configuration
-##
-
-
-@configclass
-class ViPlannerWarehouseCfg(ViPlannerBaseCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Scene settings
-    scene: TerrainSceneCfg = TerrainSceneCfg(num_envs=1, env_spacing=1.0)
-
-    def __post_init__(self):
-        """Post initialization."""
-        super().__post_init__()
-        # adapt viewer
-        self.viewer.eye = (5, 12, 5)
-        self.viewer.lookat = (5, 0, 0.0)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/warehouse_objnav_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/warehouse_objnav_cfg.py
deleted file mode 100644
index 85222b0..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/config/warehouse_objnav_cfg.py
+++ /dev/null
@@ -1,356 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-
-import omni.isaac.viplanner.viplanner.mdp as mdp
-from omni.isaac.lab.envs import ManagerBasedRLEnvCfg
-from omni.isaac.lab.managers import ObservationGroupCfg as ObsGroup
-from omni.isaac.lab.managers import ObservationTermCfg as ObsTerm
-from omni.isaac.lab.managers import EventTermCfg as EventTerm
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers import TerminationTermCfg as DoneTerm
-from omni.isaac.lab.utils import configclass
-# from omni.isaac.lab.utils.assets import LOCAL_ISAAC_DIR
-from omni.isaac.lab.utils.assets import ISAACLAB_NUCLEUS_DIR
-from ..viplanner import DATA_DIR
-
-import omni.isaac.lab.sim as sim_utils
-from omni.isaac.lab.assets import AssetBaseCfg
-from omni.isaac.lab.scene import InteractiveSceneCfg
-from omni.isaac.lab.sensors import CameraCfg, ContactSensorCfg, RayCasterCfg, patterns
-from omni.isaac.viplanner.utils import UnRealImporterCfg
-
-
-from omni.isaac.lab_assets.anymal import ANYMAL_C_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_H1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_G1_CFG
-from omni.isaac.lab_assets.unitree import UNITREE_GO2_CFG
-
-##
-# MDP settings
-##
-
-@configclass
-class RewardsCfg:
-    pass
-
-@configclass
-class ActionsCfg:
-    """Action specifications for the MDP."""
-
-    paths = mdp.NavigationActionCfg(
-        asset_name="robot",
-        low_level_decimation=4,
-        low_level_action=mdp.JointPositionActionCfg(
-            asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True
-        ),
-        low_level_policy_file=os.path.join(LOCAL_ISAAC_DIR, "low_level_policy", "policy.jit"),
-        image_size=(58, 87),
-    )
-
-@configclass
-class ObservationsCfg:
-    """Observation specifications for the MDP."""
-
-    @configclass
-    class PolicyCfg(ObsGroup):
-        """Observations for policy group."""
-
-        # observation terms (order preserved)
-        base_lin_vel = ObsTerm(func=mdp.base_lin_vel)
-        base_ang_vel = ObsTerm(func=mdp.base_ang_vel)
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        velocity_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "lowlevel_command"})
-        joint_pos = ObsTerm(func=mdp.joint_pos_rel)
-        joint_vel = ObsTerm(func=mdp.joint_vel_rel)
-        actions = ObsTerm(func=mdp.low_level_actions)
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    @configclass
-    class MidLevelPlannerCfg(ObsGroup):
-        """Observations for the mid-level planner."""
-
-        # observation terms (order preserved)
-        base_lin_accel = ObsTerm(func=mdp.base_lin_acc, params={"asset_cfg": SceneEntityCfg("robot", body_names="base")})
-        base_ang_accel = ObsTerm(func=mdp.base_ang_acc, params={"asset_cfg": SceneEntityCfg("robot", body_names="base")})
-        projected_gravity = ObsTerm(func=mdp.projected_gravity)
-        goal_commands = ObsTerm(func=mdp.generated_commands, params={"command_name": "midlevel_command"})
-        last_mid_actions = ObsTerm(func=mdp.last_mid_actions)
-        # height_scan = ObsTerm(
-        #     func=mdp.height_scan,
-        #     params={"sensor_cfg": SceneEntityCfg("height_scanner")},
-        #     clip=(-1.0, 1.0),
-        # )
-        depth_measurement = ObsTerm(
-            func=mdp.process_depth_image,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera_resized"), "data_type": "distance_to_image_plane"},
-        )
-
-        def __post_init__(self):
-            self.enable_corruption = True
-            self.concatenate_terms = True
-
-    @configclass
-    class PlannerImageCfg(ObsGroup):
-        depth_measurement = ObsTerm(
-            func=mdp.isaac_camera_data,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera"), "data_type": "distance_to_image_plane"},
-        )
-        semantic_measurement = ObsTerm(
-            func=mdp.isaac_camera_data,
-            params={"sensor_cfg": SceneEntityCfg("semantic_camera"), "data_type": "rgb"},
-        )
-
-        def __post_init__(self):
-            self.concatenate_terms = False
-            self.enable_corruption = False
-
-    @configclass
-    class PlannerTransformCfg(ObsGroup):
-        cam_position = ObsTerm(
-            func=mdp.cam_position,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-        cam_orientation = ObsTerm(
-            func=mdp.cam_orientation,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-        cam_orientation_ros = ObsTerm(
-            func=mdp.cam_orientation_ros,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-        cam_int_matrix = ObsTerm(
-            func=mdp.cam_int_matrix,
-            params={"sensor_cfg": SceneEntityCfg("depth_camera")},
-        )
-
-        def __post_init__(self):
-            self.concatenate_terms = False
-            self.enable_corruption = False
-
-    
-
-    # observation groups
-    low_level_policy: PolicyCfg = PolicyCfg()
-    mid_level_planner: MidLevelPlannerCfg = MidLevelPlannerCfg()
-    planner_image: PlannerImageCfg = PlannerImageCfg()
-    planner_transform: PlannerTransformCfg = PlannerTransformCfg()
-
-
-@configclass
-class TerminationsCfg:
-    """Termination terms for the MDP."""
-
-    time_out = DoneTerm(func=mdp.time_out, time_out=True)
-    base_contact = DoneTerm(
-        func=mdp.illegal_contact,
-        params={"sensor_cfg": SceneEntityCfg("contact_forces", body_names="base"), "threshold": 1.0},
-    )
-
-@configclass
-class EventCfg:
-    """Configuration for events."""
-
-    # startup
-    physics_material = EventTerm(
-        func=mdp.randomize_rigid_body_material,
-        mode="startup",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
-            "static_friction_range": (0.8, 0.8),
-            "dynamic_friction_range": (0.6, 0.6),
-            "restitution_range": (0.0, 0.0),
-            "num_buckets": 64,
-        },
-    )
-
-    add_base_mass = EventTerm(
-        func=mdp.randomize_rigid_body_mass,
-        mode="startup",
-        params={"asset_cfg": SceneEntityCfg("robot", body_names="base"), "mass_range": (-0.0, 0.0), "operation": "add"},
-    )
-
-    # reset
-    base_external_force_torque = EventTerm(
-        func=mdp.apply_external_force_torque,
-        mode="reset",
-        params={
-            "asset_cfg": SceneEntityCfg("robot", body_names="base"),
-            "force_range": (0.0, 0.0),
-            "torque_range": (-0.0, 0.0),
-        },
-    )
-
-    reset_base = EventTerm(
-        func=mdp.reset_root_state_uniform,
-        mode="reset",
-        params={
-            "pose_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0), "yaw": (-1.57, -1.57)},
-            "velocity_range": {
-                "x": (-0.0, 0.0),
-                "y": (-0.0, 0.0),
-                "z": (-0.0, 0.0),
-                "roll": (-0.0, 0.0),
-                "pitch": (-0.0, 0.0),
-                "yaw": (-0.0, 0.0),
-            },
-        },
-    )
-
-    reset_robot_joints = EventTerm(
-        func=mdp.reset_joints_by_scale,
-        mode="reset",
-        params={
-            "position_range": (0.0, 0.0),
-            "velocity_range": (0.0, 0.0),
-        },
-    )
-
-    # interval
-    push_robot = EventTerm(
-        func=mdp.push_by_setting_velocity,
-        mode="interval",
-        interval_range_s=(10.0, 15.0),
-        params={"velocity_range": {"x": (-0.0, 0.0), "y": (-0.0, 0.0)}},
-    )
-
-@configclass
-class CommandsCfg:
-    """Command specifications for the MDP."""
-
-    midlevel_command: mdp.MidLevelCommandGeneratorCfg = mdp.MidLevelCommandGeneratorCfg(
-        robot_attr="robot",
-        debug_vis=True,
-    )
-    lowlevel_command: mdp.LowLevelCommandGeneratorCfg = mdp.LowLevelCommandGeneratorCfg(
-        robot_attr="robot",
-        debug_vis=True,
-    )
-
-@configclass
-class TerrainSceneCfg(InteractiveSceneCfg):
-    """Configuration for the terrain scene with a legged robot."""
-
-    # ground terrain
-    terrain = UnRealImporterCfg(
-        prim_path="/World/Custom",
-        physics_material=sim_utils.RigidBodyMaterialCfg(
-            friction_combine_mode="multiply",
-            restitution_combine_mode="multiply",
-            static_friction=1.0,
-            dynamic_friction=1.0,
-        ),
-        usd_path="/home/yji/Biped/biped_vision/viplanner/assets/warehouse_new.usd",
-        # usd_path="/home/yji/Biped/biped_vision/viplanner/assets/Bowl_bleach.usdz",
-        # usd_path="/home/yji/Biped/biped_vision/viplanner/assets/bowl_toy_w_plane.usd",
-        # usd_path="/home/yji/Biped/biped_vision/viplanner/assets/matterport/raw_data/v1/scans/17DRP5sb8fy/matterport_mesh/17DRP5sb8fy/matterport_mesh/bed1a77d92d64f5cbbaaae4feed64ec1/matterport.usd",
-        groundplane=True,
-        # sem_mesh_to_class_map=None,
-        sem_mesh_to_class_map=os.path.join(DATA_DIR, "warehouse", "keyword_mapping.yml"),
-        people_config_file=os.path.join(DATA_DIR, "warehouse", "people_cfg.yml"),
-        axis_up="Z",
-    )
-
-    # robots
-    robot = ANYMAL_C_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_H1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_G1_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    # robot = UNITREE_GO2_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
-    robot.init_state.pos = (5.0, 10.5, 0.6)
-    robot.init_state.rot = (0.7, 0.0, 0.0, -0.7)
-
-    # sensors
-    height_scanner = RayCasterCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base",
-        offset=RayCasterCfg.OffsetCfg(pos=(0.0, 0.0, 0.5)),
-        attach_yaw_only=True,
-        pattern_cfg=patterns.GridPatternCfg(resolution=0.2, size=[3.0, 2.0]),
-        debug_vis=True,
-        mesh_prim_paths=["/World/Custom"],
-    )
-    contact_forces = ContactSensorCfg(prim_path="{ENV_REGEX_NS}/Robot/.*", history_length=3, debug_vis=False)
-    # lights
-    light = AssetBaseCfg(
-        prim_path="/World/light",
-        spawn=sim_utils.DistantLightCfg(
-            color=(1.0, 1.0, 1.0),
-            intensity=1000.0,
-        ),
-    )
-    # camera
-    semantic_camera = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base/semantic_camera",
-        offset=CameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(-0.5, 0.5, -0.5, 0.5)),
-        spawn=sim_utils.PinholeCameraCfg(),
-        width=848,
-        height=480,
-        data_types=["rgb"],
-    )
-    
-    depth_camera = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base/depth_camera",
-        offset=CameraCfg.OffsetCfg(pos=(0.00, 0.0, 0.3), rot=(-0.5, 0.5, -0.5, 0.5)),
-        spawn=sim_utils.PinholeCameraCfg(),
-        width=848,
-        height=480,
-        data_types=["distance_to_image_plane"],
-    )
-    depth_camera_resized = CameraCfg(
-        prim_path="{ENV_REGEX_NS}/Robot/base/depth_camera_resized",
-        offset=CameraCfg.OffsetCfg(pos=(0.6, 0.0, 0.0), rot=(-0.5, 0.5, -0.5, 0.5)),
-        spawn=sim_utils.PinholeCameraCfg(horizontal_aperture=46.0), #horizontal_aperture=87,
-        width=25,
-        height=15,
-        data_types=["distance_to_image_plane"],
-    )
-    
-
-##
-# Environment configuration
-##
-
-@configclass
-class WarehouseObjnavCfg(ManagerBasedRLEnvCfg):
-    """Configuration for the locomotion velocity-tracking environment."""
-
-    # Basic settings
-    observations: ObservationsCfg = ObservationsCfg()
-    actions: ActionsCfg = ActionsCfg()
-    commands: CommandsCfg = CommandsCfg()
-    # managers
-    terminations: TerminationsCfg = TerminationsCfg()
-    events: EventCfg = EventCfg()
-    rewards: RewardsCfg = RewardsCfg()
-    scene: TerrainSceneCfg = TerrainSceneCfg(num_envs=1, env_spacing=1.0)
-
-    def __post_init__(self):
-        """Post initialization."""
-        # general settings
-        self.decimation = 20  # 10 Hz
-        self.episode_length_s = 60.0
-        # simulation settings
-        self.sim.dt = 0.005
-        self.sim.disable_contact_processing = True
-        self.sim.physics_material.static_friction = 1.0
-        self.sim.physics_material.dynamic_friction = 1.0
-        self.sim.physics_material.friction_combine_mode = "max"
-        self.sim.physics_material.restitution_combine_mode = "max"
-        # update sensor update periods
-        # we tick all the sensors based on the smallest update period (physics update period)
-        self.scene.height_scanner.update_period = 4 * self.sim.dt  # should we low-level decimation
-        self.scene.contact_forces.update_period = self.sim.dt
-
-        self.viewer.eye = (5, 12, 5)
-        self.viewer.lookat = (5, 0, 0.0)
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/__init__.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/__init__.py
deleted file mode 100644
index 94c8692..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/__init__.py
+++ /dev/null
@@ -1,23 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from .unreal_importer import UnRealImporter
-from .unreal_importer_cfg import UnRealImporterCfg
-from .matterport_raycast_camera import MatterportRayCasterCameraCfg
-from .viplanner_matterport_raycast_camera import (
-    VIPlannerMatterportRayCasterCamera,
-    VIPlannerMatterportRayCasterCameraCfg,
-)
-from .wrappers import RslRlVecEnvHistoryWrapper
-
-__all__ = [
-    # "VIPlannerMatterportRayCasterCamera",
-    # "VIPlannerMatterportRayCasterCameraCfg",
-    "MatterportRayCasterCameraCfg"
-    "UnRealImporter",
-    "UnRealImporterCfg",
-    "RslRlVecEnvHistoryWrapper",
-]
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/inference_seem_pano.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/inference_seem_pano.py
deleted file mode 100644
index 594ab74..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/inference_seem_pano.py
+++ /dev/null
@@ -1,164 +0,0 @@
-# --------------------------------------------------------
-# Semantic-SAM: Segment and Recognize Anything at Any Granularity
-# Copyright (c) 2023 Microsoft
-# Licensed under The MIT License [see LICENSE for details]
-# Written by Hao Zhang (hzhangcx@connect.ust.hk)
-# --------------------------------------------------------
-
-import torch
-import numpy as np
-from torchvision import transforms
-# from task_adapter.utils.visualizer import Visualizer
-from typing import Tuple
-from PIL import Image
-from detectron2.data import MetadataCatalog
-import matplotlib.pyplot as plt
-# import cv2
-# import io
-# from .automatic_mask_generator import SeemAutomaticMaskGenerator
-metadata = MetadataCatalog.get('coco_2017_train_panoptic')
-
-from segment_anything.utils.amg import (
-    MaskData,
-    area_from_rle,
-    batch_iterator,
-    batched_mask_to_box,
-    box_xyxy_to_xywh,
-    build_all_layer_point_grids,
-    calculate_stability_score,
-    coco_encode_rle,
-    generate_crop_boxes,
-    is_box_near_crop_edge,
-    mask_to_rle_pytorch,
-    remove_small_regions,
-    rle_to_mask,
-    uncrop_boxes_xyxy,
-    uncrop_masks,
-    uncrop_points,
-)
-
-
-def inference_seem_pano(model, image, text_size, label_mode='1', alpha=0.1, anno_mode=['Mask']):
-    t = []
-    t.append(transforms.Resize(int(text_size), interpolation=Image.BICUBIC))
-    transform1 = transforms.Compose(t)
-    image_ori = transform1(image)
-
-    image_ori = np.asarray(image_ori)
-    images = torch.from_numpy(image_ori.copy()).permute(2,0,1).cuda()
-
-    orig_size = images.shape[-2:]
-    orig_h, orig_w = orig_size
-    crop_box = [0,0,orig_w,orig_h]
-
-    data = {"image": images, "height": orig_h, "width": orig_w}
-    batch_inputs = [data]
-
-    model.model.metadata = metadata
-    outputs = model.model.evaluate(batch_inputs)
-
-    pano_mask = outputs[0]['panoptic_seg'][0]
-    pano_info = outputs[0]['panoptic_seg'][1]
-
-    masks = []
-    for seg_info in pano_info:
-        masks += [pano_mask == seg_info['id']]
-    masks = torch.stack(masks, dim=0)
-    iou_preds = torch.ones(masks.shape[0], dtype=torch.float32)
-    points = torch.zeros((masks.shape[0], 2), dtype=torch.float32)
-
-    mask_data = MaskData(
-        masks=masks,
-        iou_preds=iou_preds,
-        points=points,
-    )
-    mask_data["stability_score"] = torch.ones(masks.shape[0], dtype=torch.float32)
-    del masks
-
-    mask_data["boxes"] = batched_mask_to_box(mask_data["masks"])
-    mask_data["crop_boxes"] = torch.tensor([crop_box for _ in range(len(mask_data["boxes"]))])
-
-    # Compress to RLE
-    mask_data["masks"] = uncrop_masks(mask_data["masks"], crop_box, orig_h, orig_w)
-    mask_data["rles"] = mask_to_rle_pytorch(mask_data["masks"])
-    del mask_data["masks"]
-    mask_data["segmentations"] = [rle_to_mask(rle) for rle in mask_data["rles"]]
-
-    # Write mask records
-    outputs = []
-    for idx in range(len(mask_data["segmentations"])):
-        ann = {
-            "segmentation": mask_data["segmentations"][idx],
-            "area": area_from_rle(mask_data["rles"][idx]),
-            "bbox": box_xyxy_to_xywh(mask_data["boxes"][idx]).tolist(),
-            "predicted_iou": mask_data["iou_preds"][idx].item(),
-            "point_coords": [mask_data["points"][idx].tolist()],
-            "stability_score": mask_data["stability_score"][idx].item(),
-            "crop_box": box_xyxy_to_xywh(mask_data["crop_boxes"][idx]).tolist(),
-        }
-        outputs.append(ann)
-
-    from omni.isaac.viplanner.utils.visualizer import Visualizer
-    visual = Visualizer(image_ori, metadata=metadata)
-    # create a full zero image as the image_orig
-    sorted_anns = sorted(outputs, key=(lambda x: x['area']), reverse=True)
-    label = 1
-    mask_map = np.zeros(image_ori.shape, dtype=np.uint8)    
-    for i, ann in enumerate(sorted_anns):
-        mask = ann['segmentation']
-        color_mask = np.random.random((1, 3)).tolist()[0]
-        # color_mask = [int(c*255) for c in color_mask]
-        demo = visual.draw_binary_mask_with_number(mask, text=str(label), label_mode=label_mode, alpha=alpha, anno_mode=anno_mode)
-        # assign the mask to the mask_map
-        mask_map[mask == 1] = label
-        label += 1
-    im = demo.get_image()
-    # fig=plt.figure(figsize=(10, 10))
-    # plt.imshow(image_ori)
-    # show_anns(outputs)
-    # fig.canvas.draw()
-    # im=Image.frombytes('RGB', fig.canvas.get_width_height(), fig.canvas.tostring_rgb())
-    return im, sorted_anns
-
-
-def remove_small_regions(
-    mask: np.ndarray, area_thresh: float, mode: str
-) -> Tuple[np.ndarray, bool]:
-    """
-    Removes small disconnected regions and holes in a mask. Returns the
-    mask and an indicator of if the mask has been modified.
-    """
-    import cv2  # type: ignore
-
-    assert mode in ["holes", "islands"]
-    correct_holes = mode == "holes"
-    working_mask = (correct_holes ^ mask).astype(np.uint8)
-    n_labels, regions, stats, _ = cv2.connectedComponentsWithStats(working_mask, 8)
-    sizes = stats[:, -1][1:]  # Row 0 is background label
-    small_regions = [i + 1 for i, s in enumerate(sizes) if s < area_thresh]
-    if len(small_regions) == 0:
-        return mask, False
-    fill_labels = [0] + small_regions
-    if not correct_holes:
-        fill_labels = [i for i in range(n_labels) if i not in fill_labels]
-        # If every region is below threshold, keep largest
-        if len(fill_labels) == 0:
-            fill_labels = [int(np.argmax(sizes)) + 1]
-    mask = np.isin(regions, fill_labels)
-    return mask, True
-
-def show_anns(anns):
-    if len(anns) == 0:
-        return
-    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
-    ax = plt.gca()
-    ax.set_autoscale_on(False)
-    polygons = []
-    color = []
-    for ann in sorted_anns:
-        m = ann['segmentation']
-        img = np.ones((m.shape[0], m.shape[1], 3))
-        color_mask = np.random.random((1, 3)).tolist()[0]
-        for i in range(3):
-            img[:,:,i] = color_mask[i]
-        ax.imshow(np.dstack((img, m*0.35)))
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/matterport_raycast_camera.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/matterport_raycast_camera.py
deleted file mode 100644
index e25a18d..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/matterport_raycast_camera.py
+++ /dev/null
@@ -1,23 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import torch
-import yaml
-from omni.isaac.matterport.domains import MatterportRayCasterCamera
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg
-from omni.isaac.lab.utils.configclass import configclass
-from omni.isaac.viplanner.viplanner import DATA_DIR
-
-# from viplanner.config.viplanner_sem_meta import VIPlannerSemMetaHandler
-
-
-# class VIPlannerMatterportRayCasterCamera(MatterportRayCasterCamera):
-#     def __init__(self, cfg: object):
-#         super().__init__(cfg)
-
-@configclass
-class MatterportRayCasterCameraCfg(RayCasterCameraCfg):
-    class_type = MatterportRayCasterCamera
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/unreal_importer.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/unreal_importer.py
deleted file mode 100644
index 108f321..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/unreal_importer.py
+++ /dev/null
@@ -1,379 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-import os
-from typing import TYPE_CHECKING
-
-import carb
-import numpy as np
-import omni
-import omni.isaac.core.utils.prims as prim_utils
-import omni.isaac.lab.sim as sim_utils
-import trimesh
-import yaml
-from omni.isaac.core.utils.semantics import add_update_semantics, remove_all_semantics
-from omni.isaac.lab.terrains import TerrainImporter
-from omni.isaac.lab.utils.assets import ISAAC_NUCLEUS_DIR
-from omni.isaac.lab.utils.warp import convert_to_warp_mesh
-from pxr import Gf, Usd, UsdGeom
-
-if TYPE_CHECKING:
-    from .unreal_importer_cfg import UnRealImporterCfg
-
-
-class UnRealImporter(TerrainImporter):
-    """
-    Default stairs environment for testing
-    """
-
-    cfg: UnRealImporterCfg
-
-    def __init__(self, cfg: UnRealImporterCfg) -> None:
-        """
-        :param
-        """
-        super().__init__(cfg)
-
-        # modify mesh
-        if self.cfg.cw_config_file:
-            self._multiply_crosswalks()
-
-        if self.cfg.people_config_file:
-            self._insert_people()
-
-        if self.cfg.vehicle_config_file:
-            self._insert_vehicles()
-
-        # assign semantic labels
-        if self.cfg.sem_mesh_to_class_map:
-            self._add_semantics()
-
-    """
-    Import Functions
-    """
-
-    def import_usd(self, key: str, usd_path: str):
-        """Import a mesh from a USD file.
-
-        USD file can contain arbitrary many meshes.
-
-        Note:
-            We do not apply any material properties to the mesh. The material properties should
-            be defined in the USD file.
-
-        Args:
-            key: The key to store the mesh.
-            usd_path: The path to the USD file.
-
-        Raises:
-            ValueError: If a terrain with the same key already exists.
-        """
-        # add mesh to the dict
-        if key in self.meshes:
-            raise ValueError(f"Mesh with key {key} already exists. Existing keys: {self.meshes.keys()}.")
-        # add the prim path
-        cfg = sim_utils.UsdFileCfg(usd_path=usd_path)
-
-        if self.cfg.axis_up == "Y" or self.cfg.axis_up == "y":
-            cfg.func(self.cfg.prim_path + f"/{key}", cfg, orientation=(0.707, 0.707, 0.0, 0.0))
-        else:
-            cfg.func(self.cfg.prim_path + f"/{key}", cfg)
-
-        # assign each submesh it's own geometry prim --> important for raytracing to be able to identify the submesh
-        submeshes = self.get_mesh_prims(self.cfg.prim_path + f"/{key}")
-
-        # get material
-        # physics material
-        # material = PhysicsMaterial(
-        #     "/World/PhysicsMaterial", static_friction=0.7, dynamic_friction=0.7, restitution=0
-        # )
-        for submesh, submesh_name in zip(submeshes[0], submeshes[1]):
-            #     # create geometry prim
-            #     GeometryPrim(
-            #         prim_path=submesh.GetPath().pathString,
-            #         name="collision",
-            #         position=None,
-            #         orientation=None,
-            #         collision=True,
-            #     ).apply_physics_material(material)
-            # physx_utils.setCollider(submesh, approximationShape="None")
-            # "None" will use the base triangle mesh if available
-
-            # cast into UsdGeomMesh
-            mesh_prim = UsdGeom.Mesh(submesh)
-            # store the mesh
-            vertices = np.asarray(mesh_prim.GetPointsAttr().Get())
-            faces = np.asarray(mesh_prim.GetFaceVertexIndicesAttr().Get())
-            # check if both faces and vertices are valid
-            if not vertices or not faces:
-                carb.log_warn(f"Mesh {submesh_name} has no faces or vertices.")
-                continue
-            faces = faces.reshape(-1, 3)
-            self.meshes[submesh_name] = trimesh.Trimesh(vertices=vertices, faces=faces)
-            # create a warp mesh
-            device = "cuda" if "cuda" in self.device else "cpu"
-            self.warp_meshes[submesh_name] = convert_to_warp_mesh(vertices, faces, device=device)
-
-        # add colliders and physics material
-        if self.cfg.groundplane:
-            ground_plane_cfg = sim_utils.GroundPlaneCfg(
-                physics_material=self.cfg.physics_material, size=(500, 500), visible=False
-            )
-            ground_plane = ground_plane_cfg.func("/World/GroundPlane", ground_plane_cfg, translation=(0, 0, -0.1))
-            ground_plane.visible = False
-    """ Assign Semantic Labels """
-
-    def _add_semantics(self):
-        # remove all previous semantic labels
-        remove_all_semantics(prim_utils.get_prim_at_path(self.cfg.prim_path + "/terrain"), recursive=True)
-
-        # get mesh prims
-        mesh_prims, mesh_prims_name = self.get_mesh_prims(self.cfg.prim_path + "/terrain")
-
-        carb.log_info(f"Total of {len(mesh_prims)} meshes in the scene, start assigning semantic class ...")
-
-        # mapping from prim name to class
-        with open(self.cfg.sem_mesh_to_class_map) as stream:
-            class_keywords = yaml.safe_load(stream)
-
-        # make all the string lower case
-        mesh_prims_name = [mesh_prim_single.lower() for mesh_prim_single in mesh_prims_name]
-        keywords_class_mapping_lower = {
-            key: [value_single.lower() for value_single in value] for key, value in class_keywords.items()
-        }
-
-        # assign class to mesh in ISAAC
-        def recursive_semUpdate(prim, sem_class_name: str, update_submesh: bool) -> bool:
-            # Necessary for Park Mesh
-            if (
-                prim.GetName() == "HierarchicalInstancedStaticMesh"
-            ):  # or "FoliageInstancedStaticMeshComponent" in prim.GetName():
-                add_update_semantics(prim, sem_class_name)
-                update_submesh = True
-            children = prim.GetChildren()
-            if len(children) > 0:
-                for child in children:
-                    update_submesh = recursive_semUpdate(child, sem_class_name, update_submesh)
-            return update_submesh
-
-        def recursive_meshInvestigator(mesh_idx, mesh_name, mesh_prim_list) -> bool:
-            success = False
-            for class_name, keywords in keywords_class_mapping_lower.items():
-                if any([keyword in mesh_name for keyword in keywords]):
-                    update_submesh = recursive_semUpdate(mesh_prim_list[mesh_idx], class_name, False)
-                    if not update_submesh:
-                        add_update_semantics(mesh_prim_list[mesh_idx], class_name)
-                    success = True
-                    break
-
-            if not success:
-                success_child = []
-                mesh_prims_children, mesh_prims_name_children = self.get_mesh_prims(
-                    mesh_prim_list[mesh_idx].GetPrimPath().pathString
-                )
-                mesh_prims_name_children = [mesh_prim_single.lower() for mesh_prim_single in mesh_prims_name_children]
-                for mesh_idx_child, mesh_name_child in enumerate(mesh_prims_name_children):
-                    success_child.append(
-                        recursive_meshInvestigator(mesh_idx_child, mesh_name_child, mesh_prims_children)
-                    )
-                success = any(success_child)
-
-            return success
-
-        mesh_list = []
-        for mesh_idx, mesh_name in enumerate(mesh_prims_name):
-            success = recursive_meshInvestigator(mesh_idx=mesh_idx, mesh_name=mesh_name, mesh_prim_list=mesh_prims)
-            if success:
-                mesh_list.append(mesh_idx)
-
-        missing = [i for x, y in zip(mesh_list, mesh_list[1:]) for i in range(x + 1, y) if y - x > 1]
-        assert len(mesh_list) > 0, "No mesh is assigned a semantic class!"
-        assert len(mesh_list) == len(
-            mesh_prims_name
-        ), f"Not all meshes are assigned a semantic class! Following mesh names are included yet: {[mesh_prims_name[miss_idx] for miss_idx in missing]}"
-        carb.log_info("Semantic mapping done.")
-
-        return
-
-    """ Modify Mesh """
-
-    def _multiply_crosswalks(self) -> None:
-        """Increase number of crosswalks in the scene."""
-
-        with open(self.cfg.cw_config_file) as stream:
-            multipy_cfg: dict = yaml.safe_load(stream)
-
-        # get the stage
-        stage = omni.usd.get_context().get_stage()
-
-        # get town prim
-        town_prim = multipy_cfg.pop("town_prim")
-
-        # init counter
-        crosswalk_add_counter = 0
-
-        for key, value in multipy_cfg.items():
-            print(f"Execute crosswalk multiplication '{key}'")
-
-            # iterate over the number of crosswalks to be created
-            for copy_idx in range(value["factor"]):
-                success = omni.usd.duplicate_prim(
-                    stage=stage,
-                    prim_path=os.path.join(self.cfg.prim_path + "/terrain", town_prim, value["cw_prim"]),
-                    path_to=os.path.join(
-                        self.cfg.prim_path + "/terrain",
-                        town_prim,
-                        value["cw_prim"] + f"_cp{copy_idx}" + value.get("suffix", ""),
-                    ),
-                    duplicate_layers=True,
-                )
-                assert success, f"Failed to duplicate crosswalk '{key}'"
-
-                # get crosswalk prim
-                prim = prim_utils.get_prim_at_path(
-                    os.path.join(
-                        self.cfg.prim_path + "/terrain",
-                        town_prim,
-                        value["cw_prim"] + f"_cp{copy_idx}" + value.get("suffix", ""),
-                    )
-                )
-                xform = UsdGeom.Mesh(prim).AddTranslateOp()
-                xform.Set(
-                    Gf.Vec3d(value["translation"][0], value["translation"][1], value["translation"][2]) * (copy_idx + 1)
-                )
-
-                # update counter
-                crosswalk_add_counter += 1
-
-        carb.log_info(f"Number of crosswalks added: {crosswalk_add_counter}")
-        print(f"Number of crosswalks added: {crosswalk_add_counter}")
-
-        return
-
-    def _insert_vehicles(self):
-        # load vehicle config file
-        with open(self.cfg.vehicle_config_file) as stream:
-            vehicle_cfg: dict = yaml.safe_load(stream)
-
-        # get the stage
-        stage = omni.usd.get_context().get_stage()
-
-        # get town prim and all its meshes
-        town_prim = vehicle_cfg.pop("town_prim")
-        mesh_prims: dict = prim_utils.get_prim_at_path(f"{self.cfg.prim_path}/terrain/{town_prim}").GetChildren()
-        mesh_prims_name = [mesh_prim_single.GetName() for mesh_prim_single in mesh_prims]
-
-        # car counter
-        car_add_counter = 0
-
-        for key, vehicle in vehicle_cfg.items():
-            print(f"Execute vehicle multiplication '{key}'")
-
-            # get all meshs that include the keystring
-            meshs = [
-                mesh_prim_single for mesh_prim_single in mesh_prims_name if vehicle["prim_part"] in mesh_prim_single
-            ]
-
-            # iterate over the number of vehicles to be created
-            for idx, translation in enumerate(vehicle["translation"]):
-                for single_mesh in meshs:
-                    success = omni.usd.duplicate_prim(
-                        stage=stage,
-                        prim_path=os.path.join(self.cfg.prim_path + "/terrain", town_prim, single_mesh),
-                        path_to=os.path.join(
-                            self.cfg.prim_path + "/terrain", town_prim, single_mesh + key + f"_cp{idx}"
-                        ),
-                        duplicate_layers=True,
-                    )
-                    assert success, f"Failed to duplicate vehicle '{key}'"
-
-                    prim = prim_utils.get_prim_at_path(
-                        os.path.join(self.cfg.prim_path + "/terrain", town_prim, single_mesh + key + f"_cp{idx}")
-                    )
-                    xform = UsdGeom.Mesh(prim).AddTranslateOp()
-                    xform.Set(Gf.Vec3d(translation[0], translation[1], translation[2]))
-
-                car_add_counter += 1
-
-        carb.log_info(f"Number of vehicles added: {car_add_counter}")
-        print(f"Number of vehicles added: {car_add_counter}")
-
-        return
-
-    def _insert_people(self):
-        # load people config file
-        with open(self.cfg.people_config_file) as stream:
-            people_cfg: dict = yaml.safe_load(stream)
-
-        # if self.cfg.scale == 1.0:
-        #     scale_people = 100
-        # else:
-        #     scale_people = 1
-
-        for key, person_cfg in people_cfg.items():
-            carb.log_verbose(f"Insert person '{key}'")
-
-            self.insert_single_person(
-                person_cfg["prim_name"],
-                person_cfg["translation"],
-                scale_people=1,
-                usd_path=person_cfg.get("usd_path", "People/Characters/F_Business_02/F_Business_02.usd"),
-            )
-            # TODO: movement of the people
-
-        carb.log_info(f"Number of people added: {len(people_cfg)}")
-        print(f"Number of people added: {len(people_cfg)}")
-
-        return
-
-    @staticmethod
-    def insert_single_person(
-        prim_name: str,
-        translation: list,
-        scale_people: float = 1.0,
-        usd_path: str = "People/Characters/F_Business_02/F_Business_02.usd",
-    ) -> None:
-        person_prim = prim_utils.create_prim(
-            prim_path=os.path.join("/World/People", prim_name),
-            translation=tuple(translation),
-            usd_path=os.path.join(ISAAC_NUCLEUS_DIR, usd_path),
-            scale=(scale_people, scale_people, scale_people),
-        )
-
-        if isinstance(person_prim.GetAttribute("xformOp:orient").Get(), Gf.Quatd):
-            person_prim.GetAttribute("xformOp:orient").Set(Gf.Quatd(1.0, 0.0, 0.0, 0.0))
-        else:
-            person_prim.GetAttribute("xformOp:orient").Set(Gf.Quatf(1.0, 0.0, 0.0, 0.0))
-
-        add_update_semantics(person_prim, "person")
-
-        # add collision body
-        UsdGeom.Mesh(person_prim)
-
-        return
-
-    @staticmethod
-    def get_mesh_prims(env_prim: str) -> tuple[list[Usd.Prim], list[str]]:
-        def recursive_search(start_prim: str, mesh_prims: list):
-            for curr_prim in prim_utils.get_prim_at_path(start_prim).GetChildren():
-                if curr_prim.GetTypeName() == "Xform" or curr_prim.GetTypeName() == "Mesh":
-                    mesh_prims.append(curr_prim)
-                elif curr_prim.GetTypeName() == "Scope":
-                    mesh_prims = recursive_search(start_prim=curr_prim.GetPath().pathString, mesh_prims=mesh_prims)
-
-            return mesh_prims
-
-        assert prim_utils.is_prim_path_valid(env_prim), f"Prim path '{env_prim}' is not valid"
-
-        mesh_prims = []
-        mesh_prims = recursive_search(env_prim, mesh_prims)
-
-        # mesh_prims: dict = prim_utils.get_prim_at_path(self.cfg.prim_path + "/" + self.cfg.usd_name.split(".")[0]).GetChildren()
-        mesh_prims_name = [mesh_prim_single.GetName() for mesh_prim_single in mesh_prims]
-
-        return mesh_prims, mesh_prims_name
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/unreal_importer_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/unreal_importer_cfg.py
deleted file mode 100644
index 07c3b89..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/unreal_importer_cfg.py
+++ /dev/null
@@ -1,38 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from omni.isaac.lab.terrains import TerrainImporterCfg
-from omni.isaac.lab.utils import configclass
-
-from .unreal_importer import UnRealImporter
-
-
-@configclass
-class UnRealImporterCfg(TerrainImporterCfg):
-    class_type: type = UnRealImporter
-    """The class name of the terrain importer."""
-
-    terrain_type = "usd"
-    """The type of terrain to generate. Defaults to "usd".
-
-    """
-
-    # scale
-    scale: float = 0.01  # 0.01  # carla: 0.01 nomoko: 1  park: 0.01 warehouse: 1.0 # scale the scene to be in meters
-    # up axis
-    axis_up: str = "Z"  # carla, nomoko: "Y", park, warehouse: "Z"
-    # multiply crosswalks
-    cw_config_file: str | None = None
-    # mesh to semantic class mapping --> only if set, semantic classes will be added to the scene
-    sem_mesh_to_class_map: str | None = None  # os.path.join(DATA_DIR, "park", "keyword_mapping.yml")  os.path.join(DATA_DIR, "town01", "keyword_mapping.yml")
-    # add Groundplane to the scene
-    groundplane: bool = True
-    # add people to the scene
-    people_config_file: str | None = None
-    # multiply vehicles
-    vehicle_config_file: str | None = None
-
-    groundplane: bool = True
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/viplanner_matterport_raycast_camera.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/viplanner_matterport_raycast_camera.py
deleted file mode 100644
index 01e854a..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/viplanner_matterport_raycast_camera.py
+++ /dev/null
@@ -1,23 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import torch
-import yaml
-from omni.isaac.matterport.domains import MatterportRayCasterCamera
-from omni.isaac.lab.sensors.ray_caster import RayCasterCameraCfg
-from omni.isaac.lab.utils.configclass import configclass
-from omni.isaac.viplanner.viplanner import DATA_DIR
-
-# from viplanner.config.viplanner_sem_meta import VIPlannerSemMetaHandler
-
-
-class VIPlannerMatterportRayCasterCamera(MatterportRayCasterCamera):
-    def __init__(self, cfg: object):
-        super().__init__(cfg)
-
-@configclass
-class VIPlannerMatterportRayCasterCameraCfg(RayCasterCameraCfg):
-    class_type = VIPlannerMatterportRayCasterCamera
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/visualizer.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/visualizer.py
deleted file mode 100644
index bd78a98..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/visualizer.py
+++ /dev/null
@@ -1,1405 +0,0 @@
-# Copyright (c) Facebook, Inc. and its affiliates.
-import colorsys
-import logging
-import math
-import numpy as np
-from enum import Enum, unique
-import cv2
-import matplotlib as mpl
-import matplotlib.colors as mplc
-import matplotlib.figure as mplfigure
-import pycocotools.mask as mask_util
-import torch
-from matplotlib.backends.backend_agg import FigureCanvasAgg
-from PIL import Image
-
-from detectron2.data import MetadataCatalog
-from detectron2.structures import BitMasks, Boxes, BoxMode, Keypoints, PolygonMasks, RotatedBoxes
-from detectron2.utils.file_io import PathManager
-
-from detectron2.utils.colormap import random_color
-import random
-
-logger = logging.getLogger(__name__)
-
-__all__ = ["ColorMode", "VisImage", "Visualizer"]
-
-
-_SMALL_OBJECT_AREA_THRESH = 1000
-_LARGE_MASK_AREA_THRESH = 120000
-_OFF_WHITE = (1.0, 1.0, 240.0 / 255)
-_BLACK = (0, 0, 0)
-_RED = (1.0, 0, 0)
-
-_KEYPOINT_THRESHOLD = 0.05
-
-
-@unique
-class ColorMode(Enum):
-    """
-    Enum of different color modes to use for instance visualizations.
-    """
-
-    IMAGE = 0
-    """
-    Picks a random color for every instance and overlay segmentations with low opacity.
-    """
-    SEGMENTATION = 1
-    """
-    Let instances of the same category have similar colors
-    (from metadata.thing_colors), and overlay them with
-    high opacity. This provides more attention on the quality of segmentation.
-    """
-    IMAGE_BW = 2
-    """
-    Same as IMAGE, but convert all areas without masks to gray-scale.
-    Only available for drawing per-instance mask predictions.
-    """
-
-
-class GenericMask:
-    """
-    Attribute:
-        polygons (list[ndarray]): list[ndarray]: polygons for this mask.
-            Each ndarray has format [x, y, x, y, ...]
-        mask (ndarray): a binary mask
-    """
-
-    def __init__(self, mask_or_polygons, height, width):
-        self._mask = self._polygons = self._has_holes = None
-        self.height = height
-        self.width = width
-
-        m = mask_or_polygons
-        if isinstance(m, dict):
-            # RLEs
-            assert "counts" in m and "size" in m
-            if isinstance(m["counts"], list):  # uncompressed RLEs
-                h, w = m["size"]
-                assert h == height and w == width
-                m = mask_util.frPyObjects(m, h, w)
-            self._mask = mask_util.decode(m)[:, :]
-            return
-
-        if isinstance(m, list):  # list[ndarray]
-            self._polygons = [np.asarray(x).reshape(-1) for x in m]
-            return
-
-        if isinstance(m, np.ndarray):  # assumed to be a binary mask
-            assert m.shape[1] != 2, m.shape
-            assert m.shape == (
-                height,
-                width,
-            ), f"mask shape: {m.shape}, target dims: {height}, {width}"
-            self._mask = m.astype("uint8")
-            return
-
-        raise ValueError("GenericMask cannot handle object {} of type '{}'".format(m, type(m)))
-
-    @property
-    def mask(self):
-        if self._mask is None:
-            self._mask = self.polygons_to_mask(self._polygons)
-        return self._mask
-
-    @property
-    def polygons(self):
-        if self._polygons is None:
-            self._polygons, self._has_holes = self.mask_to_polygons(self._mask)
-        return self._polygons
-
-    @property
-    def has_holes(self):
-        if self._has_holes is None:
-            if self._mask is not None:
-                self._polygons, self._has_holes = self.mask_to_polygons(self._mask)
-            else:
-                self._has_holes = False  # if original format is polygon, does not have holes
-        return self._has_holes
-
-    def mask_to_polygons(self, mask):
-        # cv2.RETR_CCOMP flag retrieves all the contours and arranges them to a 2-level
-        # hierarchy. External contours (boundary) of the object are placed in hierarchy-1.
-        # Internal contours (holes) are placed in hierarchy-2.
-        # cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.
-        mask = np.ascontiguousarray(mask)  # some versions of cv2 does not support incontiguous arr
-        res = cv2.findContours(mask.astype("uint8"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)
-        hierarchy = res[-1]
-        if hierarchy is None:  # empty mask
-            return [], False
-        has_holes = (hierarchy.reshape(-1, 4)[:, 3] >= 0).sum() > 0
-        res = res[-2]
-        res = [x.flatten() for x in res]
-        # These coordinates from OpenCV are integers in range [0, W-1 or H-1].
-        # We add 0.5 to turn them into real-value coordinate space. A better solution
-        # would be to first +0.5 and then dilate the returned polygon by 0.5.
-        res = [x + 0.5 for x in res if len(x) >= 6]
-        return res, has_holes
-
-    def polygons_to_mask(self, polygons):
-        rle = mask_util.frPyObjects(polygons, self.height, self.width)
-        rle = mask_util.merge(rle)
-        return mask_util.decode(rle)[:, :]
-
-    def area(self):
-        return self.mask.sum()
-
-    def bbox(self):
-        p = mask_util.frPyObjects(self.polygons, self.height, self.width)
-        p = mask_util.merge(p)
-        bbox = mask_util.toBbox(p)
-        bbox[2] += bbox[0]
-        bbox[3] += bbox[1]
-        return bbox
-
-
-class _PanopticPrediction:
-    """
-    Unify different panoptic annotation/prediction formats
-    """
-
-    def __init__(self, panoptic_seg, segments_info, metadata=None):
-        if segments_info is None:
-            assert metadata is not None
-            # If "segments_info" is None, we assume "panoptic_img" is a
-            # H*W int32 image storing the panoptic_id in the format of
-            # category_id * label_divisor + instance_id. We reserve -1 for
-            # VOID label.
-            label_divisor = metadata.label_divisor
-            segments_info = []
-            for panoptic_label in np.unique(panoptic_seg.numpy()):
-                if panoptic_label == -1:
-                    # VOID region.
-                    continue
-                pred_class = panoptic_label // label_divisor
-                isthing = pred_class in metadata.thing_dataset_id_to_contiguous_id.values()
-                segments_info.append(
-                    {
-                        "id": int(panoptic_label),
-                        "category_id": int(pred_class),
-                        "isthing": bool(isthing),
-                    }
-                )
-        del metadata
-
-        self._seg = panoptic_seg
-
-        self._sinfo = {s["id"]: s for s in segments_info}  # seg id -> seg info
-        segment_ids, areas = torch.unique(panoptic_seg, sorted=True, return_counts=True)
-        areas = areas.numpy()
-        sorted_idxs = np.argsort(-areas)
-        self._seg_ids, self._seg_areas = segment_ids[sorted_idxs], areas[sorted_idxs]
-        self._seg_ids = self._seg_ids.tolist()
-        for sid, area in zip(self._seg_ids, self._seg_areas):
-            if sid in self._sinfo:
-                self._sinfo[sid]["area"] = float(area)
-
-    def non_empty_mask(self):
-        """
-        Returns:
-            (H, W) array, a mask for all pixels that have a prediction
-        """
-        empty_ids = []
-        for id in self._seg_ids:
-            if id not in self._sinfo:
-                empty_ids.append(id)
-        if len(empty_ids) == 0:
-            return np.zeros(self._seg.shape, dtype=np.uint8)
-        assert (
-            len(empty_ids) == 1
-        ), ">1 ids corresponds to no labels. This is currently not supported"
-        return (self._seg != empty_ids[0]).numpy().astype(np.bool)
-
-    def semantic_masks(self):
-        for sid in self._seg_ids:
-            sinfo = self._sinfo.get(sid)
-            if sinfo is None or sinfo["isthing"]:
-                # Some pixels (e.g. id 0 in PanopticFPN) have no instance or semantic predictions.
-                continue
-            yield (self._seg == sid).numpy().astype(np.bool), sinfo
-
-    def instance_masks(self):
-        for sid in self._seg_ids:
-            sinfo = self._sinfo.get(sid)
-            if sinfo is None or not sinfo["isthing"]:
-                continue
-            mask = (self._seg == sid).numpy().astype(np.bool)
-            if mask.sum() > 0:
-                yield mask, sinfo
-
-
-def _create_text_labels(classes, scores, class_names, is_crowd=None):
-    """
-    Args:
-        classes (list[int] or None):
-        scores (list[float] or None):
-        class_names (list[str] or None):
-        is_crowd (list[bool] or None):
-
-    Returns:
-        list[str] or None
-    """
-    labels = None
-    if classes is not None:
-        if class_names is not None and len(class_names) > 0:
-            labels = [class_names[i] for i in classes]
-        else:
-            labels = [str(i) for i in classes]
-    if scores is not None:
-        if labels is None:
-            labels = ["{:.0f}%".format(s * 100) for s in scores]
-        else:
-            labels = ["{} {:.0f}%".format(l, s * 100) for l, s in zip(labels, scores)]
-    if labels is not None and is_crowd is not None:
-        labels = [l + ("|crowd" if crowd else "") for l, crowd in zip(labels, is_crowd)]
-    return labels
-
-
-class VisImage:
-    def __init__(self, img, scale=1.0):
-        """
-        Args:
-            img (ndarray): an RGB image of shape (H, W, 3) in range [0, 255].
-            scale (float): scale the input image
-        """
-        self.img = img
-        self.scale = scale
-        self.width, self.height = img.shape[1], img.shape[0]
-        self._setup_figure(img)
-
-    def _setup_figure(self, img):
-        """
-        Args:
-            Same as in :meth:`__init__()`.
-
-        Returns:
-            fig (matplotlib.pyplot.figure): top level container for all the image plot elements.
-            ax (matplotlib.pyplot.Axes): contains figure elements and sets the coordinate system.
-        """
-        fig = mplfigure.Figure(frameon=False)
-        self.dpi = fig.get_dpi()
-        # add a small 1e-2 to avoid precision lost due to matplotlib's truncation
-        # (https://github.com/matplotlib/matplotlib/issues/15363)
-        fig.set_size_inches(
-            (self.width * self.scale + 1e-2) / self.dpi,
-            (self.height * self.scale + 1e-2) / self.dpi,
-        )
-        self.canvas = FigureCanvasAgg(fig)
-        # self.canvas = mpl.backends.backend_cairo.FigureCanvasCairo(fig)
-        ax = fig.add_axes([0.0, 0.0, 1.0, 1.0])
-        ax.axis("off")
-        self.fig = fig
-        self.ax = ax
-        self.reset_image(img)
-
-    def reset_image(self, img):
-        """
-        Args:
-            img: same as in __init__
-        """
-        img = img.astype("uint8")
-        self.ax.imshow(img, extent=(0, self.width, self.height, 0), interpolation="nearest")
-
-    def save(self, filepath):
-        """
-        Args:
-            filepath (str): a string that contains the absolute path, including the file name, where
-                the visualized image will be saved.
-        """
-        self.fig.savefig(filepath)
-
-    def get_image(self):
-        """
-        Returns:
-            ndarray:
-                the visualized image of shape (H, W, 3) (RGB) in uint8 type.
-                The shape is scaled w.r.t the input image using the given `scale` argument.
-        """
-        canvas = self.canvas
-        s, (width, height) = canvas.print_to_buffer()
-        # buf = io.BytesIO()  # works for cairo backend
-        # canvas.print_rgba(buf)
-        # width, height = self.width, self.height
-        # s = buf.getvalue()
-
-        buffer = np.frombuffer(s, dtype="uint8")
-
-        img_rgba = buffer.reshape(height, width, 4)
-        rgb, alpha = np.split(img_rgba, [3], axis=2)
-        return rgb.astype("uint8")
-
-
-class Visualizer:
-    """
-    Visualizer that draws data about detection/segmentation on images.
-
-    It contains methods like `draw_{text,box,circle,line,binary_mask,polygon}`
-    that draw primitive objects to images, as well as high-level wrappers like
-    `draw_{instance_predictions,sem_seg,panoptic_seg_predictions,dataset_dict}`
-    that draw composite data in some pre-defined style.
-
-    Note that the exact visualization style for the high-level wrappers are subject to change.
-    Style such as color, opacity, label contents, visibility of labels, or even the visibility
-    of objects themselves (e.g. when the object is too small) may change according
-    to different heuristics, as long as the results still look visually reasonable.
-
-    To obtain a consistent style, you can implement custom drawing functions with the
-    abovementioned primitive methods instead. If you need more customized visualization
-    styles, you can process the data yourself following their format documented in
-    tutorials (:doc:`/tutorials/models`, :doc:`/tutorials/datasets`). This class does not
-    intend to satisfy everyone's preference on drawing styles.
-
-    This visualizer focuses on high rendering quality rather than performance. It is not
-    designed to be used for real-time applications.
-    """
-
-    # TODO implement a fast, rasterized version using OpenCV
-
-    def __init__(self, img_rgb, metadata=None, scale=1.0, instance_mode=ColorMode.IMAGE):
-        """
-        Args:
-            img_rgb: a numpy array of shape (H, W, C), where H and W correspond to
-                the height and width of the image respectively. C is the number of
-                color channels. The image is required to be in RGB format since that
-                is a requirement of the Matplotlib library. The image is also expected
-                to be in the range [0, 255].
-            metadata (Metadata): dataset metadata (e.g. class names and colors)
-            instance_mode (ColorMode): defines one of the pre-defined style for drawing
-                instances on an image.
-        """
-        self.img = np.asarray(img_rgb).clip(0, 255).astype(np.uint8)
-        if metadata is None:
-            metadata = MetadataCatalog.get("__nonexist__")
-        self.metadata = metadata
-        self.output = VisImage(self.img, scale=scale)
-        self.cpu_device = torch.device("cpu")
-
-        # too small texts are useless, therefore clamp to 9
-        self._default_font_size = max(
-            np.sqrt(self.output.height * self.output.width) // 90, 10 // scale
-        )
-        self._default_font_size = 18
-        self._instance_mode = instance_mode
-        self.keypoint_threshold = _KEYPOINT_THRESHOLD
-
-        import matplotlib.colors as mcolors
-        css4_colors = mcolors.CSS4_COLORS
-        self.color_proposals = [list(mcolors.hex2color(color)) for color in css4_colors.values()]
-
-    def draw_instance_predictions(self, predictions):
-        """
-        Draw instance-level prediction results on an image.
-
-        Args:
-            predictions (Instances): the output of an instance detection/segmentation
-                model. Following fields will be used to draw:
-                "pred_boxes", "pred_classes", "scores", "pred_masks" (or "pred_masks_rle").
-
-        Returns:
-            output (VisImage): image object with visualizations.
-        """
-        boxes = predictions.pred_boxes if predictions.has("pred_boxes") else None
-        scores = predictions.scores if predictions.has("scores") else None
-        classes = predictions.pred_classes.tolist() if predictions.has("pred_classes") else None
-        labels = _create_text_labels(classes, scores, self.metadata.get("thing_classes", None))
-        keypoints = predictions.pred_keypoints if predictions.has("pred_keypoints") else None
-
-        keep = (scores > 0.5).cpu()
-        boxes = boxes[keep]
-        scores = scores[keep]
-        classes = np.array(classes)
-        classes = classes[np.array(keep)]
-        labels = np.array(labels)
-        labels = labels[np.array(keep)]
-
-        if predictions.has("pred_masks"):
-            masks = np.asarray(predictions.pred_masks)
-            masks = masks[np.array(keep)]
-            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]
-        else:
-            masks = None
-
-        if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get("thing_colors"):
-        # if self.metadata.get("thing_colors"):
-            colors = [
-                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes
-            ]
-            alpha = 0.4
-        else:
-            colors = None
-            alpha = 0.4
-
-        if self._instance_mode == ColorMode.IMAGE_BW:
-            self.output.reset_image(
-                self._create_grayscale_image(
-                    (predictions.pred_masks.any(dim=0) > 0).numpy()
-                    if predictions.has("pred_masks")
-                    else None
-                )
-            )
-            alpha = 0.3
-        
-        self.overlay_instances(
-            masks=masks,
-            boxes=boxes,
-            labels=labels,
-            keypoints=keypoints,
-            assigned_colors=colors,
-            alpha=alpha,
-        )
-        return self.output
-
-    def draw_sem_seg(self, sem_seg, area_threshold=None, alpha=0.7):
-        """
-        Draw semantic segmentation predictions/labels.
-
-        Args:
-            sem_seg (Tensor or ndarray): the segmentation of shape (H, W).
-                Each value is the integer label of the pixel.
-            area_threshold (int): segments with less than `area_threshold` are not drawn.
-            alpha (float): the larger it is, the more opaque the segmentations are.
-
-        Returns:
-            output (VisImage): image object with visualizations.
-        """
-        if isinstance(sem_seg, torch.Tensor):
-            sem_seg = sem_seg.numpy()
-        labels, areas = np.unique(sem_seg, return_counts=True)
-        sorted_idxs = np.argsort(-areas).tolist()
-        labels = labels[sorted_idxs]
-        for label in filter(lambda l: l < len(self.metadata.stuff_classes), labels):
-            try:
-                mask_color = [x / 255 for x in self.metadata.stuff_colors[label]]
-            except (AttributeError, IndexError):
-                mask_color = None
-
-            binary_mask = (sem_seg == label).astype(np.uint8)
-            text = self.metadata.stuff_classes[label]
-            self.draw_binary_mask(
-                binary_mask,
-                color=mask_color,
-                edge_color=_OFF_WHITE,
-                text=text,
-                alpha=alpha,
-                area_threshold=area_threshold,
-            )
-        return self.output
-
-    def draw_panoptic_seg(self, panoptic_seg, segments_info, area_threshold=None, alpha=0.7):
-        """
-        Draw panoptic prediction annotations or results.
-
-        Args:
-            panoptic_seg (Tensor): of shape (height, width) where the values are ids for each
-                segment.
-            segments_info (list[dict] or None): Describe each segment in `panoptic_seg`.
-                If it is a ``list[dict]``, each dict contains keys "id", "category_id".
-                If None, category id of each pixel is computed by
-                ``pixel // metadata.label_divisor``.
-            area_threshold (int): stuff segments with less than `area_threshold` are not drawn.
-
-        Returns:
-            output (VisImage): image object with visualizations.
-        """
-        pred = _PanopticPrediction(panoptic_seg, segments_info, self.metadata)
-
-        if self._instance_mode == ColorMode.IMAGE_BW:
-            self.output.reset_image(self._create_grayscale_image(pred.non_empty_mask()))
-
-        # draw mask for all semantic segments first i.e. "stuff"
-        for mask, sinfo in pred.semantic_masks():
-            category_idx = sinfo["category_id"]
-            try:
-                mask_color = [x / 255 for x in self.metadata.stuff_colors[category_idx]]
-            except AttributeError:
-                mask_color = None
-
-            text = self.metadata.stuff_classes[category_idx].replace('-other','').replace('-merged','')
-            self.draw_binary_mask(
-                mask,
-                color=mask_color,
-                edge_color=_OFF_WHITE,
-                text=text,
-                alpha=alpha,
-                area_threshold=area_threshold,
-            )
-
-        # draw mask for all instances second
-        all_instances = list(pred.instance_masks())
-        if len(all_instances) == 0:
-            return self.output
-        masks, sinfo = list(zip(*all_instances))
-        category_ids = [x["category_id"] for x in sinfo]
-
-        try:
-            scores = [x["score"] for x in sinfo]
-        except KeyError:
-            scores = None
-        class_names = [name.replace('-other','').replace('-merged','') for name in self.metadata.thing_classes]
-        labels = _create_text_labels(
-            category_ids, scores, class_names, [x.get("iscrowd", 0) for x in sinfo]
-        )
-
-        try:
-            colors = [
-                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in category_ids
-            ]
-        except AttributeError:
-            colors = None
-        self.overlay_instances(masks=masks, labels=labels, assigned_colors=colors, alpha=alpha)
-
-        return self.output
-
-    draw_panoptic_seg_predictions = draw_panoptic_seg  # backward compatibility
-
-    def draw_dataset_dict(self, dic):
-        """
-        Draw annotations/segmentaions in Detectron2 Dataset format.
-
-        Args:
-            dic (dict): annotation/segmentation data of one image, in Detectron2 Dataset format.
-
-        Returns:
-            output (VisImage): image object with visualizations.
-        """
-        annos = dic.get("annotations", None)
-        if annos:
-            if "segmentation" in annos[0]:
-                masks = [x["segmentation"] for x in annos]
-            else:
-                masks = None
-            if "keypoints" in annos[0]:
-                keypts = [x["keypoints"] for x in annos]
-                keypts = np.array(keypts).reshape(len(annos), -1, 3)
-            else:
-                keypts = None
-
-            boxes = [
-                BoxMode.convert(x["bbox"], x["bbox_mode"], BoxMode.XYXY_ABS)
-                if len(x["bbox"]) == 4
-                else x["bbox"]
-                for x in annos
-            ]
-
-            colors = None
-            category_ids = [x["category_id"] for x in annos]
-            if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get("thing_colors"):
-                colors = [
-                    self._jitter([x / 255 for x in self.metadata.thing_colors[c]])
-                    for c in category_ids
-                ]
-            names = self.metadata.get("thing_classes", None)
-            labels = _create_text_labels(
-                category_ids,
-                scores=None,
-                class_names=names,
-                is_crowd=[x.get("iscrowd", 0) for x in annos],
-            )
-            self.overlay_instances(
-                labels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors
-            )
-
-        sem_seg = dic.get("sem_seg", None)
-        if sem_seg is None and "sem_seg_file_name" in dic:
-            with PathManager.open(dic["sem_seg_file_name"], "rb") as f:
-                sem_seg = Image.open(f)
-                sem_seg = np.asarray(sem_seg, dtype="uint8")
-        if sem_seg is not None:
-            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.4)
-
-        pan_seg = dic.get("pan_seg", None)
-        if pan_seg is None and "pan_seg_file_name" in dic:
-            with PathManager.open(dic["pan_seg_file_name"], "rb") as f:
-                pan_seg = Image.open(f)
-                pan_seg = np.asarray(pan_seg)
-                from panopticapi.utils import rgb2id
-
-                pan_seg = rgb2id(pan_seg)
-        if pan_seg is not None:
-            segments_info = dic["segments_info"]
-            pan_seg = torch.tensor(pan_seg)
-            self.draw_panoptic_seg(pan_seg, segments_info, area_threshold=0, alpha=0.7)
-        return self.output
-
-    def overlay_instances(
-        self,
-        *,
-        boxes=None,
-        labels=None,
-        masks=None,
-        keypoints=None,
-        assigned_colors=None,
-        alpha=0.5,
-    ):
-        """
-        Args:
-            boxes (Boxes, RotatedBoxes or ndarray): either a :class:`Boxes`,
-                or an Nx4 numpy array of XYXY_ABS format for the N objects in a single image,
-                or a :class:`RotatedBoxes`,
-                or an Nx5 numpy array of (x_center, y_center, width, height, angle_degrees) format
-                for the N objects in a single image,
-            labels (list[str]): the text to be displayed for each instance.
-            masks (masks-like object): Supported types are:
-
-                * :class:`detectron2.structures.PolygonMasks`,
-                  :class:`detectron2.structures.BitMasks`.
-                * list[list[ndarray]]: contains the segmentation masks for all objects in one image.
-                  The first level of the list corresponds to individual instances. The second
-                  level to all the polygon that compose the instance, and the third level
-                  to the polygon coordinates. The third level should have the format of
-                  [x0, y0, x1, y1, ..., xn, yn] (n >= 3).
-                * list[ndarray]: each ndarray is a binary mask of shape (H, W).
-                * list[dict]: each dict is a COCO-style RLE.
-            keypoints (Keypoint or array like): an array-like object of shape (N, K, 3),
-                where the N is the number of instances and K is the number of keypoints.
-                The last dimension corresponds to (x, y, visibility or score).
-            assigned_colors (list[matplotlib.colors]): a list of colors, where each color
-                corresponds to each mask or box in the image. Refer to 'matplotlib.colors'
-                for full list of formats that the colors are accepted in.
-        Returns:
-            output (VisImage): image object with visualizations.
-        """
-        num_instances = 0
-        if boxes is not None:
-            boxes = self._convert_boxes(boxes)
-            num_instances = len(boxes)
-        if masks is not None:
-            masks = self._convert_masks(masks)
-            if num_instances:
-                assert len(masks) == num_instances
-            else:
-                num_instances = len(masks)
-        if keypoints is not None:
-            if num_instances:
-                assert len(keypoints) == num_instances
-            else:
-                num_instances = len(keypoints)
-            keypoints = self._convert_keypoints(keypoints)
-        if labels is not None:
-            assert len(labels) == num_instances
-        if assigned_colors is None:
-            assigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]
-        if num_instances == 0:
-            return self.output
-        if boxes is not None and boxes.shape[1] == 5:
-            return self.overlay_rotated_instances(
-                boxes=boxes, labels=labels, assigned_colors=assigned_colors
-            )
-
-        # Display in largest to smallest order to reduce occlusion.
-        areas = None
-        if boxes is not None:
-            areas = np.prod(boxes[:, 2:] - boxes[:, :2], axis=1)
-        elif masks is not None:
-            areas = np.asarray([x.area() for x in masks])
-
-        if areas is not None:
-            sorted_idxs = np.argsort(-areas).tolist()
-            # Re-order overlapped instances in descending order.
-            boxes = boxes[sorted_idxs] if boxes is not None else None
-            labels = [labels[k] for k in sorted_idxs] if labels is not None else None
-            masks = [masks[idx] for idx in sorted_idxs] if masks is not None else None
-            assigned_colors = [assigned_colors[idx] for idx in sorted_idxs]
-            keypoints = keypoints[sorted_idxs] if keypoints is not None else None
-
-        for i in range(num_instances):
-            color = assigned_colors[i]
-            if boxes is not None:
-                self.draw_box(boxes[i], edge_color=color)
-
-            if masks is not None:
-                for segment in masks[i].polygons:
-                    self.draw_polygon(segment.reshape(-1, 2), color, alpha=alpha)
-
-            if labels is not None:
-                # first get a box
-                if boxes is not None:
-                    x0, y0, x1, y1 = boxes[i]
-                    text_pos = (x0, y0)  # if drawing boxes, put text on the box corner.
-                    horiz_align = "left"
-                elif masks is not None:
-                    # skip small mask without polygon
-                    if len(masks[i].polygons) == 0:
-                        continue
-
-                    x0, y0, x1, y1 = masks[i].bbox()
-
-                    # draw text in the center (defined by median) when box is not drawn
-                    # median is less sensitive to outliers.
-                    text_pos = np.median(masks[i].mask.nonzero(), axis=1)[::-1]
-                    horiz_align = "center"
-                else:
-                    continue  # drawing the box confidence for keypoints isn't very useful.
-                # for small objects, draw text at the side to avoid occlusion
-                instance_area = (y1 - y0) * (x1 - x0)
-                if (
-                    instance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale
-                    or y1 - y0 < 40 * self.output.scale
-                ):
-                    if y1 >= self.output.height - 5:
-                        text_pos = (x1, y0)
-                    else:
-                        text_pos = (x0, y1)
-
-                height_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)
-                lighter_color = self._change_color_brightness(color, brightness_factor=0.7)
-                font_size = (
-                    np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)
-                    * 0.5
-                    * self._default_font_size
-                )
-                self.draw_text(
-                    labels[i],
-                    text_pos,
-                    color=lighter_color,
-                    horizontal_alignment=horiz_align,
-                    font_size=font_size,
-                )
-
-        # draw keypoints
-        if keypoints is not None:
-            for keypoints_per_instance in keypoints:
-                self.draw_and_connect_keypoints(keypoints_per_instance)
-
-        return self.output
-
-    def overlay_rotated_instances(self, boxes=None, labels=None, assigned_colors=None):
-        """
-        Args:
-            boxes (ndarray): an Nx5 numpy array of
-                (x_center, y_center, width, height, angle_degrees) format
-                for the N objects in a single image.
-            labels (list[str]): the text to be displayed for each instance.
-            assigned_colors (list[matplotlib.colors]): a list of colors, where each color
-                corresponds to each mask or box in the image. Refer to 'matplotlib.colors'
-                for full list of formats that the colors are accepted in.
-
-        Returns:
-            output (VisImage): image object with visualizations.
-        """
-        num_instances = len(boxes)
-
-        if assigned_colors is None:
-            assigned_colors = [random_color(rgb=True, maximum=1) for _ in range(num_instances)]
-        if num_instances == 0:
-            return self.output
-
-        # Display in largest to smallest order to reduce occlusion.
-        if boxes is not None:
-            areas = boxes[:, 2] * boxes[:, 3]
-
-        sorted_idxs = np.argsort(-areas).tolist()
-        # Re-order overlapped instances in descending order.
-        boxes = boxes[sorted_idxs]
-        labels = [labels[k] for k in sorted_idxs] if labels is not None else None
-        colors = [assigned_colors[idx] for idx in sorted_idxs]
-
-        for i in range(num_instances):
-            self.draw_rotated_box_with_label(
-                boxes[i], edge_color=colors[i], label=labels[i] if labels is not None else None
-            )
-
-        return self.output
-
-    def draw_and_connect_keypoints(self, keypoints):
-        """
-        Draws keypoints of an instance and follows the rules for keypoint connections
-        to draw lines between appropriate keypoints. This follows color heuristics for
-        line color.
-
-        Args:
-            keypoints (Tensor): a tensor of shape (K, 3), where K is the number of keypoints
-                and the last dimension corresponds to (x, y, probability).
-
-        Returns:
-            output (VisImage): image object with visualizations.
-        """
-        visible = {}
-        keypoint_names = self.metadata.get("keypoint_names")
-        for idx, keypoint in enumerate(keypoints):
-
-            # draw keypoint
-            x, y, prob = keypoint
-            if prob > self.keypoint_threshold:
-                self.draw_circle((x, y), color=_RED)
-                if keypoint_names:
-                    keypoint_name = keypoint_names[idx]
-                    visible[keypoint_name] = (x, y)
-
-        if self.metadata.get("keypoint_connection_rules"):
-            for kp0, kp1, color in self.metadata.keypoint_connection_rules:
-                if kp0 in visible and kp1 in visible:
-                    x0, y0 = visible[kp0]
-                    x1, y1 = visible[kp1]
-                    color = tuple(x / 255.0 for x in color)
-                    self.draw_line([x0, x1], [y0, y1], color=color)
-
-        # draw lines from nose to mid-shoulder and mid-shoulder to mid-hip
-        # Note that this strategy is specific to person keypoints.
-        # For other keypoints, it should just do nothing
-        try:
-            ls_x, ls_y = visible["left_shoulder"]
-            rs_x, rs_y = visible["right_shoulder"]
-            mid_shoulder_x, mid_shoulder_y = (ls_x + rs_x) / 2, (ls_y + rs_y) / 2
-        except KeyError:
-            pass
-        else:
-            # draw line from nose to mid-shoulder
-            nose_x, nose_y = visible.get("nose", (None, None))
-            if nose_x is not None:
-                self.draw_line([nose_x, mid_shoulder_x], [nose_y, mid_shoulder_y], color=_RED)
-
-            try:
-                # draw line from mid-shoulder to mid-hip
-                lh_x, lh_y = visible["left_hip"]
-                rh_x, rh_y = visible["right_hip"]
-            except KeyError:
-                pass
-            else:
-                mid_hip_x, mid_hip_y = (lh_x + rh_x) / 2, (lh_y + rh_y) / 2
-                self.draw_line([mid_hip_x, mid_shoulder_x], [mid_hip_y, mid_shoulder_y], color=_RED)
-        return self.output
-
-    """
-    Primitive drawing functions:
-    """
-
-    def draw_text(
-        self,
-        text,
-        position,
-        *,
-        font_size=None,
-        color="g",
-        horizontal_alignment="center",
-        rotation=0,
-    ):
-        """
-        Args:
-            text (str): class label
-            position (tuple): a tuple of the x and y coordinates to place text on image.
-            font_size (int, optional): font of the text. If not provided, a font size
-                proportional to the image width is calculated and used.
-            color: color of the text. Refer to `matplotlib.colors` for full list
-                of formats that are accepted.
-            horizontal_alignment (str): see `matplotlib.text.Text`
-            rotation: rotation angle in degrees CCW
-
-        Returns:
-            output (VisImage): image object with text drawn.
-        """
-        if not font_size:
-            font_size = self._default_font_size
-
-        # since the text background is dark, we don't want the text to be dark
-        color = np.maximum(list(mplc.to_rgb(color)), 0.15)
-        color[np.argmax(color)] = max(0.8, np.max(color))
-
-        def contrasting_color(rgb):
-            """Returns 'white' or 'black' depending on which color contrasts more with the given RGB value."""
-            
-            # Decompose the RGB tuple
-            R, G, B = rgb
-
-            # Calculate the Y value
-            Y = 0.299 * R + 0.587 * G + 0.114 * B
-
-            # If Y value is greater than 128, it's closer to white so return black. Otherwise, return white.
-            return 'black' if Y > 128 else 'white'
-
-        bbox_background = contrasting_color(color*255)
-
-        x, y = position
-        self.output.ax.text(
-            x,
-            y,
-            text,
-            size=font_size * self.output.scale,
-            family="sans-serif",
-            bbox={"facecolor": bbox_background, "alpha": 0.8, "pad": 0.7, "edgecolor": "none"},
-            verticalalignment="top",
-            horizontalalignment=horizontal_alignment,
-            color=color,
-            zorder=10,
-            rotation=rotation,
-        )
-        return self.output
-
-    def draw_box(self, box_coord, alpha=0.5, edge_color="g", line_style="-"):
-        """
-        Args:
-            box_coord (tuple): a tuple containing x0, y0, x1, y1 coordinates, where x0 and y0
-                are the coordinates of the image's top left corner. x1 and y1 are the
-                coordinates of the image's bottom right corner.
-            alpha (float): blending efficient. Smaller values lead to more transparent masks.
-            edge_color: color of the outline of the box. Refer to `matplotlib.colors`
-                for full list of formats that are accepted.
-            line_style (string): the string to use to create the outline of the boxes.
-
-        Returns:
-            output (VisImage): image object with box drawn.
-        """
-        x0, y0, x1, y1 = box_coord
-        width = x1 - x0
-        height = y1 - y0
-
-        linewidth = max(self._default_font_size / 12, 1)
-
-        self.output.ax.add_patch(
-            mpl.patches.Rectangle(
-                (x0, y0),
-                width,
-                height,
-                fill=False,
-                edgecolor=edge_color,
-                linewidth=linewidth * self.output.scale,
-                alpha=alpha,
-                linestyle=line_style,
-            )
-        )
-        return self.output
-
-    def draw_rotated_box_with_label(
-        self, rotated_box, alpha=0.5, edge_color="g", line_style="-", label=None
-    ):
-        """
-        Draw a rotated box with label on its top-left corner.
-
-        Args:
-            rotated_box (tuple): a tuple containing (cnt_x, cnt_y, w, h, angle),
-                where cnt_x and cnt_y are the center coordinates of the box.
-                w and h are the width and height of the box. angle represents how
-                many degrees the box is rotated CCW with regard to the 0-degree box.
-            alpha (float): blending efficient. Smaller values lead to more transparent masks.
-            edge_color: color of the outline of the box. Refer to `matplotlib.colors`
-                for full list of formats that are accepted.
-            line_style (string): the string to use to create the outline of the boxes.
-            label (string): label for rotated box. It will not be rendered when set to None.
-
-        Returns:
-            output (VisImage): image object with box drawn.
-        """
-        cnt_x, cnt_y, w, h, angle = rotated_box
-        area = w * h
-        # use thinner lines when the box is small
-        linewidth = self._default_font_size / (
-            6 if area < _SMALL_OBJECT_AREA_THRESH * self.output.scale else 3
-        )
-
-        theta = angle * math.pi / 180.0
-        c = math.cos(theta)
-        s = math.sin(theta)
-        rect = [(-w / 2, h / 2), (-w / 2, -h / 2), (w / 2, -h / 2), (w / 2, h / 2)]
-        # x: left->right ; y: top->down
-        rotated_rect = [(s * yy + c * xx + cnt_x, c * yy - s * xx + cnt_y) for (xx, yy) in rect]
-        for k in range(4):
-            j = (k + 1) % 4
-            self.draw_line(
-                [rotated_rect[k][0], rotated_rect[j][0]],
-                [rotated_rect[k][1], rotated_rect[j][1]],
-                color=edge_color,
-                linestyle="--" if k == 1 else line_style,
-                linewidth=linewidth,
-            )
-
-        if label is not None:
-            text_pos = rotated_rect[1]  # topleft corner
-
-            height_ratio = h / np.sqrt(self.output.height * self.output.width)
-            label_color = self._change_color_brightness(edge_color, brightness_factor=0.7)
-            font_size = (
-                np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2) * 0.5 * self._default_font_size
-            )
-            self.draw_text(label, text_pos, color=label_color, font_size=font_size, rotation=angle)
-
-        return self.output
-
-    def draw_circle(self, circle_coord, color, radius=3):
-        """
-        Args:
-            circle_coord (list(int) or tuple(int)): contains the x and y coordinates
-                of the center of the circle.
-            color: color of the polygon. Refer to `matplotlib.colors` for a full list of
-                formats that are accepted.
-            radius (int): radius of the circle.
-
-        Returns:
-            output (VisImage): image object with box drawn.
-        """
-        x, y = circle_coord
-        self.output.ax.add_patch(
-            mpl.patches.Circle(circle_coord, radius=radius, fill=True, color=color)
-        )
-        return self.output
-
-    def draw_line(self, x_data, y_data, color, linestyle="-", linewidth=None):
-        """
-        Args:
-            x_data (list[int]): a list containing x values of all the points being drawn.
-                Length of list should match the length of y_data.
-            y_data (list[int]): a list containing y values of all the points being drawn.
-                Length of list should match the length of x_data.
-            color: color of the line. Refer to `matplotlib.colors` for a full list of
-                formats that are accepted.
-            linestyle: style of the line. Refer to `matplotlib.lines.Line2D`
-                for a full list of formats that are accepted.
-            linewidth (float or None): width of the line. When it's None,
-                a default value will be computed and used.
-
-        Returns:
-            output (VisImage): image object with line drawn.
-        """
-        if linewidth is None:
-            linewidth = self._default_font_size / 3
-        linewidth = max(linewidth, 1)
-        self.output.ax.add_line(
-            mpl.lines.Line2D(
-                x_data,
-                y_data,
-                linewidth=linewidth * self.output.scale,
-                color=color,
-                linestyle=linestyle,
-            )
-        )
-        return self.output
-
-    def draw_binary_mask(
-        self, binary_mask, color=None, *, edge_color=None, text=None, alpha=0.7, area_threshold=10
-    ):
-        """
-        Args:
-            binary_mask (ndarray): numpy array of shape (H, W), where H is the image height and
-                W is the image width. Each value in the array is either a 0 or 1 value of uint8
-                type.
-            color: color of the mask. Refer to `matplotlib.colors` for a full list of
-                formats that are accepted. If None, will pick a random color.
-            edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a
-                full list of formats that are accepted.
-            text (str): if None, will be drawn on the object
-            alpha (float): blending efficient. Smaller values lead to more transparent masks.
-            area_threshold (float): a connected component smaller than this area will not be shown.
-
-        Returns:
-            output (VisImage): image object with mask drawn.
-        """
-        if color is None:
-            color = random_color(rgb=True, maximum=1)
-        color = mplc.to_rgb(color)
-
-        has_valid_segment = False
-        binary_mask = binary_mask.astype("uint8")  # opencv needs uint8
-        mask = GenericMask(binary_mask, self.output.height, self.output.width)
-        shape2d = (binary_mask.shape[0], binary_mask.shape[1])
-
-        if not mask.has_holes:
-            # draw polygons for regular masks
-            for segment in mask.polygons:
-                area = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))
-                if area < (area_threshold or 0):
-                    continue
-                has_valid_segment = True
-                segment = segment.reshape(-1, 2)
-                self.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)
-        else:
-            # TODO: Use Path/PathPatch to draw vector graphics:
-            # https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon
-            rgba = np.zeros(shape2d + (4,), dtype="float32")
-            rgba[:, :, :3] = color
-            rgba[:, :, 3] = (mask.mask == 1).astype("float32") * alpha
-            has_valid_segment = True
-            self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))
-
-        if text is not None and has_valid_segment:
-            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)
-            self._draw_text_in_mask(binary_mask, text, lighter_color)
-        return self.output
-    
-    def draw_binary_mask_with_number(
-        self, binary_mask, color=None, *, edge_color=None, text=None, label_mode='1', alpha=0.1, anno_mode=['Mask'], area_threshold=10
-    ):
-        """
-        Args:
-            binary_mask (ndarray): numpy array of shape (H, W), where H is the image height and
-                W is the image width. Each value in the array is either a 0 or 1 value of uint8
-                type.
-            color: color of the mask. Refer to `matplotlib.colors` for a full list of
-                formats that are accepted. If None, will pick a random color.
-            edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a
-                full list of formats that are accepted.
-            text (str): if None, will be drawn on the object
-            alpha (float): blending efficient. Smaller values lead to more transparent masks.
-            area_threshold (float): a connected component smaller than this area will not be shown.
-
-        Returns:
-            output (VisImage): image object with mask drawn.
-        """
-        if color is None:
-            randint = random.randint(0, len(self.color_proposals)-1)
-            color = self.color_proposals[randint]
-        color = mplc.to_rgb(color)
-
-        has_valid_segment = True
-        binary_mask = binary_mask.astype("uint8")  # opencv needs uint8
-        mask = GenericMask(binary_mask, self.output.height, self.output.width)
-        shape2d = (binary_mask.shape[0], binary_mask.shape[1])
-        bbox = mask.bbox()
-
-        if 'Mask' in anno_mode:
-            if not mask.has_holes:
-                # draw polygons for regular masks
-                for segment in mask.polygons:
-                    area = mask_util.area(mask_util.frPyObjects([segment], shape2d[0], shape2d[1]))
-                    if area < (area_threshold or 0):
-                        continue
-                    has_valid_segment = True
-                    segment = segment.reshape(-1, 2)
-                    self.draw_polygon(segment, color=color, edge_color=edge_color, alpha=alpha)
-            else:
-                # TODO: Use Path/PathPatch to draw vector graphics:
-                # https://stackoverflow.com/questions/8919719/how-to-plot-a-complex-polygon
-                rgba = np.zeros(shape2d + (4,), dtype="float32")
-                rgba[:, :, :3] = color
-                rgba[:, :, 3] = (mask.mask == 1).astype("float32") * alpha
-                has_valid_segment = True
-                self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))
-
-        if 'Box' in anno_mode:
-            self.draw_box(bbox, edge_color=color, alpha=0.75)
-
-        if 'Mark' in anno_mode:
-            has_valid_segment = True
-        else:
-            has_valid_segment = False
-
-        if text is not None and has_valid_segment:
-            # lighter_color = tuple([x*0.2 for x in color])
-            lighter_color = [1,1,1] # self._change_color_brightness(color, brightness_factor=0.7)
-            self._draw_number_in_mask(binary_mask, text, lighter_color, label_mode)
-        return self.output
-
-    def draw_soft_mask(self, soft_mask, color=None, *, text=None, alpha=0.5):
-        """
-        Args:
-            soft_mask (ndarray): float array of shape (H, W), each value in [0, 1].
-            color: color of the mask. Refer to `matplotlib.colors` for a full list of
-                formats that are accepted. If None, will pick a random color.
-            text (str): if None, will be drawn on the object
-            alpha (float): blending efficient. Smaller values lead to more transparent masks.
-
-        Returns:
-            output (VisImage): image object with mask drawn.
-        """
-        if color is None:
-            color = random_color(rgb=True, maximum=1)
-        color = mplc.to_rgb(color)
-
-        shape2d = (soft_mask.shape[0], soft_mask.shape[1])
-        rgba = np.zeros(shape2d + (4,), dtype="float32")
-        rgba[:, :, :3] = color
-        rgba[:, :, 3] = soft_mask * alpha
-        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))
-
-        if text is not None:
-            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)
-            binary_mask = (soft_mask > 0.5).astype("uint8")
-            self._draw_text_in_mask(binary_mask, text, lighter_color)
-        return self.output
-
-    def draw_polygon(self, segment, color, edge_color=None, alpha=0.5):
-        """
-        Args:
-            segment: numpy array of shape Nx2, containing all the points in the polygon.
-            color: color of the polygon. Refer to `matplotlib.colors` for a full list of
-                formats that are accepted.
-            edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a
-                full list of formats that are accepted. If not provided, a darker shade
-                of the polygon color will be used instead.
-            alpha (float): blending efficient. Smaller values lead to more transparent masks.
-
-        Returns:
-            output (VisImage): image object with polygon drawn.
-        """
-        if edge_color is None:
-            # make edge color darker than the polygon color
-            if alpha > 0.8:
-                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)
-            else:
-                edge_color = color
-        edge_color = mplc.to_rgb(edge_color) + (1,)
-
-        polygon = mpl.patches.Polygon(
-            segment,
-            fill=True,
-            facecolor=mplc.to_rgb(color) + (alpha,),
-            edgecolor=edge_color,
-            linewidth=max(self._default_font_size // 15 * self.output.scale, 1),
-        )
-        self.output.ax.add_patch(polygon)
-        return self.output
-
-    """
-    Internal methods:
-    """
-
-    def _jitter(self, color):
-        """
-        Randomly modifies given color to produce a slightly different color than the color given.
-
-        Args:
-            color (tuple[double]): a tuple of 3 elements, containing the RGB values of the color
-                picked. The values in the list are in the [0.0, 1.0] range.
-
-        Returns:
-            jittered_color (tuple[double]): a tuple of 3 elements, containing the RGB values of the
-                color after being jittered. The values in the list are in the [0.0, 1.0] range.
-        """
-        color = mplc.to_rgb(color)
-        # np.random.seed(0)
-        vec = np.random.rand(3)
-        # better to do it in another color space
-        vec = vec / np.linalg.norm(vec) * 0.5
-        res = np.clip(vec + color, 0, 1)
-        return tuple(res)
-
-    def _create_grayscale_image(self, mask=None):
-        """
-        Create a grayscale version of the original image.
-        The colors in masked area, if given, will be kept.
-        """
-        img_bw = self.img.astype("f4").mean(axis=2)
-        img_bw = np.stack([img_bw] * 3, axis=2)
-        if mask is not None:
-            img_bw[mask] = self.img[mask]
-        return img_bw
-
-    def _change_color_brightness(self, color, brightness_factor):
-        """
-        Depending on the brightness_factor, gives a lighter or darker color i.e. a color with
-        less or more saturation than the original color.
-
-        Args:
-            color: color of the polygon. Refer to `matplotlib.colors` for a full list of
-                formats that are accepted.
-            brightness_factor (float): a value in [-1.0, 1.0] range. A lightness factor of
-                0 will correspond to no change, a factor in [-1.0, 0) range will result in
-                a darker color and a factor in (0, 1.0] range will result in a lighter color.
-
-        Returns:
-            modified_color (tuple[double]): a tuple containing the RGB values of the
-                modified color. Each value in the tuple is in the [0.0, 1.0] range.
-        """
-        assert brightness_factor >= -1.0 and brightness_factor <= 1.0
-        color = mplc.to_rgb(color)
-        polygon_color = colorsys.rgb_to_hls(*mplc.to_rgb(color))
-        modified_lightness = polygon_color[1] + (brightness_factor * polygon_color[1])
-        modified_lightness = 0.0 if modified_lightness < 0.0 else modified_lightness
-        modified_lightness = 1.0 if modified_lightness > 1.0 else modified_lightness
-        modified_color = colorsys.hls_to_rgb(polygon_color[0], modified_lightness, polygon_color[2])
-        return modified_color
-
-    def _convert_boxes(self, boxes):
-        """
-        Convert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.
-        """
-        if isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):
-            return boxes.tensor.detach().numpy()
-        else:
-            return np.asarray(boxes)
-
-    def _convert_masks(self, masks_or_polygons):
-        """
-        Convert different format of masks or polygons to a tuple of masks and polygons.
-
-        Returns:
-            list[GenericMask]:
-        """
-
-        m = masks_or_polygons
-        if isinstance(m, PolygonMasks):
-            m = m.polygons
-        if isinstance(m, BitMasks):
-            m = m.tensor.numpy()
-        if isinstance(m, torch.Tensor):
-            m = m.numpy()
-        ret = []
-        for x in m:
-            if isinstance(x, GenericMask):
-                ret.append(x)
-            else:
-                ret.append(GenericMask(x, self.output.height, self.output.width))
-        return ret
-
-    def _draw_number_in_mask(self, binary_mask, text, color, label_mode='1'):
-        """
-        Find proper places to draw text given a binary mask.
-        """
-
-        def number_to_string(n):
-            chars = []
-            while n:
-                n, remainder = divmod(n-1, 26)
-                chars.append(chr(97 + remainder))
-            return ''.join(reversed(chars))
-
-        binary_mask = np.pad(binary_mask, ((1, 1), (1, 1)), 'constant')
-        mask_dt = cv2.distanceTransform(binary_mask, cv2.DIST_L2, 0)
-        mask_dt = mask_dt[1:-1, 1:-1]
-        max_dist = np.max(mask_dt)
-        coords_y, coords_x = np.where(mask_dt == max_dist)  # coords is [y, x]
-
-        if label_mode == 'a':
-            text = number_to_string(int(text))
-        else:
-            text = text
-
-        self.draw_text(text, (coords_x[len(coords_x)//2] + 2, coords_y[len(coords_y)//2] - 6), color=color)
-
-        # TODO sometimes drawn on wrong objects. the heuristics here can improve.
-        # _num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, 8)
-        # if stats[1:, -1].size == 0:
-        #     return
-        # largest_component_id = np.argmax(stats[1:, -1]) + 1
-
-        # # draw text on the largest component, as well as other very large components.
-        # for cid in range(1, _num_cc):
-        #     if cid == largest_component_id or stats[cid, -1] > _LARGE_MASK_AREA_THRESH:
-        #         # median is more stable than centroid
-        #         # center = centroids[largest_component_id]
-        #         center = np.median((cc_labels == cid).nonzero(), axis=1)[::-1]
-        #         # bottom=np.max((cc_labels == cid).nonzero(), axis=1)[::-1]
-        #         # center[1]=bottom[1]+2
-        #         self.draw_text(text, center, color=color)
-    
-    def _draw_text_in_mask(self, binary_mask, text, color):
-        """
-        Find proper places to draw text given a binary mask.
-        """
-        # TODO sometimes drawn on wrong objects. the heuristics here can improve.
-        _num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, 8)
-        if stats[1:, -1].size == 0:
-            return
-        largest_component_id = np.argmax(stats[1:, -1]) + 1
-
-        # draw text on the largest component, as well as other very large components.
-        for cid in range(1, _num_cc):
-            if cid == largest_component_id or stats[cid, -1] > _LARGE_MASK_AREA_THRESH:
-                # median is more stable than centroid
-                # center = centroids[largest_component_id]
-                center = np.median((cc_labels == cid).nonzero(), axis=1)[::-1]
-                bottom=np.max((cc_labels == cid).nonzero(), axis=1)[::-1]
-                center[1]=bottom[1]+2
-                self.draw_text(text, center, color=color)
-
-    def _convert_keypoints(self, keypoints):
-        if isinstance(keypoints, Keypoints):
-            keypoints = keypoints.tensor
-        keypoints = np.asarray(keypoints)
-        return keypoints
-
-    def get_output(self):
-        """
-        Returns:
-            output (VisImage): the image output containing the visualizations added
-            to the image.
-        """
-        return self.output
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/wrappers.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/wrappers.py
deleted file mode 100644
index 8dca2d2..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/utils/wrappers.py
+++ /dev/null
@@ -1,109 +0,0 @@
-# Copyright (c) 2022-2024, The Isaac Lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Wrapper to configure an :class:`ManagerBasedRLEnv` instance to RSL-RL vectorized environment.
-
-The following example shows how to wrap an environment for RSL-RL:
-
-.. code-block:: python
-
-    from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import RslRlVecEnvWrapper
-
-    env = RslRlVecEnvWrapper(env)
-
-"""
-
-
-import gymnasium as gym
-import torch
-
-from rsl_rl.env import VecEnv
-
-from omni.isaac.lab.envs import DirectRLEnv, ManagerBasedRLEnv
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import RslRlVecEnvWrapper
-
-
-def get_proprio_obs_dim(env: ManagerBasedRLEnv) -> int:
-    """Returns the dimension of the proprioceptive observations."""
-    return env.unwrapped.observation_manager.compute_group("proprio").shape[1]
-
-
-class RslRlVecEnvHistoryWrapper(RslRlVecEnvWrapper):
-    """Wraps around Isaac Lab environment for RSL-RL to add history buffer to the proprioception observations.
-
-    .. caution::
-
-        This class must be the last wrapper in the wrapper chain. This is because the wrapper does not follow
-        the :class:`gym.Wrapper` interface. Any subsequent wrappers will need to be modified to work with this
-        wrapper.
-
-    Reference:
-        https://github.com/leggedrobotics/rsl_rl/blob/master/rsl_rl/env/vec_env.py
-    """
-
-    def __init__(self, env: ManagerBasedRLEnv, history_length: int = 1):
-        """Initializes the wrapper."""
-        super().__init__(env)
-
-        self.history_length = history_length
-        self.proprio_obs_dim = get_proprio_obs_dim(env)
-        self.proprio_obs_buf = torch.zeros(self.num_envs, self.history_length, self.proprio_obs_dim,
-                                                    dtype=torch.float, device=self.unwrapped.device)
-        
-        self.clip_actions = 20.0
-
-    """
-    Properties
-    """
-    def get_observations(self) -> tuple[torch.Tensor, dict]:
-        """Returns the current observations of the environment."""
-        if hasattr(self.unwrapped, "observation_manager"):
-            obs_dict = self.unwrapped.observation_manager.compute()
-        else:
-            obs_dict = self.unwrapped._get_observations()
-        proprio_obs, obs = obs_dict["proprio"], obs_dict["policy"]
-        self.proprio_obs_buf = torch.cat([proprio_obs.unsqueeze(1)] * self.history_length, dim=1)
-        proprio_obs_history = self.proprio_obs_buf.view(self.num_envs, -1)
-        curr_obs = torch.cat([obs, proprio_obs_history], dim=1)
-        obs_dict["policy"] = curr_obs
-
-        return curr_obs, {"observations": obs_dict}
-
-    def step(self, actions: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:
-        # clip the actions (for testing only)
-        actions = torch.clamp(actions, -self.clip_actions, self.clip_actions)
-
-        # record step information
-        obs_dict, rew, terminated, truncated, extras = self.env.step(actions)
-        # compute dones for compatibility with RSL-RL
-        dones = (terminated | truncated).to(dtype=torch.long)
-        # move extra observations to the extras dict
-        proprio_obs, obs = obs_dict["proprio"], obs_dict["policy"]
-        # print("============== Height Map ==============")
-        # print(obs_dict["test_height_map"])
-        extras["observations"] = obs_dict
-        # move time out information to the extras dict
-        # this is only needed for infinite horizon tasks
-        if not self.unwrapped.cfg.is_finite_horizon:
-            extras["time_outs"] = truncated
-
-        # update obsservation history buffer & reset the history buffer for done environments
-        self.proprio_obs_buf = torch.where(
-            (self.episode_length_buf < 1)[:, None, None], 
-            torch.stack([torch.zeros_like(proprio_obs)] * self.history_length, dim=1),
-            torch.cat([
-                self.proprio_obs_buf[:, 1:],
-                proprio_obs.unsqueeze(1)
-            ], dim=1)
-        )
-        proprio_obs_history = self.proprio_obs_buf.view(self.num_envs, -1)
-        curr_obs = torch.cat([obs, proprio_obs_history], dim=1)
-        extras["observations"]["policy"] = curr_obs
-
-        # return the step information
-        return curr_obs, rew, dones, extras
-
-    def close(self):  # noqa: D102
-        return self.env.close()
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/__init__.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/__init__.py
deleted file mode 100644
index 37362bc..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/__init__.py
+++ /dev/null
@@ -1,14 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# from .vip_anymal import VIPlanner
-import os
-
-DATA_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../data"))
-
-# from .viplanner_algo import VIPlannerAlgo
-
-__all__ = ["DATA_DIR"]
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/__init__.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/__init__.py
deleted file mode 100644
index b9f2518..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/__init__.py
+++ /dev/null
@@ -1,17 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""This sub-module contains the functions that are specific to the viplanner environments."""
-
-from omni.isaac.lab.envs.mdp import *  # noqa: F401, F403
-from omni.isaac.lab_tasks.manager_based.locomotion.velocity.mdp.rewards import *
-
-from .actions import *  # noqa: F401, F403
-from .commands import *  # noqa: F401, F403
-from .observations import *  # noqa: F401, F403
-from .rewards import *  # noqa: F401, F403
-from .curriculums import *  # noqa: F401, F403
-from .events import *  # noqa: F401, F403
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/__init__.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/__init__.py
deleted file mode 100644
index d18d533..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/__init__.py
+++ /dev/null
@@ -1,9 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from .navigation_actions import *  # noqa: F401, F403
-from .vlm_navigation_actions import *  # noqa: F401, F403
-from .vlm_navigation_actions_gpt import *  # noqa: F401, F403
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/navigation_actions.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/navigation_actions.py
deleted file mode 100644
index ddf0151..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/navigation_actions.py
+++ /dev/null
@@ -1,144 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-from dataclasses import MISSING
-
-import torch
-import torch.nn as nn
-import torchvision
-from omni.isaac.lab.envs import ManagerBasedRLEnv
-from omni.isaac.lab.managers.action_manager import ActionTerm, ActionTermCfg
-from omni.isaac.lab.utils import configclass
-from omni.isaac.lab.utils.assets import check_file_path, read_file
-
-# class DepthImageProcessor(nn.Module):
-#     def __init__(self, image_height, image_width, num_output_units):
-#         super(DepthImageProcessor, self).__init__()
-#         self.cnn = nn.Sequential(
-#             nn.Conv2d(1, 32, 3, 1),
-#             nn.ReLU(),
-#             nn.Conv2d(32, 64, 3, 1),
-#             nn.ReLU(),
-#             nn.MaxPool2d(2),
-#         )
-#         self.fc = nn.Sequential(
-#             nn.Linear(64 * (image_height//4) * (image_width//4), num_output_units)  # Adjust based on your specific task
-#         )
-    
-#     def forward(self, x):
-#         x = self.cnn(x)
-#         x = x.view(x.size(0), -1)
-#         x = self.fc(x)
-#         return x
-    
-# -- Navigation Action
-class NavigationAction(ActionTerm):
-    """Actions to navigate a robot by following some path."""
-
-    cfg: NavigationActionCfg
-    _env: ManagerBasedRLEnv
-
-    def __init__(self, cfg: NavigationActionCfg, env: ManagerBasedRLEnv):
-        super().__init__(cfg, env)
-        # self.depth_cnn = DepthImageProcessor(image_height=self.cfg.image_size[0], 
-        #                                      image_width=self.cfg.image_size[1],num_output_units=32).to(self.device)
-        # self.resize_transform = torchvision.transforms.Resize((58, 87), 
-        #                                                       interpolation=torchvision.transforms.InterpolationMode.BICUBIC)
-
-        self.image_count = 0
-        # # check if policy file exists
-        if not check_file_path(self.cfg.low_level_policy_file):
-            raise FileNotFoundError(f"Policy file '{self.cfg.low_level_policy_file}' does not exist.")
-        file_bytes = read_file(self.cfg.low_level_policy_file)
-
-        # # # load policies
-        self.low_level_policy = torch.jit.load(file_bytes, map_location=self.device)
-        self.low_level_policy = torch.jit.freeze(self.low_level_policy.eval())
-
-        # prepare joint position actions
-        self.low_level_action_term: ActionTerm = self.cfg.low_level_action.class_type(cfg.low_level_action, env)
-
-        # prepare buffers
-        self._action_dim = (
-            3
-        )  # [vx, vy, omega] --> vx: [-0.5,1.0], vy: [-0.5,0.5], omega: [-1.0,1.0]
-        self._raw_navigation_velocity_actions = torch.zeros(self.num_envs, self._action_dim, device=self.device)
-        self._processed_navigation_velocity_actions = torch.zeros(
-            (self.num_envs, 3), device=self.device
-        )
-
-        self._low_level_actions = torch.zeros(self.num_envs, self.low_level_action_term.action_dim, device=self.device)
-        self._low_level_step_dt = self.cfg.low_level_decimation * self._env.physics_dt
-
-        self._counter = 0
-
-    """
-    Properties.
-    """
-
-    @property
-    def action_dim(self) -> int:
-        return self._action_dim
-
-    @property
-    def raw_actions(self) -> torch.Tensor:
-        return self._raw_navigation_velocity_actions
-
-    @property
-    def processed_actions(self) -> torch.Tensor:
-        return self._processed_navigation_velocity_actions
-
-    @property
-    def low_level_actions(self) -> torch.Tensor:
-        return self._low_level_actions
-
-    """
-    Operations.
-    """
-
-    def process_actions(self, actions):
-        """Process high-level navigation actions. This function is called with a frequency of 10Hz"""
-
-        # Store low level navigation actions
-        self._raw_navigation_velocity_actions[:] = actions
-        # reshape into 3D path
-        self._processed_navigation_velocity_actions[:] = actions.clone().view(self.num_envs, 3)
-
-    def apply_actions(self):
-        """Apply low-level actions for the simulator to the physics engine. This functions is called with the
-        simulation frequency of 200Hz. Since low-level locomotion runs at 50Hz, we need to decimate the actions."""
-        if self._counter % self.cfg.low_level_decimation == 0:
-            self._counter = 0
-        #     # -- update command
-            self._env.command_manager.compute(dt=self._low_level_step_dt)
-            # Get low level actions from low level policy
-            self._low_level_actions[:] = self.low_level_policy(
-                self._env.observation_manager.compute_group(group_name="low_level_policy")
-            )
-            # Process low level actions
-            self.low_level_action_term.process_actions(self._low_level_actions)
-
-        # Apply low level actions
-        self.low_level_action_term.apply_actions()
-        self._counter += 1
-
-
-@configclass
-class NavigationActionCfg(ActionTermCfg):
-    class_type: type[ActionTerm] = NavigationAction
-    """ Class of the action term."""
-    low_level_decimation: int = 4
-    """Decimation factor for the low level action term."""
-    low_level_action: ActionTermCfg = MISSING
-    """Configuration of the low level action term."""
-    low_level_policy_file: str = MISSING
-    """Path to the low level policy file."""
-    path_length: int = 51
-    """Length of the path to be followed."""
-    # low_level_agent_cfg: dict = {}
-    image_size: tuple = ()
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/vlm_navigation_actions.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/vlm_navigation_actions.py
deleted file mode 100644
index 01145e9..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/vlm_navigation_actions.py
+++ /dev/null
@@ -1,124 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-from dataclasses import MISSING
-
-import torch
-import torch.nn as nn
-import torchvision
-from omni.isaac.lab.envs import ManagerBasedRLEnv
-from omni.isaac.lab.managers.action_manager import ActionTerm, ActionTermCfg
-from omni.isaac.lab.utils import configclass
-from omni.isaac.lab.utils.assets import check_file_path, read_file
-
-# -- Navigation Action
-class VLMActions(ActionTerm):
-    """Actions to navigate a robot by following some path."""
-
-    cfg: VLMActionsCfg
-    _env: ManagerBasedRLEnv
-
-    def __init__(self, cfg: VLMActionsCfg, env: ManagerBasedRLEnv):
-        super().__init__(cfg, env)
-        # self.depth_cnn = DepthImageProcessor(image_height=self.cfg.image_size[0], 
-        #                                      image_width=self.cfg.image_size[1],num_output_units=32).to(self.device)
-        # self.resize_transform = torchvision.transforms.Resize((58, 87), 
-        #                                                       interpolation=torchvision.transforms.InterpolationMode.BICUBIC)
-
-        self.image_count = 0
-        # # check if policy file exists
-        if not check_file_path(self.cfg.low_level_policy_file):
-            raise FileNotFoundError(f"Policy file '{self.cfg.low_level_policy_file}' does not exist.")
-        file_bytes = read_file(self.cfg.low_level_policy_file)
-
-        # # # load policies
-        self.low_level_policy = torch.jit.load(file_bytes, map_location=self.device)
-        self.low_level_policy = torch.jit.freeze(self.low_level_policy.eval())
-
-        # prepare joint position actions
-        self.low_level_action_term: ActionTerm = self.cfg.low_level_action.class_type(cfg.low_level_action, env)
-
-        # prepare buffers
-        self._action_dim = (
-            3
-        )  # [vx, vy, omega] --> vx: [-0.5,1.0], vy: [-0.5,0.5], omega: [-1.0,1.0]
-        self._raw_navigation_velocity_actions = torch.zeros(self.num_envs, self._action_dim, device=self.device)
-        self._processed_command_velocity_actions = torch.zeros(
-            (self.num_envs, 3), device=self.device
-        )
-
-        self._low_level_actions = torch.zeros(self.num_envs, self.low_level_action_term.action_dim, device=self.device)
-        self._low_level_step_dt = self.cfg.low_level_decimation * self._env.physics_dt
-
-        self._counter = 0
-
-    """
-    Properties.
-    """
-
-    @property
-    def action_dim(self) -> int:
-        return self._action_dim
-
-    @property
-    def raw_actions(self) -> torch.Tensor:
-        return self._raw_navigation_velocity_actions
-
-    @property
-    def processed_actions(self) -> torch.Tensor:
-        return self._processed_command_velocity_actions
-
-    @property
-    def low_level_actions(self) -> torch.Tensor:
-        return self._low_level_actions
-
-    """
-    Operations.
-    """
-
-    def process_actions(self, actions):
-        """Process high-level navigation actions. This function is called with a frequency of 10Hz"""
-
-        # Store low level navigation actions
-        self._raw_navigation_velocity_actions[:] = actions
-        # reshape into 3D path
-        self._processed_command_velocity_actions[:] = actions.clone().view(self.num_envs, 3)
-
-    def apply_actions(self):
-        """Apply low-level actions for the simulator to the physics engine. This functions is called with the
-        simulation frequency of 200Hz. Since low-level locomotion runs at 50Hz, we need to decimate the actions."""
-        if self._counter % self.cfg.low_level_decimation == 0:
-            self._counter = 0
-        #     # -- update command
-            self._env.command_manager.compute(dt=self._low_level_step_dt)
-            # Get low level actions from low level policy
-            self._low_level_actions[:] = self.low_level_policy(
-                self._env.observation_manager.compute_group(group_name="low_level_policy")
-            )
-            # Process low level actions
-            self.low_level_action_term.process_actions(self._low_level_actions)
-
-        # Apply low level actions
-        self.low_level_action_term.apply_actions()
-        self._counter += 1
-
-
-@configclass
-class VLMActionsCfg(ActionTermCfg):
-    class_type: type[ActionTerm] = VLMActions
-    """ Class of the action term."""
-    low_level_decimation: int = 4
-    """Decimation factor for the low level action term."""
-    low_level_action: ActionTermCfg = MISSING
-    """Configuration of the low level action term."""
-    low_level_policy_file: str = MISSING
-    """Path to the low level policy file."""
-    # path_length: int = 51
-    # """Length of the path to be followed."""
-    # # low_level_agent_cfg: dict = {}
-    # image_size: tuple = ()
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/vlm_navigation_actions_gpt.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/vlm_navigation_actions_gpt.py
deleted file mode 100644
index 24d9445..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/actions/vlm_navigation_actions_gpt.py
+++ /dev/null
@@ -1,124 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-from dataclasses import MISSING
-
-import torch
-import torch.nn as nn
-import torchvision
-from omni.isaac.lab.envs import ManagerBasedRLEnv
-from omni.isaac.lab.managers.action_manager import ActionTerm, ActionTermCfg
-from omni.isaac.lab.utils import configclass
-from omni.isaac.lab.utils.assets import check_file_path, read_file
-
-# -- Navigation Action
-class VLMActionsGPT(ActionTerm):
-    """Actions to navigate a robot by following some path."""
-
-    cfg: VLMActionsGPTCfg
-    _env: ManagerBasedRLEnv
-
-    def __init__(self, cfg: VLMActionsGPTCfg, env: ManagerBasedRLEnv):
-        super().__init__(cfg, env)
-        # self.depth_cnn = DepthImageProcessor(image_height=self.cfg.image_size[0], 
-        #                                      image_width=self.cfg.image_size[1],num_output_units=32).to(self.device)
-        # self.resize_transform = torchvision.transforms.Resize((58, 87), 
-        #                                                       interpolation=torchvision.transforms.InterpolationMode.BICUBIC)
-
-        self.image_count = 0
-        # # check if policy file exists
-        if not check_file_path(self.cfg.low_level_policy_file):
-            raise FileNotFoundError(f"Policy file '{self.cfg.low_level_policy_file}' does not exist.")
-        file_bytes = read_file(self.cfg.low_level_policy_file)
-
-        # # # load policies
-        self.low_level_policy = torch.jit.load(file_bytes, map_location=self.device)
-        self.low_level_policy = torch.jit.freeze(self.low_level_policy.eval())
-
-        # prepare joint position actions
-        self.low_level_action_term: ActionTerm = self.cfg.low_level_action.class_type(cfg.low_level_action, env)
-
-        # prepare buffers
-        self._action_dim = (
-            3
-        )  # [vx, vy, omega] --> vx: [-0.5,1.0], vy: [-0.5,0.5], omega: [-1.0,1.0]
-        self._raw_navigation_velocity_actions = torch.zeros(self.num_envs, self._action_dim, device=self.device)
-        self._processed_command_velocity_actions = torch.zeros(
-            (self.num_envs, 3), device=self.device
-        )
-
-        self._low_level_actions = torch.zeros(self.num_envs, self.low_level_action_term.action_dim, device=self.device)
-        self._low_level_step_dt = self.cfg.low_level_decimation * self._env.physics_dt
-
-        self._counter = 0
-
-    """
-    Properties.
-    """
-
-    @property
-    def action_dim(self) -> int:
-        return self._action_dim
-
-    @property
-    def raw_actions(self) -> torch.Tensor:
-        return self._raw_navigation_velocity_actions
-
-    @property
-    def processed_actions(self) -> torch.Tensor:
-        return self._processed_command_velocity_actions
-
-    @property
-    def low_level_actions(self) -> torch.Tensor:
-        return self._low_level_actions
-
-    """
-    Operations.
-    """
-
-    def process_actions(self, actions):
-        """Process high-level navigation actions. This function is called with a frequency of 10Hz"""
-
-        # Store low level navigation actions
-        self._raw_navigation_velocity_actions[:] = actions
-        # reshape into 3D path
-        self._processed_command_velocity_actions[:] = actions.clone().view(self.num_envs, 3)
-
-    def apply_actions(self):
-        """Apply low-level actions for the simulator to the physics engine. This functions is called with the
-        simulation frequency of 200Hz. Since low-level locomotion runs at 50Hz, we need to decimate the actions."""
-        if self._counter % self.cfg.low_level_decimation == 0:
-            self._counter = 0
-        #     # -- update command
-            self._env.command_manager.compute(dt=self._low_level_step_dt)
-            # Get low level actions from low level policy
-            self._low_level_actions[:] = self.low_level_policy(
-                self._env.observation_manager.compute_group(group_name="low_level_policy")
-            )
-            # Process low level actions
-            self.low_level_action_term.process_actions(self._low_level_actions)
-
-        # Apply low level actions
-        self.low_level_action_term.apply_actions()
-        self._counter += 1
-
-
-@configclass
-class VLMActionsGPTCfg(ActionTermCfg):
-    class_type: type[ActionTerm] = VLMActionsGPT
-    """ Class of the action term."""
-    low_level_decimation: int = 4
-    """Decimation factor for the low level action term."""
-    low_level_action: ActionTermCfg = MISSING
-    """Configuration of the low level action term."""
-    low_level_policy_file: str = MISSING
-    """Path to the low level policy file."""
-    # path_length: int = 51
-    # """Length of the path to be followed."""
-    # # low_level_agent_cfg: dict = {}
-    # image_size: tuple = ()
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/__init__.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/__init__.py
deleted file mode 100644
index be8fea9..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/__init__.py
+++ /dev/null
@@ -1,34 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from .path_follower_command_generator import PathFollowerCommandGenerator
-from .path_follower_command_generator_cfg import PathFollowerCommandGeneratorCfg
-
-from .path_follower_command_generator_gpt import PathFollowerCommandGeneratorGPT
-from .path_follower_command_generator_gpt_cfg import PathFollowerCommandGeneratorGPTCfg
-
-from .rl_command_generator import RLCommandGenerator
-from .rl_command_generator_cfg import RLCommandGeneratorCfg
-
-from .midlevel_command_generator import MidLevelCommandGenerator
-from .midlevel_command_generator_cfg import MidLevelCommandGeneratorCfg
-
-from .lowlevel_command_generator import LowLevelCommandGenerator
-from .lowlevel_command_generator_cfg import LowLevelCommandGeneratorCfg
-
-from .goal_command_generator import GoalCommandGenerator
-from .goal_command_generator_cfg import GoalCommandGeneratorCfg
-
-from .robot_vel_command_generator import RobotVelCommandGenerator
-from .robot_vel_command_generator_cfg import RobotVelCommandGeneratorCfg
-
-__all__ = ["PathFollowerCommandGeneratorCfg", "PathFollowerCommandGenerator", 
-           "PathFollowerCommandGeneratorGPTCfg", "PathFollowerCommandGeneratorGPT",
-           "RLCommandGeneratorCfg", "RLCommandGenerator",
-           "MidLevelCommandGeneratorCfg", "MidLevelCommandGenerator",
-           "LowLevelCommandGeneratorCfg", "LowLevelCommandGenerator",
-           "GoalCommandGenerator", "GoalCommandGeneratorCfg",
-           "RobotVelCommandGenerator", "RobotVelCommandGeneratorCfg"]
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/goal_command_generator.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/goal_command_generator.py
deleted file mode 100644
index a97e7f0..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/goal_command_generator.py
+++ /dev/null
@@ -1,166 +0,0 @@
-from __future__ import annotations
-
-import math
-from typing import TYPE_CHECKING, Sequence
-
-import omni.isaac.lab.utils.math as math_utils
-from omni.isaac.lab.utils.math import wrap_to_pi, quat_rotate_inverse, yaw_quat
-import torch
-from omni.isaac.lab.assets.articulation import Articulation
-from omni.isaac.lab.envs import ManagerBasedRLEnv
-from omni.isaac.lab.managers import CommandTerm
-from omni.isaac.lab.markers import VisualizationMarkers
-from omni.isaac.lab.markers.config import (
-    BLUE_ARROW_X_MARKER_CFG,
-    GREEN_ARROW_X_MARKER_CFG,
-)
-from omni.isaac.lab.sim import SimulationContext
-
-if TYPE_CHECKING:
-    from .goal_command_generator_cfg import GoalCommandGeneratorCfg
-
-
-
-
-
-class GoalCommandGenerator(CommandTerm):
-    """Command generator that generates pose commands containing a 3-D position and heading.
-
-    The command generator samples uniform 2D positions around the environment origin. It sets
-    the height of the position command to the default root height of the robot. The heading
-    command is either set to point towards the target or is sampled uniformly.
-    This can be configured through the :attr:`Pose2dCommandCfg.simple_heading` parameter in
-    the configuration.
-    """
-
-    cfg: GoalCommandGeneratorCfg
-    """Configuration for the command generator."""
-
-    def __init__(self, cfg: GoalCommandGeneratorCfg, env: ManagerBasedRLEnv):
-        """Initialize the command generator class.
-
-        Args:
-            cfg: The configuration parameters for the command generator.
-            env: The environment object.
-        """
-        # initialize the base class
-        super().__init__(cfg, env)
-
-        # obtain the robot and terrain assets
-        # -- robot
-        self.robot: Articulation = env.scene[cfg.asset_name]
-
-        # crete buffers to store the command
-        # -- commands: (x, y, z, heading)
-        self.pos_command_w = torch.zeros(self.num_envs, 3, device=self.device)
-        self.heading_command_w = torch.zeros(self.num_envs, device=self.device)
-        self.pos_command_b = torch.zeros_like(self.pos_command_w)
-        self.heading_command_b = torch.zeros_like(self.heading_command_w)
-        # -- metrics
-        self.metrics["error_pos_2d"] = torch.zeros(self.num_envs, device=self.device)
-        self.metrics["error_heading"] = torch.zeros(self.num_envs, device=self.device)
-
-    def __str__(self) -> str:
-        msg = "PositionCommand:\n"
-        msg += f"\tCommand dimension: {tuple(self.command.shape[1:])}\n"
-        msg += f"\tResampling time range: {self.cfg.resampling_time_range}"
-        return msg
-
-    """
-    Properties
-    """
-
-    @property
-    def command(self) -> torch.Tensor:
-        """The desired 2D-pose in base frame. Shape is (num_envs, 4)."""
-        return torch.cat([self.pos_command_b, self.heading_command_b.unsqueeze(1)], dim=1)
-
-    """
-    Implementation specific functions.
-    """
-
-    def _update_metrics(self):
-        # logs data
-        self.metrics["error_pos_2d"] = torch.norm(self.pos_command_w[:, :2] - self.robot.data.root_pos_w[:, :2], dim=1)
-        self.metrics["error_heading"] = torch.abs(wrap_to_pi(self.heading_command_w - self.robot.data.heading_w))
-
-    def _resample_command(self, env_ids: Sequence[int]):
-        # import ipdb; ipdb.set_trace()
-        r = torch.empty(len(env_ids), device=self.device)
-        self.pos_command_b[env_ids, 0] = r.uniform_(*self.cfg.ranges.pos_x)#6.4756#, r.uniform_(*self.cfg.ranges.pos_x)
-        self.pos_command_b[env_ids, 1] = r.uniform_(*self.cfg.ranges.pos_y)#-0.2762#r.uniform_(*self.cfg.ranges.pos_y)
-        self.pos_command_b[env_ids, 2] = 0.0
-        # self.pos_command_b[env_ids, 0] = 6.4756#6.4756#, r.uniform_(*self.cfg.ranges.pos_x)
-        # self.pos_command_b[env_ids, 1] = -0.2762#-0.2762#r.uniform_(*self.cfg.ranges.pos_y)
-        # self.pos_command_b[env_ids, 2] = 0.0
-        # print("resampled command: ", self.pos_command_b[env_ids,:3])
-
-        self.pos_command_w[env_ids,:3] = math_utils.quat_apply(math_utils.yaw_quat(self.robot.data.root_quat_w[env_ids,:]), self.pos_command_b[env_ids,:3])
-        self.pos_command_w[env_ids,:3] += self.robot.data.root_pos_w[env_ids,:3]
-        self.pos_command_w[env_ids, 2] = self._env.scene.env_origins[env_ids, 2] + self.robot.data.default_root_state[env_ids, 2]
-        # self.pos_command_w[env_ids, 1] = -100
-        # self.pos_command_w[env_ids, 0] = torch.clamp(self.pos_command_w[env_ids, 0], torch.min(self._env.scene.env_origins[:, 0]), torch.max(self._env.scene.env_origins[:, 0]))
-        # self.pos_command_w[env_ids, 1] = torch.clamp(self.pos_command_w[env_ids, 1], torch.min(self._env.scene.env_origins[:, 1]), torch.max(self._env.scene.env_origins[:, 1]))
-        # print("world command: ", self.pos_command_w[env_ids,:3])
-        # import ipdb; ipdb.set_trace()
-        # # obtain env origins for the environments 
-        # self.pos_command_w[env_ids] = self._env.scene.env_origins[env_ids]
-        # # offset the position command by the current root position
-        # r = torch.empty(len(env_ids), device=self.device)
-        # self.pos_command_w[env_ids, 0] += r.uniform_(*self.cfg.ranges.pos_x)
-        # self.pos_command_w[env_ids, 1] += r.uniform_(*self.cfg.ranges.pos_y)
-        # self.pos_command_w[env_ids, 2] += self.robot.data.default_root_state[env_ids, 2]
-
-        if self.cfg.simple_heading:
-            # set heading command to point towards target
-            target_vec = self.pos_command_w[env_ids] - self.robot.data.root_pos_w[env_ids]
-            target_direction = torch.atan2(target_vec[:, 1], target_vec[:, 0])
-            flipped_target_direction = wrap_to_pi(target_direction + torch.pi)
-
-            # compute errors to find the closest direction to the current heading
-            # this is done to avoid the discontinuity at the -pi/pi boundary
-            curr_to_target = wrap_to_pi(target_direction - self.robot.data.heading_w[env_ids]).abs()
-            curr_to_flipped_target = wrap_to_pi(flipped_target_direction - self.robot.data.heading_w[env_ids]).abs()
-
-            # set the heading command to the closest direction
-            self.heading_command_w[env_ids] = torch.where(
-                curr_to_target < curr_to_flipped_target,
-                target_direction,
-                flipped_target_direction,
-            )
-        else:
-            # random heading command
-            self.heading_command_w[env_ids] = r.uniform_(*self.cfg.ranges.heading)
-
-    def _update_command(self):
-        """Re-target the position command to the current root state."""
-        target_vec = self.pos_command_w - self.robot.data.root_pos_w[:, :3]
-        self.pos_command_b[:] = quat_rotate_inverse(yaw_quat(self.robot.data.root_quat_w), target_vec)
-        self.pos_command_b[:, 2] = 0.0
-        self.heading_command_b[:] = wrap_to_pi(self.heading_command_w - self.robot.data.heading_w)*0.0
-
-    def _set_debug_vis_impl(self, debug_vis: bool):
-        # create markers if necessary for the first tome
-        if debug_vis:
-            if not hasattr(self, "arrow_goal_visualizer"):
-                marker_cfg = GREEN_ARROW_X_MARKER_CFG.copy()
-                marker_cfg.markers["arrow"].scale = (0.2, 0.2, 0.8)
-                marker_cfg.prim_path = "/Visuals/Command/pose_goal"
-                self.arrow_goal_visualizer = VisualizationMarkers(marker_cfg)
-            # set their visibility to true
-            self.arrow_goal_visualizer.set_visibility(True)
-        else:
-            if hasattr(self, "arrow_goal_visualizer"):
-                self.arrow_goal_visualizer.set_visibility(False)
-
-    def _debug_vis_callback(self, event):
-        # update the box marker
-        self.arrow_goal_visualizer.visualize(
-            translations=self.pos_command_w,
-            orientations=quat_from_euler_xyz(
-                torch.zeros_like(self.heading_command_w),
-                torch.zeros_like(self.heading_command_w),
-                self.heading_command_w,
-            ),
-        )
-
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/goal_command_generator_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/goal_command_generator_cfg.py
deleted file mode 100644
index 87f631b..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/goal_command_generator_cfg.py
+++ /dev/null
@@ -1,41 +0,0 @@
-import math
-from dataclasses import MISSING
-
-from omni.isaac.lab.managers import CommandTermCfg
-from omni.isaac.lab.utils.configclass import configclass
-from typing_extensions import Literal
-
-from .goal_command_generator import GoalCommandGenerator
-
-
-@configclass
-class GoalCommandGeneratorCfg(CommandTermCfg):
-    """Configuration for the uniform 2D-pose command generator."""
-
-    class_type: type = GoalCommandGenerator
-
-    asset_name: str = MISSING
-    """Name of the asset in the environment for which the commands are generated."""
-
-    simple_heading: bool = MISSING
-    """Whether to use simple heading or not.
-
-    If True, the heading is in the direction of the target position.
-    """
-
-    @configclass
-    class Ranges:
-        """Uniform distribution ranges for the position commands."""
-
-        pos_x: tuple[float, float] = MISSING
-        """Range for the x position (in m)."""
-        pos_y: tuple[float, float] = MISSING
-        """Range for the y position (in m)."""
-        heading: tuple[float, float] = MISSING
-        """Heading range for the position commands (in rad).
-
-        Used only if :attr:`simple_heading` is False.
-        """
-
-    ranges: Ranges = MISSING
-    """Distribution ranges for the position commands."""
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/lowlevel_command_generator.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/lowlevel_command_generator.py
deleted file mode 100644
index 664d3c8..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/lowlevel_command_generator.py
+++ /dev/null
@@ -1,227 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Copyright (c) 2022-2024, The lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Sub-module containing command generators for the velocity-based locomotion task."""
-from __future__ import annotations
-
-import os
-import math
-from typing import TYPE_CHECKING, Sequence
-
-import omni.isaac.lab.utils.math as math_utils
-import torch
-from omni.isaac.lab.assets.articulation import Articulation
-from omni.isaac.lab.envs import ManagerBasedRLEnv
-from omni.isaac.lab.managers import CommandTerm
-from omni.isaac.lab.markers import VisualizationMarkers
-from omni.isaac.lab.markers.config import (
-    BLUE_ARROW_X_MARKER_CFG,
-    GREEN_ARROW_X_MARKER_CFG,
-    CUBOID_MARKER_CFG,
-)
-from omni.isaac.lab.sim import SimulationContext
-from omni.isaac.lab.utils.assets import check_file_path, read_file
-# from omni.isaac.lab.utils.assets import LOCAL_ISAAC_DIR
-
-
-if TYPE_CHECKING:
-    from .lowlevel_command_generator_cfg import LowLevelCommandGeneratorCfg
-
-
-class LowLevelCommandGenerator(CommandTerm):
-    r"""Command generator that generates a velocity command in SE(2) from a path given by a local planner.
-
-    The command comprises of a linear velocity in x and y direction and an angular velocity around
-    the z-axis. It is given in the robot's base frame.
-
-    The path follower acts as a PD-controller that checks for the last point on the path within a lookahead distance
-    and uses it to compute the steering angle and the linear velocity.
-    """
-
-    cfg: LowLevelCommandGeneratorCfg
-    """The configuration of the command generator."""
-
-    def __init__(self, cfg: LowLevelCommandGeneratorCfg, env: ManagerBasedRLEnv):
-        """Initialize the command generator.
-
-        Args:
-            cfg (LowLevelCommandGeneratorCfg): The configuration of the command generator.
-            env (object): The environment.
-        """
-        super().__init__(cfg, env)
-        # -- robot
-        self.robot: Articulation = env.scene[cfg.robot_attr]
-        # -- Simulation Context
-        self.sim: SimulationContext = SimulationContext.instance()
-        self.twist: torch.Tensor = torch.zeros((self.num_envs, 3), device=self.device)
-        # -- debug vis
-        self._base_vel_goal_markers = None
-        self._base_vel_markers = None
-
-        # Rotation mark
-        self.rotation_mark = False
-        self.initialized = False
-        self.goal_reached = False
-        self.identity_quat = torch.tensor([1.0, 0.0, 0.0, 0.0], device=self._env.device).repeat(self._env.num_envs, 1)
-
-        self.objnav_policy_path = os.path.join(LOCAL_ISAAC_DIR, "low_level_policy", "mid_policy_height_scan.jit")
-        self.objnav_policy_path = os.path.join(LOCAL_ISAAC_DIR, "low_level_policy", "mid_policy_depth.jit")
-        if not check_file_path(self.objnav_policy_path):
-            raise FileNotFoundError(f"Policy file '{self.objnav_policy_path}' does not exist.")
-        file_bytes = read_file(self.objnav_policy_path)
-        self.mid_level_policy = torch.jit.load(file_bytes, map_location=self.device)
-        self.mid_level_policy = torch.jit.freeze(self.mid_level_policy.eval())
-
-
-    def __str__(self) -> str:
-        """Return a string representation of the command generator."""
-        msg = "rlCommandGenerator:\n"
-        msg += f"\tCommand dimension: {tuple(self.command.shape[1:])}\n"
-        msg += f"\tLookahead distance: {self.cfg.lookAheadDistance}\n"
-        return msg
-
-    """
-    Properties
-    """
-
-    @property
-    def command(self) -> torch.Tensor:
-        """The desired base velocity command in the base frame. Shape is (num_envs, 3)."""
-        # print("twist: ", self.twist)
-        return self.twist
-
-    """
-    Operations.
-    """
-
-    def reset(self, env_ids: Sequence[int] | None = None) -> dict:
-        """Reset the command generator.
-
-        This function resets the command generator. It should be called whenever the environment is reset.
-
-        Args:
-            env_ids (Optional[Sequence[int]], optional): The list of environment IDs to reset. Defaults to None.
-        """
-        if env_ids is None:
-            env_ids = ...
-
-        self.twist = torch.zeros((self.num_envs, 3), device=self.device)
-        self.goal_reached = False
-
-        return {}
-
-    def compute(self, dt: float):
-        """Compute the command.
-
-        Paths as a tensor of shape (num_envs, N, 3) where N is number of poses on the path. The paths
-        should be given in the base frame of the robot. Num_envs is equal to the number of robots spawned in all
-        environments.
-        """
-        if not self.rotation_mark and not self.goal_reached and self.initialized:
-            # get paths
-            self.twist = self.mid_level_policy(self._env.observation_manager.compute_group(group_name="mid_level_planner")).reshape(self._env.num_envs, 3)
-            # self.twist = self._env.action_manager._terms['paths']._processed_navigation_velocity_actions.clone()
-            # import ipdb; ipdb.set_trace()
-            self.twist[:, 0] = torch.clip(self.twist[:, 0], 0.0, 0.5)
-            self.twist[:,1] = 0.0
-            self.twist[:,2] = torch.clip(self.twist[:,2], -math.pi, math.pi)
-        elif self.goal_reached or (not self.initialized):
-            # self.twist[:, 0] = torch.clip(self.twist[:, 0], 0.0, 0.5)
-            # self.twist[:,1] = 0.0
-            self.twist[:,:2] = torch.zeros((self.num_envs, 2), device=self.device)
-        else:
-            # self.twist = torch.zeros((self.num_envs, 3), device=self.device)
-            self.twist[:,:2] = torch.zeros((self.num_envs, 2), device=self.device)
-            self.twist[:, 2] += 0.1
-            print("rotation")
-        self.goal_reached = self._env.command_manager._terms['midlevel_command'].command[:, :2].norm(dim=1) < 2.5
-        return self.twist
-
-    """
-    Implementation specific functions.
-    """
-
-    def _update_command(self):
-        pass
-
-    def _update_metrics(self):
-        pass
-
-    def _resample_command(self, env_ids: Sequence[int]):
-        pass
-
-    def _set_debug_vis_impl(self, debug_vis: bool):
-        # set visibility of markers
-        # note: parent only deals with callbacks. not their visibility
-        if debug_vis:
-            # create markers if necessary for the first tome
-            if not hasattr(self, "base_vel_goal_visualizer"):
-                # -- goal
-                marker_cfg = GREEN_ARROW_X_MARKER_CFG.copy()
-                marker_cfg.prim_path = "/Visuals/Command/velocity_goal"
-                marker_cfg.markers["arrow"].scale = (0.5, 0.5, 0.5)
-                self.base_vel_goal_visualizer = VisualizationMarkers(marker_cfg)
-                # -- current
-                marker_cfg = BLUE_ARROW_X_MARKER_CFG.copy()
-                marker_cfg.prim_path = "/Visuals/Command/velocity_current"
-                marker_cfg.markers["arrow"].scale = (0.5, 0.5, 0.5)
-                self.base_vel_visualizer = VisualizationMarkers(marker_cfg)
-                # -- goal command
-                marker_cfg = CUBOID_MARKER_CFG.copy()
-                marker_cfg.prim_path = "/Visuals/Command/pos_goal_command"
-                marker_cfg.markers["cuboid"].scale = (0.5, 0.5, 0.5)
-                # self.base_vel_goal_command_visualizer = VisualizationMarkers(marker_cfg)
-            # set their visibility to true
-            self.base_vel_goal_visualizer.set_visibility(True)
-            self.base_vel_visualizer.set_visibility(True)
-            # self.base_vel_goal_command_visualizer.set_visibility(True)
-        else:
-            if hasattr(self, "base_vel_goal_visualizer"):
-                self.base_vel_goal_visualizer.set_visibility(False)
-                self.base_vel_visualizer.set_visibility(False)
-                # self.base_vel_goal_command_visualizer.set_visibility(False)
-
-    def _debug_vis_callback(self, event):
-        # get marker location
-        # -- base state
-        base_pos_w = self.robot.data.root_pos_w.clone()
-        base_pos_w[:, 2] += 0.5
-        base_quat_w = self.robot.data.root_quat_w.clone()
-
-        # -- resolve the scales and quaternions
-        vel_des_arrow_scale, vel_des_arrow_quat = self._resolve_xy_velocity_to_arrow(self.command[:, :2])
-        vel_arrow_scale, vel_arrow_quat = self._resolve_xy_velocity_to_arrow(self.robot.data.root_lin_vel_b[:, :2])
-        # display markers
-        self.base_vel_goal_visualizer.visualize(base_pos_w, vel_des_arrow_quat, vel_des_arrow_scale)
-        self.base_vel_visualizer.visualize(base_pos_w, vel_arrow_quat, vel_arrow_scale)
-
-        # pos_command_w = self._env.command_manager._terms['midlevel_command'].pos_command_w.clone()
-        # default_scale = self.base_vel_goal_visualizer.cfg.markers["arrow"].scale
-        # larger_scale = 2.0*torch.tensor(default_scale, device=self.device).repeat(pos_command_w.shape[0], 1)
-        # self.base_vel_goal_command_visualizer.visualize(pos_command_w, self.identity_quat,larger_scale)
-
-    """
-    Internal helpers.
-    """
-
-    def _resolve_xy_velocity_to_arrow(self, xy_velocity: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
-        """Converts the XY base velocity command to arrow direction rotation."""
-        # obtain default scale of the marker
-        default_scale = self.base_vel_goal_visualizer.cfg.markers["arrow"].scale
-        # arrow-scale
-        arrow_scale = torch.tensor(default_scale, device=self.device).repeat(xy_velocity.shape[0], 1)
-        arrow_scale[:, 0] *= torch.linalg.norm(xy_velocity, dim=1)
-        # arrow-direction
-        heading_angle = torch.atan2(xy_velocity[:, 1], xy_velocity[:, 0])
-        zeros = torch.zeros_like(heading_angle)
-        arrow_quat = math_utils.quat_from_euler_xyz(zeros, zeros, heading_angle)
-
-        return arrow_scale, arrow_quat
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/lowlevel_command_generator_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/lowlevel_command_generator_cfg.py
deleted file mode 100644
index 0c76894..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/lowlevel_command_generator_cfg.py
+++ /dev/null
@@ -1,37 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Copyright (c) 2022-2024, The lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Sub-module containing command generators for the velocity-based locomotion task."""
-
-import math
-from dataclasses import MISSING
-
-from omni.isaac.lab.managers import CommandTermCfg
-from omni.isaac.lab.utils.configclass import configclass
-from typing_extensions import Literal
-
-from .lowlevel_command_generator import LowLevelCommandGenerator
-
-
-@configclass
-class LowLevelCommandGeneratorCfg(CommandTermCfg):
-    class_type: LowLevelCommandGenerator = LowLevelCommandGenerator
-    """Name of the command generator class."""
-
-    robot_attr: str = MISSING
-    """Name of the robot attribute from the environment."""
-
-    path_frame: Literal["world", "robot"] = "world"
-    """Frame in which the path is defined.
-    - ``world``: the path is defined in the world frame. Also called ``odom``.
-    - ``robot``: the path is defined in the robot frame. Also called ``base``.
-    """
-
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/midlevel_command_generator.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/midlevel_command_generator.py
deleted file mode 100644
index c3821ad..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/midlevel_command_generator.py
+++ /dev/null
@@ -1,124 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Copyright (c) 2022-2024, The lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Sub-module containing command generators for the velocity-based locomotion task."""
-from __future__ import annotations
-
-import math
-from typing import TYPE_CHECKING, Sequence
-
-import omni.isaac.lab.utils.math as math_utils
-import torch
-from omni.isaac.lab.assets.articulation import Articulation
-from omni.isaac.lab.envs import ManagerBasedRLEnv
-from omni.isaac.lab.managers import CommandTerm
-from omni.isaac.lab.sim import SimulationContext
-
-if TYPE_CHECKING:
-    from .midlevel_command_generator_cfg import MidLevelCommandGeneratorCfg
-
-
-class MidLevelCommandGenerator(CommandTerm):
-    r"""Command generator that generates a velocity command in SE(2) from a path given by a local planner.
-
-    The command comprises of a linear velocity in x and y direction and an angular velocity around
-    the z-axis. It is given in the robot's base frame.
-
-    The path follower acts as a PD-controller that checks for the last point on the path within a lookahead distance
-    and uses it to compute the steering angle and the linear velocity.
-    """
-
-    cfg: MidLevelCommandGeneratorCfg
-    """The configuration of the command generator."""
-
-    def __init__(self, cfg: MidLevelCommandGeneratorCfg, env: ManagerBasedRLEnv):
-        """Initialize the command generator.
-
-        Args:
-            cfg (RLCommandGeneratorCfg): The configuration of the command generator.
-            env (object): The environment.
-        """
-        super().__init__(cfg, env)
-
-        # -- robot
-        self.robot: Articulation = env.scene[cfg.robot_attr]
-        # -- Simulation Context
-        self.sim: SimulationContext = SimulationContext.instance()
-        self.command_b: torch.Tensor = torch.zeros((self.num_envs, 4), device=self.device)
-        # -- debug vis
-        self._base_vel_goal_markers = None
-        self._base_vel_markers = None
-
-        # Rotation mark
-        self.rotation_mark = False
-        self.initialized = False
-
-    def __str__(self) -> str:
-        """Return a string representation of the command generator."""
-        msg = "rlCommandGenerator:\n"
-        msg += f"\tCommand dimension: {tuple(self.command.shape[1:])}\n"
-        msg += f"\tLookahead distance: {self.cfg.lookAheadDistance}\n"
-        return msg
-
-    """
-    Properties
-    """
-
-    @property
-    def command(self) -> torch.Tensor:
-        """The desired base velocity command in the base frame. Shape is (num_envs, 3)."""
-        # print("twist: ", self.twist)
-        return self.command_b
-
-    """
-    Operations.
-    """
-
-    def reset(self, env_ids: Sequence[int] | None = None) -> dict:
-        """Reset the command generator.
-
-        This function resets the command generator. It should be called whenever the environment is reset.
-
-        Args:
-            env_ids (Optional[Sequence[int]], optional): The list of environment IDs to reset. Defaults to None.
-        """
-        if env_ids is None:
-            env_ids = ...
-
-        self.command_b = torch.zeros((self.num_envs, 4), device=self.device)
-        self.goal_reached = False
-
-        return {}
-
-    def compute(self, dt: float):
-        """Compute the command.
-
-        Paths as a tensor of shape (num_envs, N, 3) where N is number of poses on the path. The paths
-        should be given in the base frame of the robot. Num_envs is equal to the number of robots spawned in all
-        environments.
-        """
-        # get paths
-        self.command_b[:,:2] = self._env.action_manager._terms['paths']._processed_navigation_velocity_actions.clone()[:,:2]
-
-        return self.command_b
-
-    """
-    Implementation specific functions.
-    """
-
-    def _update_command(self):
-        pass
-
-    def _update_metrics(self):
-        pass
-
-    def _resample_command(self, env_ids: Sequence[int]):
-        pass
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/midlevel_command_generator_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/midlevel_command_generator_cfg.py
deleted file mode 100644
index b7ae5be..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/midlevel_command_generator_cfg.py
+++ /dev/null
@@ -1,37 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Copyright (c) 2022-2024, The lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Sub-module containing command generators for the velocity-based locomotion task."""
-
-import math
-from dataclasses import MISSING
-
-from omni.isaac.lab.managers import CommandTermCfg
-from omni.isaac.lab.utils.configclass import configclass
-from typing_extensions import Literal
-
-from .midlevel_command_generator import MidLevelCommandGenerator
-
-
-@configclass
-class MidLevelCommandGeneratorCfg(CommandTermCfg):
-    class_type: MidLevelCommandGenerator = MidLevelCommandGenerator
-    """Name of the command generator class."""
-
-    robot_attr: str = MISSING
-    """Name of the robot attribute from the environment."""
-
-    path_frame: Literal["world", "robot"] = "world"
-    """Frame in which the path is defined.
-    - ``world``: the path is defined in the world frame. Also called ``odom``.
-    - ``robot``: the path is defined in the robot frame. Also called ``base``.
-    """
-
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator.py
deleted file mode 100644
index 18cecb6..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator.py
+++ /dev/null
@@ -1,230 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Copyright (c) 2022-2024, The lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Sub-module containing command generators for the velocity-based locomotion task."""
-from __future__ import annotations
-
-import math
-from typing import TYPE_CHECKING, Sequence
-
-import omni.isaac.lab.utils.math as math_utils
-import torch
-from omni.isaac.lab.assets.articulation import Articulation
-from omni.isaac.lab.envs import ManagerBasedRLEnv
-from omni.isaac.lab.managers import CommandTerm
-from omni.isaac.lab.markers import VisualizationMarkers
-from omni.isaac.lab.markers.config import (
-    BLUE_ARROW_X_MARKER_CFG,
-    GREEN_ARROW_X_MARKER_CFG,
-)
-from omni.isaac.lab.sim import SimulationContext
-
-if TYPE_CHECKING:
-    from .path_follower_command_generator_cfg import PathFollowerCommandGeneratorCfg
-
-
-class PathFollowerCommandGenerator(CommandTerm):
-    r"""Command generator that generates a velocity command in SE(2) from a path given by a local planner.
-
-    The command comprises of a linear velocity in x and y direction and an angular velocity around
-    the z-axis. It is given in the robot's base frame.
-
-    The path follower acts as a PD-controller that checks for the last point on the path within a lookahead distance
-    and uses it to compute the steering angle and the linear velocity.
-    """
-
-    cfg: PathFollowerCommandGeneratorCfg
-    """The configuration of the command generator."""
-
-    def __init__(self, cfg: PathFollowerCommandGeneratorCfg, env: ManagerBasedRLEnv):
-        """Initialize the command generator.
-
-        Args:
-            cfg (PathFollowerCommandGeneratorCfg): The configuration of the command generator.
-            env (object): The environment.
-        """
-        super().__init__(cfg, env)
-        # -- robot
-        self.robot: Articulation = env.scene[cfg.robot_attr]
-        # -- Simulation Context
-        self.sim: SimulationContext = SimulationContext.instance()
-        # -- buffers
-        self.vehicleSpeed: torch.Tensor = torch.zeros(self.num_envs, device=self.device)
-        self.switch_time: torch.Tensor = torch.zeros(self.num_envs, device=self.device)
-        self.vehicleYawRate: torch.Tensor = torch.zeros(self.num_envs, device=self.device)
-        self.navigation_forward: torch.Tensor = torch.ones(self.num_envs, device=self.device, dtype=torch.bool)
-        self.twist: torch.Tensor = torch.zeros((self.num_envs, 3), device=self.device)
-        # -- debug vis
-        self._base_vel_goal_markers = None
-        self._base_vel_markers = None
-
-        # Rotation mark
-        self.rotation_mark = False
-        self.initialized = False
-        self.goal_reached = False
-
-    def __str__(self) -> str:
-        """Return a string representation of the command generator."""
-        msg = "PathFollowerCommandGenerator:\n"
-        msg += f"\tCommand dimension: {tuple(self.command.shape[1:])}\n"
-        msg += f"\tLookahead distance: {self.cfg.lookAheadDistance}\n"
-        return msg
-
-    """
-    Properties
-    """
-
-    @property
-    def command(self) -> torch.Tensor:
-        """The desired base velocity command in the base frame. Shape is (num_envs, 3)."""
-        return self.twist
-
-    """
-    Operations.
-    """
-
-    def reset(self, env_ids: Sequence[int] | None = None) -> dict:
-        """Reset the command generator.
-
-        This function resets the command generator. It should be called whenever the environment is reset.
-
-        Args:
-            env_ids (Optional[Sequence[int]], optional): The list of environment IDs to reset. Defaults to None.
-        """
-        if env_ids is None:
-            env_ids = ...
-
-        self.vehicleSpeed = torch.zeros(self.num_envs, device=self.device)
-        self.switch_time = torch.zeros(self.num_envs, device=self.device)
-        self.vehicleYawRate = torch.zeros(self.num_envs, device=self.device)
-        self.navigation_forward = torch.ones(self.num_envs, device=self.device, dtype=torch.bool)
-        self.twist = torch.zeros((self.num_envs, 3), device=self.device)
-        self.goal_reached = False
-
-        return {}
-
-    def compute(self, dt: float):
-        """Compute the command.
-
-        Paths as a tensor of shape (num_envs, N, 3) where N is number of poses on the path. The paths
-        should be given in the base frame of the robot. Num_envs is equal to the number of robots spawned in all
-        environments.
-        """
-        # get paths
-        paths = self._env.action_manager._terms['paths']._processed_navigation_velocity_actions.clone()
-        # get number of pases of the paths
-        num_envs, N = paths.shape
-        assert N > 0, "PathFollowerCommandGenerator: paths must have at least one poses."
-        # get the current simulation time
-        curr_time = self.sim.current_time
-        # define current maxSpeed for the velocities
-        max_speed = torch.ones(num_envs, device=self.device) * self.cfg.maxSpeed
-
-        # # transform path in base/ robot frame if given in world frame
-        # if self.cfg.path_frame == "world":
-        #     paths = math_utils.quat_apply(
-        #         math_utils.quat_inv(self.robot.data.root_quat_w[:, None, :].repeat(1, N, 1)),
-        #         paths - self.robot.data.root_pos_w[:, None, :],
-        #     )
-        self.paths_diff_global = paths[:,:2] - self.robot.data.root_pos_w[:, :2]
-
-        if self.initialized and not self.goal_reached:
-            if not self.rotation_mark:
-                # if abs(self.paths_diff_global[0,0,0]) < 0.5:
-                #     self.twist[:, 0] = 0.0
-                # else:
-                self.twist[:, 0] = min(max(self.paths_diff_global[0,0], -0.5), 0.5)
-                # if abs(self.paths_diff_global[0,0,1]) < 2.0:
-                #     self.twist[:, 1] = 0.0
-                # else:
-                self.twist[:, 1] = min(max(self.paths_diff_global[0,1], -0.3), 0.3)
-                self.twist[:, 2] = min(max(0.01*paths[0,2], -0.2), 0.2)
-                # TODO: add yaw rotation mechanism
-            else:
-                self.twist[:,0] = 0.0
-                self.twist[:,1] = 0.0
-                self.twist[:,2] = 0.5
-        else:
-            self.twist[:,0] = 0.0
-            self.twist[:,1] = 0.0
-            self.twist[:,2] = 0.0
-        if (torch.linalg.norm(self.paths_diff_global[0,:2], dim=-1))<3.0 and self.initialized:
-            self.goal_reached = True
-        # print("goal_reached: ", self.goal_reached, " rotation_mark: ", self.rotation_mark, " twist: ", self.twist[:,:3], " norm: ", torch.linalg.norm(self.paths_diff_global[0,0,:2], dim=-1), " initialized: ", self.initialized)
-
-        return self.twist
-
-    """
-    Implementation specific functions.
-    """
-
-    def _update_command(self):
-        pass
-
-    def _update_metrics(self):
-        pass
-
-    def _resample_command(self, env_ids: Sequence[int]):
-        pass
-
-    def _set_debug_vis_impl(self, debug_vis: bool):
-        # set visibility of markers
-        # note: parent only deals with callbacks. not their visibility
-        if debug_vis:
-            # create markers if necessary for the first tome
-            if not hasattr(self, "base_vel_goal_visualizer"):
-                # -- goal
-                marker_cfg = GREEN_ARROW_X_MARKER_CFG.copy()
-                marker_cfg.prim_path = "/Visuals/Command/velocity_goal"
-                marker_cfg.markers["arrow"].scale = (0.5, 0.5, 0.5)
-                self.base_vel_goal_visualizer = VisualizationMarkers(marker_cfg)
-                # -- current
-                marker_cfg = BLUE_ARROW_X_MARKER_CFG.copy()
-                marker_cfg.prim_path = "/Visuals/Command/velocity_current"
-                marker_cfg.markers["arrow"].scale = (0.5, 0.5, 0.5)
-                self.base_vel_visualizer = VisualizationMarkers(marker_cfg)
-            # set their visibility to true
-            self.base_vel_goal_visualizer.set_visibility(True)
-            self.base_vel_visualizer.set_visibility(True)
-        else:
-            if hasattr(self, "base_vel_goal_visualizer"):
-                self.base_vel_goal_visualizer.set_visibility(False)
-                self.base_vel_visualizer.set_visibility(False)
-
-    def _debug_vis_callback(self, event):
-        # get marker location
-        # -- base state
-        base_pos_w = self.robot.data.root_pos_w.clone()
-        base_pos_w[:, 2] += 0.5
-        # -- resolve the scales and quaternions
-        vel_des_arrow_scale, vel_des_arrow_quat = self._resolve_xy_velocity_to_arrow(self.command[:, :2])
-        vel_arrow_scale, vel_arrow_quat = self._resolve_xy_velocity_to_arrow(self.robot.data.root_lin_vel_b[:, :2])
-        # display markers
-        self.base_vel_goal_visualizer.visualize(base_pos_w, vel_des_arrow_quat, vel_des_arrow_scale)
-        self.base_vel_visualizer.visualize(base_pos_w, vel_arrow_quat, vel_arrow_scale)
-
-    """
-    Internal helpers.
-    """
-
-    def _resolve_xy_velocity_to_arrow(self, xy_velocity: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
-        """Converts the XY base velocity command to arrow direction rotation."""
-        # obtain default scale of the marker
-        default_scale = self.base_vel_goal_visualizer.cfg.markers["arrow"].scale
-        # arrow-scale
-        arrow_scale = torch.tensor(default_scale, device=self.device).repeat(xy_velocity.shape[0], 1)
-        arrow_scale[:, 0] *= torch.linalg.norm(xy_velocity, dim=1)
-        # arrow-direction
-        heading_angle = torch.atan2(xy_velocity[:, 1], xy_velocity[:, 0])
-        zeros = torch.zeros_like(heading_angle)
-        arrow_quat = math_utils.quat_from_euler_xyz(zeros, zeros, heading_angle)
-
-        return arrow_scale, arrow_quat
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator_cfg.py
deleted file mode 100644
index bd0b98b..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator_cfg.py
+++ /dev/null
@@ -1,64 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Copyright (c) 2022-2024, The lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Sub-module containing command generators for the velocity-based locomotion task."""
-
-import math
-from dataclasses import MISSING
-
-from omni.isaac.lab.managers import CommandTermCfg
-from omni.isaac.lab.utils.configclass import configclass
-from typing_extensions import Literal
-
-from .path_follower_command_generator import PathFollowerCommandGenerator
-
-
-@configclass
-class PathFollowerCommandGeneratorCfg(CommandTermCfg):
-    class_type: PathFollowerCommandGenerator = PathFollowerCommandGenerator
-    """Name of the command generator class."""
-
-    robot_attr: str = MISSING
-    """Name of the robot attribute from the environment."""
-
-    path_frame: Literal["world", "robot"] = "world"
-    """Frame in which the path is defined.
-    - ``world``: the path is defined in the world frame. Also called ``odom``.
-    - ``robot``: the path is defined in the robot frame. Also called ``base``.
-    """
-
-    lookAheadDistance: float = MISSING
-    """The lookahead distance for the path follower."""
-    two_way_drive: bool = False
-    """Allow robot to use reverse gear."""
-    switch_time_threshold: float = 1.0
-    """Time threshold to switch between the forward and backward drive."""
-    maxSpeed: float = 0.5
-    """Maximum speed of the robot."""
-    maxAccel: float = 2.5 / 100.0  # 2.5 / 100
-    """Maximum acceleration of the robot."""
-    joyYaw: float = 1.0
-    """TODO: add description"""
-    yawRateGain: float = 7.0  # 3.5
-    """Gain for the yaw rate."""
-    stopYawRateGain: float = 7.0  # 3.5
-    """"""
-    maxYawRate: float = 90.0 * math.pi / 360
-    dirDiffThre: float = 0.7
-    stopDisThre: float = 0.2
-    slowDwnDisThre: float = 0.3
-    slowRate1: float = 0.25
-    slowRate2: float = 0.5
-    noRotAtGoal: bool = True
-    autonomyMode: bool = False
-
-    dynamic_lookahead: bool = False
-    min_points_within_lookahead: int = 3
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator_gpt.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator_gpt.py
deleted file mode 100644
index 7c07fdb..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator_gpt.py
+++ /dev/null
@@ -1,223 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Copyright (c) 2022-2024, The lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Sub-module containing command generators for the velocity-based locomotion task."""
-from __future__ import annotations
-
-import math
-from typing import TYPE_CHECKING, Sequence
-
-import omni.isaac.lab.utils.math as math_utils
-import torch
-from omni.isaac.lab.assets.articulation import Articulation
-from omni.isaac.lab.envs import ManagerBasedRLEnv
-from omni.isaac.lab.managers import CommandTerm
-from omni.isaac.lab.markers import VisualizationMarkers
-from omni.isaac.lab.markers.config import (
-    BLUE_ARROW_X_MARKER_CFG,
-    GREEN_ARROW_X_MARKER_CFG,
-)
-from omni.isaac.lab.sim import SimulationContext
-
-if TYPE_CHECKING:
-    from .path_follower_command_generator_gpt_cfg import PathFollowerCommandGeneratorGPTCfg
-
-
-class PathFollowerCommandGeneratorGPT(CommandTerm):
-    r"""Command generator that generates a velocity command in SE(2) from a path given by a local planner.
-
-    The command comprises of a linear velocity in x and y direction and an angular velocity around
-    the z-axis. It is given in the robot's base frame.
-
-    The path follower acts as a PD-controller that checks for the last point on the path within a lookahead distance
-    and uses it to compute the steering angle and the linear velocity.
-    """
-
-    cfg: PathFollowerCommandGeneratorGPTCfg
-    """The configuration of the command generator."""
-
-    def __init__(self, cfg: PathFollowerCommandGeneratorGPTCfg, env: ManagerBasedRLEnv):
-        """Initialize the command generator.
-
-        Args:
-            cfg (PathFollowerCommandGeneratorCfg): The configuration of the command generator.
-            env (object): The environment.
-        """
-        super().__init__(cfg, env)
-        # -- robot
-        self.robot: Articulation = env.scene[cfg.robot_attr]
-        # -- Simulation Context
-        self.sim: SimulationContext = SimulationContext.instance()
-        # -- buffers
-        self.twist: torch.Tensor = torch.zeros((self.num_envs, 3), device=self.device)
-        # -- debug vis
-        self._base_vel_goal_markers = None
-        self._base_vel_markers = None
-
-        # Rotation mark
-        self.rotation_mark = False
-        self.initialized = False
-        self.goal_reached = False
-
-    def __str__(self) -> str:
-        """Return a string representation of the command generator."""
-        msg = "PathFollowerCommandGenerator:\n"
-        msg += f"\tCommand dimension: {tuple(self.command.shape[1:])}\n"
-        return msg
-
-    """
-    Properties
-    """
-
-    @property
-    def command(self) -> torch.Tensor:
-        """The desired base velocity command in the base frame. Shape is (num_envs, 3)."""
-        return self.twist
-
-    """
-    Operations.
-    """
-
-    def reset(self, env_ids: Sequence[int] | None = None) -> dict:
-        """Reset the command generator.
-
-        This function resets the command generator. It should be called whenever the environment is reset.
-
-        Args:
-            env_ids (Optional[Sequence[int]], optional): The list of environment IDs to reset. Defaults to None.
-        """
-        if env_ids is None:
-            env_ids = ...
-
-        self.twist = torch.zeros((self.num_envs, 3), device=self.device)
-        self.goal_reached = False
-
-        return {}
-
-    def compute(self, dt: float):
-        """Compute the command.
-
-        Paths as a tensor of shape (num_envs, N, 3) where N is number of poses on the path. The paths
-        should be given in the base frame of the robot. Num_envs is equal to the number of robots spawned in all
-        environments.
-        """
-        # get paths
-        goals = self._env.action_manager._terms['vlm_actions_gpt']._processed_command_velocity_actions.clone()
-        # get number of pases of the paths
-        num_envs, N = goals.shape
-        assert N > 0, "PathFollowerCommandGenerator: paths must have at least one poses."
-        # get the current simulation time
-        curr_time = self.sim.current_time
-        # define current maxSpeed for the velocities
-        max_speed = torch.ones(num_envs, device=self.device) * self.cfg.maxSpeed
-
-        # # transform path in base/ robot frame if given in world frame
-        # if self.cfg.path_frame == "world":
-        #     paths = math_utils.quat_apply(
-        #         math_utils.quat_inv(self.robot.data.root_quat_w[:, None, :].repeat(1, N, 1)),
-        #         paths - self.robot.data.root_pos_w[:, None, :],
-        #     )
-        # self.paths_diff_global = goals[:,:2] #- self.robot.data.root_pos_w[:, :2]
-        # yaw_angle_
-
-        # if self.initialized and not self.goal_reached:
-        #     if not self.rotation_mark:
-        #         # if abs(self.paths_diff_global[0,0,0]) < 0.5:
-        #         #     self.twist[:, 0] = 0.0
-        #         # else:
-        #         self.twist[:, 0] = min(max(self.paths_diff_global[0,0], -0.5), 0.5)
-        #         # if abs(self.paths_diff_global[0,0,1]) < 2.0:
-        #         #     self.twist[:, 1] = 0.0
-        #         # else:
-        #         self.twist[:, 1] = 0.0#min(max(self.paths_diff_global[0,1], -0.3), 0.3)
-        #         self.twist[:, 2] = min(max(0.01*goals[0,2], -0.2), 0.2)
-        #         # TODO: add yaw rotation mechanism
-        #     else:
-        #         self.twist[:,0] = 0.0
-        #         self.twist[:,1] = 0.0
-        #         self.twist[:,2] = 0.5
-        # else:
-        #     self.twist[:,0] = 0.0
-        #     self.twist[:,1] = 0.0
-        #     self.twist[:,2] = 0.0
-        self.twist[0,:3] = goals[0,:3]
-        # if (torch.linalg.norm(self.paths_diff_global[0,:2], dim=-1))<3.0 and self.initialized:
-        #     self.goal_reached = True
-        # print("goal_reached: ", self.goal_reached, " rotation_mark: ", self.rotation_mark, " twist: ", self.twist[:,:3], " norm: ", torch.linalg.norm(self.paths_diff_global[0,0,:2], dim=-1), " initialized: ", self.initialized)
-
-        return self.twist
-
-    """
-    Implementation specific functions.
-    """
-
-    def _update_command(self):
-        pass
-
-    def _update_metrics(self):
-        pass
-
-    def _resample_command(self, env_ids: Sequence[int]):
-        pass
-
-    def _set_debug_vis_impl(self, debug_vis: bool):
-        # set visibility of markers
-        # note: parent only deals with callbacks. not their visibility
-        if debug_vis:
-            # create markers if necessary for the first tome
-            if not hasattr(self, "base_vel_goal_visualizer"):
-                # -- goal
-                marker_cfg = GREEN_ARROW_X_MARKER_CFG.copy()
-                marker_cfg.prim_path = "/Visuals/Command/velocity_goal"
-                marker_cfg.markers["arrow"].scale = (0.5, 0.5, 0.5)
-                self.base_vel_goal_visualizer = VisualizationMarkers(marker_cfg)
-                # -- current
-                marker_cfg = BLUE_ARROW_X_MARKER_CFG.copy()
-                marker_cfg.prim_path = "/Visuals/Command/velocity_current"
-                marker_cfg.markers["arrow"].scale = (0.5, 0.5, 0.5)
-                self.base_vel_visualizer = VisualizationMarkers(marker_cfg)
-            # set their visibility to true
-            self.base_vel_goal_visualizer.set_visibility(True)
-            self.base_vel_visualizer.set_visibility(True)
-        else:
-            if hasattr(self, "base_vel_goal_visualizer"):
-                self.base_vel_goal_visualizer.set_visibility(False)
-                self.base_vel_visualizer.set_visibility(False)
-
-    def _debug_vis_callback(self, event):
-        # get marker location
-        # -- base state
-        base_pos_w = self.robot.data.root_pos_w.clone()
-        base_pos_w[:, 2] += 0.5
-        # -- resolve the scales and quaternions
-        vel_des_arrow_scale, vel_des_arrow_quat = self._resolve_xy_velocity_to_arrow(self.command[:, :2])
-        vel_arrow_scale, vel_arrow_quat = self._resolve_xy_velocity_to_arrow(self.robot.data.root_lin_vel_b[:, :2])
-        # display markers
-        self.base_vel_goal_visualizer.visualize(base_pos_w, vel_des_arrow_quat, vel_des_arrow_scale)
-        self.base_vel_visualizer.visualize(base_pos_w, vel_arrow_quat, vel_arrow_scale)
-
-    """
-    Internal helpers.
-    """
-
-    def _resolve_xy_velocity_to_arrow(self, xy_velocity: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
-        """Converts the XY base velocity command to arrow direction rotation."""
-        # obtain default scale of the marker
-        default_scale = self.base_vel_goal_visualizer.cfg.markers["arrow"].scale
-        # arrow-scale
-        arrow_scale = torch.tensor(default_scale, device=self.device).repeat(xy_velocity.shape[0], 1)
-        arrow_scale[:, 0] *= torch.linalg.norm(xy_velocity, dim=1)
-        # arrow-direction
-        heading_angle = torch.atan2(xy_velocity[:, 1], xy_velocity[:, 0])
-        zeros = torch.zeros_like(heading_angle)
-        arrow_quat = math_utils.quat_from_euler_xyz(zeros, zeros, heading_angle)
-
-        return arrow_scale, arrow_quat
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator_gpt_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator_gpt_cfg.py
deleted file mode 100644
index a757113..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/path_follower_command_generator_gpt_cfg.py
+++ /dev/null
@@ -1,64 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Copyright (c) 2022-2024, The lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Sub-module containing command generators for the velocity-based locomotion task."""
-
-import math
-from dataclasses import MISSING
-
-from omni.isaac.lab.managers import CommandTermCfg
-from omni.isaac.lab.utils.configclass import configclass
-from typing_extensions import Literal
-
-from .path_follower_command_generator_gpt import PathFollowerCommandGeneratorGPT
-
-
-@configclass
-class PathFollowerCommandGeneratorGPTCfg(CommandTermCfg):
-    class_type: PathFollowerCommandGeneratorGPT = PathFollowerCommandGeneratorGPT
-    """Name of the command generator class."""
-
-    robot_attr: str = MISSING
-    """Name of the robot attribute from the environment."""
-
-    path_frame: Literal["world", "robot"] = "world"
-    """Frame in which the path is defined.
-    - ``world``: the path is defined in the world frame. Also called ``odom``.
-    - ``robot``: the path is defined in the robot frame. Also called ``base``.
-    """
-
-    lookAheadDistance: float = MISSING
-    """The lookahead distance for the path follower."""
-    two_way_drive: bool = False
-    """Allow robot to use reverse gear."""
-    switch_time_threshold: float = 1.0
-    """Time threshold to switch between the forward and backward drive."""
-    maxSpeed: float = 0.5
-    """Maximum speed of the robot."""
-    maxAccel: float = 2.5 / 100.0  # 2.5 / 100
-    """Maximum acceleration of the robot."""
-    joyYaw: float = 1.0
-    """TODO: add description"""
-    yawRateGain: float = 7.0  # 3.5
-    """Gain for the yaw rate."""
-    stopYawRateGain: float = 7.0  # 3.5
-    """"""
-    maxYawRate: float = 90.0 * math.pi / 360
-    dirDiffThre: float = 0.7
-    stopDisThre: float = 0.2
-    slowDwnDisThre: float = 0.3
-    slowRate1: float = 0.25
-    slowRate2: float = 0.5
-    noRotAtGoal: bool = True
-    autonomyMode: bool = False
-
-    dynamic_lookahead: bool = False
-    min_points_within_lookahead: int = 3
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/rl_command_generator.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/rl_command_generator.py
deleted file mode 100644
index a0d129d..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/rl_command_generator.py
+++ /dev/null
@@ -1,202 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Copyright (c) 2022-2024, The lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Sub-module containing command generators for the velocity-based locomotion task."""
-from __future__ import annotations
-
-import math
-from typing import TYPE_CHECKING, Sequence
-
-import omni.isaac.lab.utils.math as math_utils
-import torch
-from omni.isaac.lab.assets.articulation import Articulation
-from omni.isaac.lab.envs import ManagerBasedRLEnv
-from omni.isaac.lab.managers import CommandTerm
-from omni.isaac.lab.markers import VisualizationMarkers
-from omni.isaac.lab.markers.config import (
-    BLUE_ARROW_X_MARKER_CFG,
-    GREEN_ARROW_X_MARKER_CFG,
-    CUBOID_MARKER_CFG,
-)
-from omni.isaac.lab.sim import SimulationContext
-
-if TYPE_CHECKING:
-    from .rl_command_generator_cfg import RLCommandGeneratorCfg
-
-
-class RLCommandGenerator(CommandTerm):
-    r"""Command generator that generates a velocity command in SE(2) from a path given by a local planner.
-
-    The command comprises of a linear velocity in x and y direction and an angular velocity around
-    the z-axis. It is given in the robot's base frame.
-
-    The path follower acts as a PD-controller that checks for the last point on the path within a lookahead distance
-    and uses it to compute the steering angle and the linear velocity.
-    """
-
-    cfg: RLCommandGeneratorCfg
-    """The configuration of the command generator."""
-
-    def __init__(self, cfg: RLCommandGeneratorCfg, env: ManagerBasedRLEnv):
-        """Initialize the command generator.
-
-        Args:
-            cfg (RLCommandGeneratorCfg): The configuration of the command generator.
-            env (object): The environment.
-        """
-        super().__init__(cfg, env)
-        # -- robot
-        self.robot: Articulation = env.scene[cfg.robot_attr]
-        # -- Simulation Context
-        self.sim: SimulationContext = SimulationContext.instance()
-        self.twist: torch.Tensor = torch.zeros((self.num_envs, 3), device=self.device)
-        # -- debug vis
-        self._base_vel_goal_markers = None
-        self._base_vel_markers = None
-
-        # Rotation mark
-        self.rotation_mark = False
-        self.initialized = False
-        self.goal_reached = False
-        self.identity_quat = torch.tensor([1.0, 0.0, 0.0, 0.0], device=self._env.device).repeat(self._env.num_envs, 1)
-
-    def __str__(self) -> str:
-        """Return a string representation of the command generator."""
-        msg = "rlCommandGenerator:\n"
-        msg += f"\tCommand dimension: {tuple(self.command.shape[1:])}\n"
-        msg += f"\tLookahead distance: {self.cfg.lookAheadDistance}\n"
-        return msg
-
-    """
-    Properties
-    """
-
-    @property
-    def command(self) -> torch.Tensor:
-        """The desired base velocity command in the base frame. Shape is (num_envs, 3)."""
-        # print("twist: ", self.twist)
-        return self.twist
-
-    """
-    Operations.
-    """
-
-    def reset(self, env_ids: Sequence[int] | None = None) -> dict:
-        """Reset the command generator.
-
-        This function resets the command generator. It should be called whenever the environment is reset.
-
-        Args:
-            env_ids (Optional[Sequence[int]], optional): The list of environment IDs to reset. Defaults to None.
-        """
-        if env_ids is None:
-            env_ids = ...
-
-        self.twist = torch.zeros((self.num_envs, 3), device=self.device)
-        self.goal_reached = False
-
-        return {}
-
-    def compute(self, dt: float):
-        """Compute the command.
-
-        Paths as a tensor of shape (num_envs, N, 3) where N is number of poses on the path. The paths
-        should be given in the base frame of the robot. Num_envs is equal to the number of robots spawned in all
-        environments.
-        """
-        # get paths
-        self.twist = self._env.action_manager._terms['paths']._processed_navigation_velocity_actions.clone()
-        self.twist[:, 0] = torch.clip(self.twist[:, 0], 0.0, 0.5)
-        self.twist[:,1] = 0.0
-        self.twist[:,2] = torch.clip(self.twist[:,2], -math.pi, math.pi)
-
-        return self.twist
-
-    """
-    Implementation specific functions.
-    """
-
-    def _update_command(self):
-        pass
-
-    def _update_metrics(self):
-        pass
-
-    def _resample_command(self, env_ids: Sequence[int]):
-        pass
-
-    def _set_debug_vis_impl(self, debug_vis: bool):
-        # set visibility of markers
-        # note: parent only deals with callbacks. not their visibility
-        if debug_vis:
-            # create markers if necessary for the first tome
-            if not hasattr(self, "base_vel_goal_visualizer"):
-                # -- goal
-                marker_cfg = GREEN_ARROW_X_MARKER_CFG.copy()
-                marker_cfg.prim_path = "/Visuals/Command/velocity_goal"
-                marker_cfg.markers["arrow"].scale = (0.5, 0.5, 0.5)
-                self.base_vel_goal_visualizer = VisualizationMarkers(marker_cfg)
-                # -- current
-                marker_cfg = BLUE_ARROW_X_MARKER_CFG.copy()
-                marker_cfg.prim_path = "/Visuals/Command/velocity_current"
-                marker_cfg.markers["arrow"].scale = (0.5, 0.5, 0.5)
-                self.base_vel_visualizer = VisualizationMarkers(marker_cfg)
-                # -- goal command
-                marker_cfg = CUBOID_MARKER_CFG.copy()
-                marker_cfg.prim_path = "/Visuals/Command/pos_goal_command"
-                marker_cfg.markers["cuboid"].scale = (0.5, 0.5, 0.5)
-                self.base_vel_goal_command_visualizer = VisualizationMarkers(marker_cfg)
-            # set their visibility to true
-            self.base_vel_goal_visualizer.set_visibility(True)
-            self.base_vel_visualizer.set_visibility(True)
-            self.base_vel_goal_command_visualizer.set_visibility(True)
-        else:
-            if hasattr(self, "base_vel_goal_visualizer"):
-                self.base_vel_goal_visualizer.set_visibility(False)
-                self.base_vel_visualizer.set_visibility(False)
-                self.base_vel_goal_command_visualizer.set_visibility(False)
-
-    def _debug_vis_callback(self, event):
-        # get marker location
-        # -- base state
-        base_pos_w = self.robot.data.root_pos_w.clone()
-        base_pos_w[:, 2] += 0.5
-        base_quat_w = self.robot.data.root_quat_w.clone()
-
-        # -- resolve the scales and quaternions
-        vel_des_arrow_scale, vel_des_arrow_quat = self._resolve_xy_velocity_to_arrow(self.command[:, :2])
-        vel_arrow_scale, vel_arrow_quat = self._resolve_xy_velocity_to_arrow(self.robot.data.root_lin_vel_b[:, :2])
-        # display markers
-        self.base_vel_goal_visualizer.visualize(base_pos_w, vel_des_arrow_quat, vel_des_arrow_scale)
-        self.base_vel_visualizer.visualize(base_pos_w, vel_arrow_quat, vel_arrow_scale)
-
-        pos_command_w = self._env.command_manager._terms['goal_command'].pos_command_w.clone()
-        default_scale = self.base_vel_goal_visualizer.cfg.markers["arrow"].scale
-        larger_scale = 2.0*torch.tensor(default_scale, device=self.device).repeat(pos_command_w.shape[0], 1)
-        self.base_vel_goal_command_visualizer.visualize(pos_command_w, self.identity_quat,larger_scale)
-
-    """
-    Internal helpers.
-    """
-
-    def _resolve_xy_velocity_to_arrow(self, xy_velocity: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
-        """Converts the XY base velocity command to arrow direction rotation."""
-        # obtain default scale of the marker
-        default_scale = self.base_vel_goal_visualizer.cfg.markers["arrow"].scale
-        # arrow-scale
-        arrow_scale = torch.tensor(default_scale, device=self.device).repeat(xy_velocity.shape[0], 1)
-        arrow_scale[:, 0] *= torch.linalg.norm(xy_velocity, dim=1)
-        # arrow-direction
-        heading_angle = torch.atan2(xy_velocity[:, 1], xy_velocity[:, 0])
-        zeros = torch.zeros_like(heading_angle)
-        arrow_quat = math_utils.quat_from_euler_xyz(zeros, zeros, heading_angle)
-
-        return arrow_scale, arrow_quat
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/rl_command_generator_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/rl_command_generator_cfg.py
deleted file mode 100644
index a3dba7a..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/rl_command_generator_cfg.py
+++ /dev/null
@@ -1,37 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Copyright (c) 2022-2024, The lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Sub-module containing command generators for the velocity-based locomotion task."""
-
-import math
-from dataclasses import MISSING
-
-from omni.isaac.lab.managers import CommandTermCfg
-from omni.isaac.lab.utils.configclass import configclass
-from typing_extensions import Literal
-
-from .rl_command_generator import RLCommandGenerator
-
-
-@configclass
-class RLCommandGeneratorCfg(CommandTermCfg):
-    class_type: RLCommandGenerator = RLCommandGenerator
-    """Name of the command generator class."""
-
-    robot_attr: str = MISSING
-    """Name of the robot attribute from the environment."""
-
-    path_frame: Literal["world", "robot"] = "world"
-    """Frame in which the path is defined.
-    - ``world``: the path is defined in the world frame. Also called ``odom``.
-    - ``robot``: the path is defined in the robot frame. Also called ``base``.
-    """
-
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/robot_vel_command_generator.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/robot_vel_command_generator.py
deleted file mode 100644
index da8f45c..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/robot_vel_command_generator.py
+++ /dev/null
@@ -1,124 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Copyright (c) 2022-2024, The lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Sub-module containing command generators for the velocity-based locomotion task."""
-from __future__ import annotations
-
-import math
-from typing import TYPE_CHECKING, Sequence
-
-import omni.isaac.lab.utils.math as math_utils
-import torch
-from omni.isaac.lab.assets.articulation import Articulation
-from omni.isaac.lab.envs import ManagerBasedRLEnv
-from omni.isaac.lab.managers import CommandTerm
-from omni.isaac.lab.sim import SimulationContext
-
-if TYPE_CHECKING:
-    from .robot_vel_command_generator_cfg import RobotVelCommandGeneratorCfg
-
-
-class RobotVelCommandGenerator(CommandTerm):
-    r"""Command generator that generates a velocity command in SE(2) from a path given by a local planner.
-
-    The command comprises of a linear velocity in x and y direction and an angular velocity around
-    the z-axis. It is given in the robot's base frame.
-
-    The path follower acts as a PD-controller that checks for the last point on the path within a lookahead distance
-    and uses it to compute the steering angle and the linear velocity.
-    """
-
-    cfg: RobotVelCommandGeneratorCfg
-    """The configuration of the command generator."""
-
-    def __init__(self, cfg: RobotVelCommandGeneratorCfg, env: ManagerBasedRLEnv):
-        """Initialize the command generator.
-
-        Args:
-            cfg (RLCommandGeneratorCfg): The configuration of the command generator.
-            env (object): The environment.
-        """
-        super().__init__(cfg, env)
-
-        # -- robot
-        self.robot: Articulation = env.scene[cfg.robot_attr]
-        # -- Simulation Context
-        self.sim: SimulationContext = SimulationContext.instance()
-        self.command_b: torch.Tensor = torch.zeros((self.num_envs, 3), device=self.device)
-        # -- debug vis
-        self._base_vel_goal_markers = None
-        self._base_vel_markers = None
-
-        # Rotation mark
-        self.rotation_mark = False
-        self.initialized = False
-
-    def __str__(self) -> str:
-        """Return a string representation of the command generator."""
-        msg = "rlCommandGenerator:\n"
-        msg += f"\tCommand dimension: {tuple(self.command.shape[1:])}\n"
-        msg += f"\tLookahead distance: {self.cfg.lookAheadDistance}\n"
-        return msg
-
-    """
-    Properties
-    """
-
-    @property
-    def command(self) -> torch.Tensor:
-        """The desired base velocity command in the base frame. Shape is (num_envs, 3)."""
-        # print("twist: ", self.twist)
-        return self.command_b
-
-    """
-    Operations.
-    """
-
-    def reset(self, env_ids: Sequence[int] | None = None) -> dict:
-        """Reset the command generator.
-
-        This function resets the command generator. It should be called whenever the environment is reset.
-
-        Args:
-            env_ids (Optional[Sequence[int]], optional): The list of environment IDs to reset. Defaults to None.
-        """
-        if env_ids is None:
-            env_ids = ...
-
-        self.command_b = torch.zeros((self.num_envs, 3), device=self.device)
-        self.goal_reached = False
-
-        return {}
-
-    def compute(self, dt: float):
-        """Compute the command.
-
-        Paths as a tensor of shape (num_envs, N, 3) where N is number of poses on the path. The paths
-        should be given in the base frame of the robot. Num_envs is equal to the number of robots spawned in all
-        environments.
-        """
-        # get paths
-        self.command_b[:,:3] = self._env.action_manager._terms['vlm_actions']._processed_command_velocity_actions.clone()[:,:3]
-
-        return self.command_b
-
-    """
-    Implementation specific functions.
-    """
-
-    def _update_command(self):
-        pass
-
-    def _update_metrics(self):
-        pass
-
-    def _resample_command(self, env_ids: Sequence[int]):
-        pass
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/robot_vel_command_generator_cfg.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/robot_vel_command_generator_cfg.py
deleted file mode 100644
index c04142f..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/commands/robot_vel_command_generator_cfg.py
+++ /dev/null
@@ -1,37 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Copyright (c) 2022-2024, The lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Sub-module containing command generators for the velocity-based locomotion task."""
-
-import math
-from dataclasses import MISSING
-
-from omni.isaac.lab.managers import CommandTermCfg
-from omni.isaac.lab.utils.configclass import configclass
-from typing_extensions import Literal
-
-from .robot_vel_command_generator import RobotVelCommandGenerator
-
-
-@configclass
-class RobotVelCommandGeneratorCfg(CommandTermCfg):
-    class_type: RobotVelCommandGenerator = RobotVelCommandGenerator
-    """Name of the command generator class."""
-
-    robot_attr: str = MISSING
-    """Name of the robot attribute from the environment."""
-
-    # path_frame: Literal["world", "robot"] = "world"
-    """Frame in which the path is defined.
-    - ``world``: the path is defined in the world frame. Also called ``odom``.
-    - ``robot``: the path is defined in the robot frame. Also called ``base``.
-    """
-
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/curriculums.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/curriculums.py
deleted file mode 100644
index eb6e2a2..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/curriculums.py
+++ /dev/null
@@ -1,55 +0,0 @@
-# Copyright (c) 2022-2024, The lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Common functions that can be used to create curriculum for the learning environment.
-
-The functions can be passed to the :class:`omni.isaac.lab.managers.CurriculumTermCfg` object to enable
-the curriculum introduced by the function.
-"""
-
-from __future__ import annotations
-
-import torch
-from collections.abc import Sequence
-from typing import TYPE_CHECKING
-
-from omni.isaac.lab.assets import Articulation
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.terrains import TerrainImporter
-
-if TYPE_CHECKING:
-    from omni.isaac.lab.envs import ManagerBasedRLEnv
-
-
-def terrain_levels_vel(
-    env: ManagerBasedRLEnv, env_ids: Sequence[int], asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
-) -> torch.Tensor:
-    """Curriculum based on the distance the robot walked when commanded to move at a desired velocity.
-
-    This term is used to increase the difficulty of the terrain when the robot walks far enough and decrease the
-    difficulty when the robot walks less than half of the distance required by the commanded velocity.
-
-    .. note::
-        It is only possible to use this term with the terrain type ``generator``. For further information
-        on different terrain types, check the :class:`omni.isaac.lab.terrains.TerrainImporter` class.
-
-    Returns:
-        The mean terrain level for the given environment ids.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    terrain: TerrainImporter = env.scene.terrain
-    command = env.command_manager.get_command("base_velocity")
-    # compute the distance the robot walked
-    distance = torch.norm(asset.data.root_pos_w[env_ids, :2] - env.scene.env_origins[env_ids, :2], dim=1)
-    # robots that walked far enough progress to harder terrains
-    move_up = distance > terrain.cfg.terrain_generator.size[0] / 2
-    # robots that walked less than half of their required distance go to simpler terrains
-    move_down = distance < torch.norm(command[env_ids, :2], dim=1) * env.max_episode_length_s * 0.5
-    move_down *= ~move_up
-    # update terrain levels
-    terrain.update_env_origins(env_ids, move_up, move_down)
-    # return the mean terrain level
-    return torch.mean(terrain.terrain_levels.float())
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/events.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/events.py
deleted file mode 100644
index 7f95548..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/events.py
+++ /dev/null
@@ -1,48 +0,0 @@
-import torch
-
-import omni.isaac.lab.sim as sim_utils
-import omni.isaac.lab.utils.math as math_utils
-from omni.isaac.lab.actuators import ImplicitActuator
-from omni.isaac.lab.assets import Articulation, RigidObject
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.terrains import TerrainImporter
-
-
-from omni.isaac.lab.envs import ManagerBasedEnv
-
-
-def reset_camera_pos_uniform(
-    env: ManagerBasedEnv,
-    env_ids: torch.Tensor,
-    asset_cfg: SceneEntityCfg,
-    pose_delta_range: dict[str, tuple[float, float]],
-):
-    """Reset the camera to a random position uniformly within the given ranges.
-
-    This function randomizes the position of the asset.
-
-    * It samples the delta position from the given ranges and adds them to the default camera position, before setting
-      them into the physics simulation.
-    * It samples the root orientation from the given ranges and sets them into the physics simulation.
-    * It samples the root velocity from the given ranges and sets them into the physics simulation.
-
-    The function takes a dictionary of pose and velocity ranges for each axis and rotation. The keys of the
-    dictionary are ``x``, ``y``, ``z``, ``roll``, ``pitch``, and ``yaw``. The values are tuples of the form
-    ``(min, max)``. If the dictionary does not contain a key, the position or velocity is set to zero for that axis.
-    """
-    # extract the used quantities (to enable type-hinting)
-    camera: RigidObject | Articulation = env.scene[asset_cfg.name]
-    # get default root state
-    pos_w, quat_w = camera._compute_view_world_poses(env_ids)
-
-    # poses
-    range_list = [pose_delta_range.get(key, (0.0, 0.0)) for key in ["x", "y", "z", "roll", "pitch", "yaw"]]
-    ranges = torch.tensor(range_list, device=camera.device)
-    rand_samples = math_utils.sample_uniform(ranges[:, 0], ranges[:, 1], (len(env_ids), 6), device=camera.device)
-
-    positions = pos_w + rand_samples[:, 0:3]
-    orientations_delta = math_utils.quat_from_euler_xyz(rand_samples[:, 3], rand_samples[:, 4], rand_samples[:, 5])
-    orientations = math_utils.quat_mul(quat_w, orientations_delta)
-
-    # set into the physics simulation
-    camera.set_world_poses(positions, orientations, env_ids=env_ids, convention="world")
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/observations.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/observations.py
deleted file mode 100644
index 0ba97c1..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/observations.py
+++ /dev/null
@@ -1,386 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""This sub-module contains observation terms specific for viplanner.
-
-The functions can be passed to the :class:`omni.isaac.lab.managers.ObservationTermCfg` object to enable
-the observation introduced by the function.
-"""
-
-from __future__ import annotations
-
-from typing import TYPE_CHECKING
-
-import os
-import torch
-import torch.nn.functional as F
-
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.sensors import RayCaster
-from omni.isaac.lab.sensors.camera import CameraData
-from omni.isaac.lab.sensors.camera.utils import convert_orientation_convention
-from viplanner.config import VIPlannerSemMetaHandler
-import omni.isaac.lab.utils.math as math_utils
-from omni.isaac.lab.assets import Articulation, RigidObject
-
-from .actions import NavigationAction, VLMActions, VLMActionsGPT
-import matplotlib.pyplot as plt
-import cv2
-
-if TYPE_CHECKING:
-    from omni.isaac.lab.envs.base_env import BaseEnv
-    from omni.isaac.lab.envs import ManagerBasedEnv
-
-
-def matterport_raycast_camera_data(env: BaseEnv, sensor_cfg: SceneEntityCfg, data_type: str) -> torch.Tensor:
-    """Images generated by the raycast camera."""
-    # extract the used quantities (to enable type-hinting)
-    sensor: CameraData = env.scene.sensors[sensor_cfg.name].data
-
-    # return the data
-    if data_type == "distance_to_image_plane":
-        output = sensor.output[data_type].clone().unsqueeze(1)
-        output[torch.isnan(output)] = 0.0
-        output[torch.isinf(output)] = 0.0
-        return output
-    else:
-        return sensor.output[data_type].clone().permute(0, 3, 1, 2)
-
-def isaac_camera_data(env: BaseEnv, sensor_cfg: SceneEntityCfg, data_type: str) -> torch.Tensor:
-    """Images generated by the usd camera."""
-    # extract the used quantities (to enable type-hinting)
-    sensor: CameraData = env.scene.sensors[sensor_cfg.name].data
-
-    # import ipdb; ipdb.set_trace()
-    # return the data
-    if data_type == "distance_to_image_plane":
-        output = sensor.output[data_type].clone().unsqueeze(1)
-        output[torch.isnan(output)] = 0.0
-        output[torch.isinf(output)] = 0.0
-        # output = torch.clip(output, 0.0, 10.0)
-        # near_clip = 0.0
-        # far_clip = 10.0
-        # output = (output - near_clip) / (far_clip - near_clip)  - 0.5
-        # depth_image_size = (output.shape[2], output.shape[3])
-        # output_clone = output.clone().reshape(env.num_envs, depth_image_size[0], depth_image_size[1])[0,:,:]
-        # window_name = "Depth Image"
-        # cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
-        # cv2.imshow("Depth Image", output_clone.cpu().numpy())
-        # cv2.waitKey(1)
-        return output
-    else:
-        # import ipdb; ipdb.set_trace()
-        rgb_image = sensor.output[data_type].clone().cpu().numpy()[0,:,:,:]
-        # # depth_image_size = (output.shape[2], output.shape[3])
-        # output_clone = output.clone().reshape(env.num_envs, depth_image_size[0], depth_image_size[1])[0,:,:]
-        # window_name = "RGB Image"
-        # cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
-        # cv2.imshow(window_name, rgb_image)
-        # cv2.waitKey(1)
-        return sensor.output[data_type].clone()
-
-def process_depth_image(env: BaseEnv, sensor_cfg: SceneEntityCfg, data_type: str, visualize=False) -> torch.Tensor:
-    """Process the depth image."""
-    # import ipdb; ipdb.set_trace()
-    # extract the used quantities (to enable type-hinting)
-    sensor: CameraData = env.scene.sensors[sensor_cfg.name].data
-
-    output = sensor.output[data_type].clone().unsqueeze(1)
-    # # output = output[:,:, :-2, 4:-4]
-    near_clip = 0.3
-    far_clip = 2.0
-    output[torch.isnan(output)] = far_clip
-    output[torch.isinf(output)] = far_clip
-
-    # depth_image_size = (output.shape[2], output.shape[3])
-    # output_clone = output.clone().reshape(env.num_envs, depth_image_size[0], depth_image_size[1])[0,:,:]
-    # window_name = "Before clipping"
-    # import ipdb; ipdb.set_trace()
-    # cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
-    # cv2.imshow(window_name, output_clone.cpu().numpy())
-    # cv2.waitKey(1)
-
-    
-    output = torch.clip(output, near_clip, far_clip)
-    output = output - near_clip
-    # output = F.interpolate(output, size=(53, 30), mode='nearest')
-    # depth_image_size = (output.shape[2], output.shape[3])
-    # output_clone = output.clone().reshape(env.num_envs, depth_image_size[0], depth_image_size[1])[0,:,:]
-    # window_name = "after clipping and normalization"
-    # cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
-    # cv2.imshow(window_name, output_clone.cpu().numpy())
-    # cv2.waitKey(1)
-
-
-    # process_depth_feature = env.action_manager._terms['paths'].depth_cnn(output)
-    # print("depth shape: ", output.reshape(env.num_envs, -1).shape)
-    # import ipdb; ipdb.set_trace()
-    # path = "/home/yji/Biped/biped_vision/depth_image/"
-    # path=os.path.join(path, str(env.action_manager._terms['paths'].image_count)+".png")
-    # import ipdb; ipdb.set_trace()
-    if visualize:
-        depth_image_size = (output.shape[2], output.shape[3])
-        output_clone = output.clone().reshape(env.num_envs, depth_image_size[0], depth_image_size[1])[0,:,:]
-        window_name = "Depth Image"
-        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
-        cv2.imshow("Depth Image", output_clone.cpu().numpy())
-        cv2.waitKey(1)
-
-    # plt.imsave(path, output_clone.cpu().numpy(), cmap="gray")
-    
-    # print(output)
-    return output.reshape(env.num_envs, -1)
-
-def process_lidar(env: BaseEnv, sensor_cfg: SceneEntityCfg, offset: float = 0.5) -> torch.Tensor:
-    """Process the lidar input."""
-    # import ipdb; ipdb.set_trace()
-    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
-    # import pdb; pdb.set_trace()
-    output = sensor.data.pos_w[:, 2].unsqueeze(1) - sensor.data.ray_hits_w[..., 2] - offset
-    near_clip = 0.0
-    far_clip = 5.0
-    output[torch.isnan(output)] = far_clip
-    output[torch.isinf(output)] = far_clip
-    output = torch.clip(output, near_clip, far_clip)
-    output = (output - near_clip) / (far_clip - near_clip)  - 0.5
-    return output.reshape(env.num_envs, -1)
-
-
-def cam_int_matrix(env: BaseEnv, sensor_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Intrinsic matrix of the camera."""
-    # extract the used quantities (to enable type-hinting)
-    sensor: CameraData = env.scene.sensors[sensor_cfg.name].data
-    
-    return sensor.intrinsic_matrices.clone().reshape(-1,9)
-
-def cam_position(env: BaseEnv, sensor_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Position of the camera."""
-    # extract the used quantities (to enable type-hinting)
-    sensor: CameraData = env.scene.sensors[sensor_cfg.name].data
-
-    return sensor.pos_w.clone()
-
-def cam_orientation(env: BaseEnv, sensor_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Orientation of the camera."""
-    # extract the used quantities (to enable type-hinting)
-    sensor: CameraData = env.scene.sensors[sensor_cfg.name].data
-
-    return sensor.quat_w_world.clone()
-
-def cam_orientation_ros(env: BaseEnv, sensor_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Orientation of the camera."""
-    # extract the used quantities (to enable type-hinting)
-    sensor: CameraData = env.scene.sensors[sensor_cfg.name].data
-    return convert_orientation_convention(sensor.quat_w_world, origin="world", target="ros")
-    # return sensor.quat_w_world.clone()
-
-def low_level_actions(env: BaseEnv) -> torch.Tensor:
-    """Low-level actions."""
-    # extract the used quantities (to enable type-hinting)
-    action_term: NavigationAction = env.action_manager._terms['paths']
-
-    return action_term.low_level_actions.clone()
-
-def low_level_actions_llava(env: BaseEnv) -> torch.Tensor:
-    """Low-level actions."""
-    # extract the used quantities (to enable type-hinting)
-    action_term: VLMActions = env.action_manager._terms['vlm_actions']
-
-    return action_term.low_level_actions.clone()
-
-def low_level_actions_gpt(env: BaseEnv) -> torch.Tensor:
-    """Low-level actions."""
-    # extract the used quantities (to enable type-hinting)
-    action_term: VLMActionsGPT = env.action_manager._terms['vlm_actions_gpt']
-
-    return action_term.low_level_actions.clone()
-
-def last_low_level_actions(env: BaseEnv, action_name: str | None = None) -> torch.Tensor:
-
-    """Low-level actions."""
-    # extract the used quantities (to enable type-hinting)
-    action_term: NavigationAction = env.action_manager._terms['paths']
-
-    return action_term.low_level_actions.clone()
-
-def last_low_level_actions_llava(env: BaseEnv, action_name: str | None = None) -> torch.Tensor:
-
-    """Low-level actions."""
-    # extract the used quantities (to enable type-hinting)
-    action_term: VLMActions = env.action_manager._terms['vlm_actions']
-
-    return action_term.low_level_actions.clone()
-
-def last_low_level_actions_gpt(env: BaseEnv, action_name: str | None = None) -> torch.Tensor:
-
-    """Low-level actions."""
-    # extract the used quantities (to enable type-hinting)
-    action_term: VLMActionsGPT = env.action_manager._terms['vlm_actions_gpt']
-
-    return action_term.low_level_actions.clone()
-
-def last_mid_actions(env: BaseEnv, action_name: str | None = None) -> torch.Tensor:
-    """The last input action to the environment.
-
-    The name of the action term for which the action is required. If None, the
-    entire action tensor is returned.
-    """
-    if action_name is None:
-        return env.action_manager.action
-    else:
-        return env.action_manager.get_term(action_name).raw_actions
-
-def base_lin_acc(env: BaseEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Root linear velocity in the asset's root frame."""
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    lin_acc_w = asset.data.body_lin_acc_w[:, asset_cfg.body_ids[0], :]
-    lin_acc_b = math_utils.quat_rotate_inverse(
-            asset.data.root_quat_w, lin_acc_w
-        )
-    return lin_acc_b
-
-
-def base_ang_acc(env: BaseEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Root angular velocity in the asset's root frame."""
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    ang_acc_w = asset.data.body_ang_acc_w[:, asset_cfg.body_ids[0], :]
-    ang_acc_b = math_utils.quat_rotate_inverse(
-            asset.data.root_quat_w, ang_acc_w
-        )
-    return ang_acc_b
-
-
-def base_rpy(env: ManagerBasedEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Root rotation in the asset's root frame."""
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    root_quat_w = asset.data.root_quat_w
-
-    qw = root_quat_w[:, 0]
-    qx = root_quat_w[:, 1]
-    qy = root_quat_w[:, 2]
-    qz = root_quat_w[:, 3]
-
-    # Roll (x-axis rotation)
-    roll = torch.atan2(2 * (qw * qx + qy * qz), 1 - 2 * (qx * qx + qy * qy))
-
-    # Pitch (y-axis rotation)
-    sinp = 2 * (qw * qy - qz * qx)
-    pitch = torch.where(torch.abs(sinp) < 1, torch.asin(sinp), torch.sign(sinp) * (torch.tensor(torch.pi) / 2))
-
-    # Yaw (z-axis rotation)
-    yaw = torch.atan2(2 * (qw * qz + qx * qy), 1 - 2 * (qy * qy + qz * qz))
-
-    return torch.stack((roll, pitch, yaw), dim=1)
-
-
-# Define voxel grid parameters
-voxel_size_xy = 0.06  # Voxel size in the x and y dimensions
-range_x = [-0.8, 0.2+1e-9]
-# range_y = [-1.0 + 0.05, 1.0 + 0.05]
-range_y = [-0.8, 0.8+1e-9]
-range_z = [0.0, 5.0]
-
-from collections import deque
-# Create a deque with a maximum length of 10
-prev_height_maps = deque(maxlen=10)
-
-def height_map_lidar(env: ManagerBasedEnv, sensor_cfg: SceneEntityCfg, offset: float = 0.5) -> torch.Tensor:
-    """Height scan from the given sensor w.r.t. the sensor's frame.
-
-    The provided offset (Defaults to 0.5) is subtracted from the returned values.
-    """
-    # global prev_height_maps
-
-    # extract the used quantities (to enable type-hinting)
-    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
-
-    hit_vec = sensor.data.ray_hits_w - sensor.data.pos_w.unsqueeze(1)
-    hit_vec[torch.isinf(hit_vec)] = 0.0
-    hit_vec[torch.isnan(hit_vec)] = 0.0
-    
-    hit_vec_shape = hit_vec.shape
-    hit_vec = hit_vec.view(-1, hit_vec.shape[-1])
-    robot_base_quat_w = env.scene["robot"].data.root_quat_w
-    sensor_quat_default = torch.tensor([-0.131, 0.0, -0.991, 0.0], device=robot_base_quat_w.device).unsqueeze(0).repeat(hit_vec_shape[0], 1)
-    sensor_quat_w = math_utils.quat_mul(robot_base_quat_w, sensor_quat_default)
-    quat_w_dup = (sensor_quat_w.unsqueeze(1).repeat(1, hit_vec_shape[1], 1)).view(-1, sensor_quat_w.shape[-1])
-    hit_vec_lidar_frame = math_utils.quat_rotate_inverse(quat_w_dup, hit_vec)
-    hit_vec_lidar_frame = hit_vec_lidar_frame.view(hit_vec_shape[0], hit_vec_shape[1], hit_vec_lidar_frame.shape[-1])
-
-    num_envs = hit_vec_lidar_frame.shape[0]
-
-    # Calculate the number of voxels in each dimension
-    x_bins = torch.arange(range_x[0], range_x[1], voxel_size_xy, device=hit_vec_lidar_frame.device)
-    y_bins = torch.arange(range_y[0], range_y[1], voxel_size_xy, device=hit_vec_lidar_frame.device)
-
-    x = hit_vec_lidar_frame[..., 0]
-    y = hit_vec_lidar_frame[..., 1]
-    z = hit_vec_lidar_frame[..., 2]
-    
-    valid_indices = (x > range_x[0]) & (x <= range_x[1]) & \
-                    (y > range_y[0]) & (y <= range_y[1]) & \
-                    (z >= range_z[0]) & (z <= range_z[1])
-
-    x_filtered = x[valid_indices]
-    y_filtered = y[valid_indices]
-    z_filtered = z[valid_indices]
-
-    x_indices = torch.bucketize(x_filtered, x_bins) - 1
-    y_indices = torch.bucketize(y_filtered, y_bins) - 1
-
-    env_indices = torch.arange(num_envs, device=hit_vec_lidar_frame.device).unsqueeze(1).expand_as(valid_indices)
-    flat_env_indices = env_indices[valid_indices]
-
-    map_2_5D = torch.full((num_envs, len(x_bins), len(y_bins)), float('inf'), device=hit_vec_lidar_frame.device)
-    linear_indices = flat_env_indices * len(x_bins) * len(y_bins) + x_indices * len(y_bins) + y_indices
-
-    # Subtract the offset and apply dropout
-    # if torch.any(linear_indices < 0) or torch.any(linear_indices >= map_2_5D.view(-1).size(0)):
-    #     print("Index out of bounds")
-    #     print("linear_indices: ", linear_indices)
-    #     print("map_2_5D: ", map_2_5D)
-    #     import pdb; pdb.set_trace()
-    # assert torch.all(linear_indices >= 0) and torch.all(linear_indices < map_2_5D.view(-1).size(0)), "Index out of bounds"
-    map_2_5D = map_2_5D.view(-1).scatter_reduce_(0, linear_indices, z_filtered, reduce="amin") - offset
-    map_2_5D = torch.where(map_2_5D < 0.05, torch.tensor(0.0, device=map_2_5D.device), map_2_5D)
-
-    # # Append the cloned map to the deque
-    # prev_height_maps.append(map_2_5D.clone())
-    # height_maps_hist = list(prev_height_maps)
-
-    # Calculate the maximum value for each pixel across the last ten frames
-    map_2_5D = torch.where(torch.isinf(map_2_5D), torch.tensor(0.0, device=map_2_5D.device), map_2_5D)
-    # Apply maximum pooling with a kernel size of 3
-    # if len(map_2_5D.shape) == 2:
-    #     map_2_5D = map_2_5D.unsqueeze(0)
-    # import pdb; pdb.set_trace()
-    map_2_5D = map_2_5D.view(num_envs, len(x_bins), len(y_bins))
-    max_across_frames = F.max_pool2d(map_2_5D, kernel_size=3, stride=1, padding=1).view(num_envs, -1)
-
-    
-    # # # # import pdb; pdb.set_trace()
-    # # # # # Reshape map_2_5D to 2D image
-    # image = map_2_5D[0].cpu().numpy().reshape(len(x_bins), len(y_bins))
-
-    # # # Visualization (optional)
-    # image = max_across_frames[0].cpu().numpy().reshape(len(x_bins), len(y_bins))
-
-    # # image = (image * 255).astype(int)
-    # image = image.astype('uint8')
-
-    # cv2.imshow("Height Map", image)
-    # cv2.waitKey(1)
-    # cv2.destroyAllWindows()
-
-    # output = (max_across_frames * (torch.rand(map_2_5D.shape, device=map_2_5D.device) > 0.05))
-
-    # print("output: ", output)
-
-    return max_across_frames
-
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/rewards/__init__.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/rewards/__init__.py
deleted file mode 100644
index e110267..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/rewards/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-from .objnav_rewards import *
\ No newline at end of file
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/rewards/objnav_rewards.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/rewards/objnav_rewards.py
deleted file mode 100644
index 89683f5..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/mdp/rewards/objnav_rewards.py
+++ /dev/null
@@ -1,478 +0,0 @@
-# Copyright (c) 2022-2024, The lab Project Developers.
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Common functions that can be used to enable reward functions.
-
-The functions can be passed to the :class:`omni.isaac.lab.managers.RewardTermCfg` object to include
-the reward introduced by the function.
-"""
-
-from __future__ import annotations
-
-import torch
-from typing import TYPE_CHECKING
-
-from omni.isaac.lab.assets import Articulation, RigidObject
-from omni.isaac.lab.managers import SceneEntityCfg
-from omni.isaac.lab.managers.manager_base import ManagerTermBase
-from omni.isaac.lab.managers.manager_term_cfg import RewardTermCfg
-from omni.isaac.lab.sensors import ContactSensor
-import omni.isaac.lab.utils.math as math_utils
-
-if TYPE_CHECKING:
-    from omni.isaac.lab.envs import ManagerBasedRLEnv
-
-"""
-General.
-"""
-
-
-def is_alive(env: ManagerBasedRLEnv) -> torch.Tensor:
-    """Reward for being alive."""
-    return (~env.termination_manager.terminated).float()
-
-
-def is_terminated(env: ManagerBasedRLEnv) -> torch.Tensor:
-    """Penalize terminated episodes that don't correspond to episodic timeouts."""
-    return env.termination_manager.terminated.float()
-
-
-class is_terminated_term(ManagerTermBase):
-    """Penalize termination for specific terms that don't correspond to episodic timeouts.
-
-    The parameters are as follows:
-
-    * attr:`term_keys`: The termination terms to penalize. This can be a string, a list of strings
-      or regular expressions. Default is ".*" which penalizes all terminations.
-
-    The reward is computed as the sum of the termination terms that are not episodic timeouts.
-    This means that the reward is 0 if the episode is terminated due to an episodic timeout. Otherwise,
-    if two termination terms are active, the reward is 2.
-    """
-
-    def __init__(self, cfg: RewardTermCfg, env: ManagerBasedRLEnv):
-        # initialize the base class
-        super().__init__(cfg, env)
-        # find and store the termination terms
-        term_keys = cfg.params.get("term_keys", ".*")
-        self._term_names = env.termination_manager.find_terms(term_keys)
-
-    def __call__(self, env: ManagerBasedRLEnv, term_keys: str | list[str] = ".*") -> torch.Tensor:
-        # Return the unweighted reward for the termination terms
-        reset_buf = torch.zeros(env.num_envs, device=env.device)
-        for term in self._term_names:
-            # Sums over terminations term values to account for multiple terminations in the same step
-            reset_buf += env.termination_manager.get_term(term)
-
-        return (reset_buf * (~env.termination_manager.time_outs)).float()
-
-
-"""
-Root penalties.
-"""
-
-
-def lin_vel_z_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize z-axis base linear velocity using L2-kernel."""
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    return torch.square(asset.data.root_lin_vel_b[:, 2])
-
-
-def ang_vel_xy_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize xy-axis base angular velocity using L2-kernel."""
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    return torch.sum(torch.square(asset.data.root_ang_vel_b[:, :2]), dim=1)
-
-
-def flat_orientation_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize non-flat base orientation using L2-kernel.
-
-    This is computed by penalizing the xy-components of the projected gravity vector.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    return torch.sum(torch.square(asset.data.projected_gravity_b[:, :2]), dim=1)
-
-
-def base_height_l2(
-    env: ManagerBasedRLEnv, target_height: float, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
-) -> torch.Tensor:
-    """Penalize asset height from its target using L2-kernel.
-
-    Note:
-        Currently, it assumes a flat terrain, i.e. the target height is in the world frame.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    # TODO: Fix this for rough-terrain.
-    return torch.square(asset.data.root_pos_w[:, 2] - target_height)
-
-
-def body_lin_acc_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize the linear acceleration of bodies using L2-kernel."""
-    asset: Articulation = env.scene[asset_cfg.name]
-    return torch.sum(torch.norm(asset.data.body_lin_acc_w[:, asset_cfg.body_ids, :], dim=-1), dim=1)
-
-
-"""
-Joint penalties.
-"""
-
-
-def joint_torques_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize joint torques applied on the articulation using L2-kernel.
-
-    NOTE: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their joint torques contribute to the L2 norm.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    return torch.sum(torch.square(asset.data.applied_torque[:, asset_cfg.joint_ids]), dim=1)
-
-
-def joint_vel_l1(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Penalize joint velocities on the articulation using an L1-kernel."""
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    return torch.sum(torch.abs(asset.data.joint_vel[:, asset_cfg.joint_ids]), dim=1)
-
-
-def joint_vel_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize joint velocities on the articulation using L1-kernel.
-
-    NOTE: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their joint velocities contribute to the L1 norm.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    return torch.sum(torch.square(asset.data.joint_vel[:, asset_cfg.joint_ids]), dim=1)
-
-
-def joint_acc_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize joint accelerations on the articulation using L2-kernel.
-
-    NOTE: Only the joints configured in :attr:`asset_cfg.joint_ids` will have their joint accelerations contribute to the L2 norm.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    return torch.sum(torch.square(asset.data.joint_acc[:, asset_cfg.joint_ids]), dim=1)
-
-
-def joint_deviation_l1(env, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize joint positions that deviate from the default one."""
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    # compute out of limits constraints
-    angle = asset.data.joint_pos[:, asset_cfg.joint_ids] - asset.data.default_joint_pos[:, asset_cfg.joint_ids]
-    return torch.sum(torch.abs(angle), dim=1)
-
-
-def joint_pos_limits(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize joint positions if they cross the soft limits.
-
-    This is computed as a sum of the absolute value of the difference between the joint position and the soft limits.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    # compute out of limits constraints
-    out_of_limits = -(
-        asset.data.joint_pos[:, asset_cfg.joint_ids] - asset.data.soft_joint_pos_limits[:, asset_cfg.joint_ids, 0]
-    ).clip(max=0.0)
-    out_of_limits += (
-        asset.data.joint_pos[:, asset_cfg.joint_ids] - asset.data.soft_joint_pos_limits[:, asset_cfg.joint_ids, 1]
-    ).clip(min=0.0)
-    return torch.sum(out_of_limits, dim=1)
-
-
-def joint_vel_limits(
-    env: ManagerBasedRLEnv, soft_ratio: float, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
-) -> torch.Tensor:
-    """Penalize joint velocities if they cross the soft limits.
-
-    This is computed as a sum of the absolute value of the difference between the joint velocity and the soft limits.
-
-    Args:
-        soft_ratio: The ratio of the soft limits to be used.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    # compute out of limits constraints
-    out_of_limits = (
-        torch.abs(asset.data.joint_vel[:, asset_cfg.joint_ids])
-        - asset.data.soft_joint_vel_limits[:, asset_cfg.joint_ids] * soft_ratio
-    )
-    # clip to max error = 1 rad/s per joint to avoid huge penalties
-    out_of_limits = out_of_limits.clip_(min=0.0, max=1.0)
-    return torch.sum(out_of_limits, dim=1)
-
-
-"""
-Action penalties.
-"""
-
-
-def applied_torque_limits(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize applied torques if they cross the limits.
-
-    This is computed as a sum of the absolute value of the difference between the applied torques and the limits.
-
-    .. caution::
-        Currently, this only works for explicit actuators since we manually compute the applied torques.
-        For implicit actuators, we currently cannot retrieve the applied torques from the physics engine.
-    """
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    # compute out of limits constraints
-    # TODO: We need to fix this to support implicit joints.
-    out_of_limits = torch.abs(
-        asset.data.applied_torque[:, asset_cfg.joint_ids] - asset.data.computed_torque[:, asset_cfg.joint_ids]
-    )
-    return torch.sum(out_of_limits, dim=1)
-
-
-def action_rate_l2(env: ManagerBasedRLEnv) -> torch.Tensor:
-    """Penalize the rate of change of the actions using L2-kernel."""
-    # print("action rate: ", torch.sum(torch.square(env.action_manager.action - env.action_manager.prev_action), dim=1))
-    return torch.sum(torch.square(env.action_manager.action - env.action_manager.prev_action), dim=1)
-
-
-def action_l2(env: ManagerBasedRLEnv) -> torch.Tensor:
-    """Penalize the actions using L2-kernel."""
-    return torch.sum(torch.square(env.action_manager.action), dim=1)
-
-
-"""
-Contact sensor.
-"""
-
-
-def undesired_contacts(env: ManagerBasedRLEnv, threshold: float, sensor_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Penalize undesired contacts as the number of violations that are above a threshold."""
-    # extract the used quantities (to enable type-hinting)
-    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
-    # check if contact force is above threshold
-    net_contact_forces = contact_sensor.data.net_forces_w_history
-    # import ipdb; ipdb.set_trace()
-    is_contact = (torch.max(torch.norm(net_contact_forces[:, :, sensor_cfg.body_ids], dim=-1), dim=1)[0] > threshold)
-    # sum over contacts for each environment
-    return torch.sum(is_contact, dim=1)
-
-
-def contact_forces(env: ManagerBasedRLEnv, threshold: float, sensor_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Penalize contact forces as the amount of violations of the net contact force."""
-    # extract the used quantities (to enable type-hinting)
-    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
-    net_contact_forces = contact_sensor.data.net_forces_w_history
-    # compute the violation
-    violation = torch.max(torch.norm(net_contact_forces[:, :, sensor_cfg.body_ids], dim=-1), dim=1)[0] - threshold
-    # compute the penalty
-    return torch.sum(violation.clip(min=0.0), dim=1)
-
-
-"""
-Velocity-tracking rewards.
-"""
-
-
-def track_lin_vel_xy_exp(
-    env: ManagerBasedRLEnv, std: float, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
-) -> torch.Tensor:
-    """Reward tracking of linear velocity commands (xy axes) using exponential kernel."""
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    small_commands = torch.norm(env.command_manager.get_command(command_name)[:, :2], dim=-1) < 0.1
-    track_commands = env.command_manager.get_command(command_name)[:, :2] * (~small_commands)
-    # compute the error
-    lin_vel_error = torch.sum(
-        torch.square(track_commands - asset.data.root_lin_vel_b[:, :2]), 
-        dim=1,
-    )
-    # # compute the error
-    # lin_vel_error = torch.sum(
-    #     torch.square(env.command_manager.get_command(command_name)[:, :2] - asset.data.root_lin_vel_b[:, :2]),
-    #     dim=1,
-    # )
-    return torch.exp(-lin_vel_error / std**2)
-
-
-def track_ang_vel_z_exp(
-    env: ManagerBasedRLEnv, std: float, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
-) -> torch.Tensor:
-    """Reward tracking of angular velocity commands (yaw) using exponential kernel."""
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    # compute the error
-    ang_vel_error = torch.square(env.command_manager.get_command(command_name)[:, 2] - asset.data.root_ang_vel_b[:, 2])
-    return torch.exp(-ang_vel_error / std**2)
-
-def feet_air_time(env: ManagerBasedRLEnv, command_name: str, sensor_cfg: SceneEntityCfg, threshold: float) -> torch.Tensor:
-    """Reward long steps taken by the feet using L2-kernel.
-
-    This function rewards the agent for taking steps that are longer than a threshold. This helps ensure
-    that the robot lifts its feet off the ground and takes steps. The reward is computed as the sum of
-    the time for which the feet are in the air.
-
-    If the commands are small (i.e. the agent is not supposed to take a step), then the reward is zero.
-    """
-    # extract the used quantities (to enable type-hinting)
-    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
-    # compute the reward
-    first_contact = contact_sensor.compute_first_contact(env.step_dt)[:, sensor_cfg.body_ids]
-    last_air_time = contact_sensor.data.last_air_time[:, sensor_cfg.body_ids]
-    reward = torch.sum((last_air_time - threshold) * first_contact, dim=1)
-    # no reward for zero command
-    reward *= torch.norm(env.command_manager.get_command(command_name)[:, :2], dim=1) > 0.1
-    return reward
-
-
-def feet_air_time_positive_biped(env, command_name: str, threshold: float, sensor_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Reward long steps taken by the feet for bipeds.
-
-    This function rewards the agent for taking steps up to a specified threshold and also keep one foot at
-    a time in the air.
-
-    If the commands are small (i.e. the agent is not supposed to take a step), then the reward is zero.
-    """
-    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
-    # compute the reward
-    air_time = contact_sensor.data.current_air_time[:, sensor_cfg.body_ids]
-    contact_time = contact_sensor.data.current_contact_time[:, sensor_cfg.body_ids]
-    in_contact = contact_time > 0.0
-    in_mode_time = torch.where(in_contact, contact_time, air_time)
-    single_stance = torch.sum(in_contact.int(), dim=1) == 1
-    reward = torch.min(torch.where(single_stance.unsqueeze(-1), in_mode_time, 0.0), dim=1)[0]
-    reward = torch.clamp(reward, max=threshold)
-    # no reward for zero command
-    reward *= torch.norm(env.command_manager.get_command(command_name)[:, :2], dim=1) > 0.1
-    # print("contact time: ", contact_time, " reward: ", reward, "command: "  , env.command_manager.get_command(command_name)[:, :2])
-    return reward
-
-def body_lin_acc_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize the linear acceleration of bodies using L2-kernel."""
-    asset: Articulation = env.scene[asset_cfg.name]
-    return torch.sum(torch.norm(asset.data.body_lin_acc_w[:, asset_cfg.body_ids, :], dim=-1), dim=1)
-
-def body_ang_acc_l2(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")) -> torch.Tensor:
-    """Penalize the linear acceleration of bodies using L2-kernel."""
-    asset: Articulation = env.scene[asset_cfg.name]
-    return torch.sum(torch.norm(asset.data.body_ang_acc_w[:, asset_cfg.body_ids, :], dim=-1), dim=1)
-
-def goal_distance(
-    env: ManagerBasedRLEnv, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
-) -> torch.Tensor:
-    """Reward tracking of linear velocity commands (xy axes) using exponential kernel."""
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-    # compute the error
-    goal_track_error_pos = torch.norm(env.command_manager.get_command(command_name)[:,:2], dim=1)
-    reward = torch.where(goal_track_error_pos < 1.0, torch.tensor(1.0, device=env.device), torch.exp(-0.2*goal_track_error_pos))
-    # print("goal distance reward: ", torch.exp(-0.2*goal_track_error_pos), " reach mark: ", goal_track_error_pos < 1.0)
-    return reward
-
-
-def robot_goal_velocity_projection(
-    env: ManagerBasedRLEnv, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
-) -> torch.Tensor:
-    """Reward tracking of linear velocity commands (xy axes) using exponential kernel."""
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-
-    robot_vel_b_normalized = asset.data.root_lin_vel_b[:,:2]/torch.norm(asset.data.root_lin_vel_b[:,:2], dim=-1).unsqueeze(dim=-1)
-    command_b = env.command_manager.get_command(command_name)[:,:2]
-    command_b_normalized=command_b / torch.norm(command_b, dim=-1).unsqueeze(dim=-1)
-    ball_robot_velocity_projection = 1.0 - torch.sum(command_b_normalized * robot_vel_b_normalized[:,0:2], dim=-1)/2.0 # set approaching speed to velocity command
-    rew_dribbling_robot_ball_vel=torch.exp(-1.0* torch.pow(ball_robot_velocity_projection, 2) )
-
-    goal_track_error_pos = torch.norm(env.command_manager.get_command(command_name)[:,:2], dim=1)
-
-    reward = torch.where(goal_track_error_pos < 1.0, torch.tensor(1.0, device=env.device), rew_dribbling_robot_ball_vel)
-    # print("goal velocity projection reward: ", rew_dribbling_robot_ball_vel, " reach mark: ", goal_track_error_pos < 1.0)
-    return reward
-
-def stand_still_velocity_penalty(
-    env: ManagerBasedRLEnv, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
-) -> torch.Tensor:
-    """Penalize the robot for not standing still after reaching target."""
-    asset: RigidObject = env.scene[asset_cfg.name]
-    goal_track_error_pos = torch.norm(env.command_manager.get_command(command_name)[:,:2], dim=1)
-    reached_mark = goal_track_error_pos < 1.0
-
-    zero_penalty = torch.zeros(env.num_envs, device=env.device)
-    zero_penalty[reached_mark] = torch.norm(asset.data.root_lin_vel_b[reached_mark,:3], dim=-1)
-    zero_penalty[reached_mark] += torch.norm(asset.data.root_ang_vel_b[reached_mark,:3], dim=-1)
-
-    # print("stand still velocity penalty: ", zero_penalty, " reach mark: ", reached_mark)
-
-    return zero_penalty
-
-def goal_direction(
-    env: ManagerBasedRLEnv, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
-) -> torch.Tensor:
-    """Reward tracking of linear velocity commands (xy axes) using exponential kernel."""
-    # extract the used quantities (to enable type-hinting)
-    asset: RigidObject = env.scene[asset_cfg.name]
-
-    pos_command_w = env.command_manager._terms['goal_command'].pos_command_w.clone()[:,:2] - asset.data.root_pos_w[:,:2]
-    pos_command_w_normalized = pos_command_w / torch.norm(pos_command_w, dim=-1).unsqueeze(dim=-1)
-    robot_quat_w = asset.data.root_quat_w
-    robot_ori_vec2d = math_utils.quat_rotate(robot_quat_w, torch.tensor([1.0,0.0,0.0], device=env.device).repeat(env.num_envs, 1))
-    robot_ori_vec2d = robot_ori_vec2d[:,:2]
-    robot_ori_vec2d = robot_ori_vec2d / torch.norm(robot_ori_vec2d, dim=-1).unsqueeze(dim=-1)
-    robot_goal_direction_error = 1-(torch.sum(pos_command_w_normalized * robot_ori_vec2d, dim=-1)+1.0)/2.0
-    reward = torch.exp(-8.0*robot_goal_direction_error)
-
-    # goal_track_error_pos = torch.norm(env.command_manager.get_command(command_name)[:,:2], dim=1)
-
-    # # # compute the error
-    # reward = torch.where(goal_track_error_pos < 1.5, torch.tensor(1.0, device=env.device), reward)
-    # print("goal direction reward: ", reward)
-    return reward
-
-def collision_penalty(env: ManagerBasedRLEnv, threshold: float, sensor_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Penalize undesired contacts as the number of violations that are above a threshold."""
-    # extract the used quantities (to enable type-hinting)
-    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
-    # check if contact force is above threshold
-    net_contact_forces = contact_sensor.data.net_forces_w_history
-    is_contact = torch.max(torch.norm(net_contact_forces[:, :, sensor_cfg.body_ids], dim=-1), dim=1)[0] > threshold
-    # print("contact force: ", net_contact_forces[:, :, sensor_cfg.body_ids])
-
-    # sum over contacts for each environment
-    return torch.sum(is_contact, dim=1)
-
-
-def feet_stumble(env, sensor_cfg: SceneEntityCfg) -> torch.Tensor:
-    # Penalize feet stumbling
-    contact_sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
-    contacts_norm = contact_sensor.data.net_forces_w[:, sensor_cfg.body_ids, :2].norm(dim=-1)
-    vertical_contacts = torch.abs(contact_sensor.data.net_forces_w[:, sensor_cfg.body_ids, 2])
-    reward = torch.any(contacts_norm > 5 * vertical_contacts, dim=1)
-    return reward
-
-
-def action_smoothness_penalty(env: ManagerBasedRLEnv) -> torch.Tensor:
-    """Penalize large instantaneous changes in the network action output"""
-    return torch.linalg.norm((env.action_manager.action - env.action_manager.prev_action), dim=1)
-
-
-def power_penalty(env: ManagerBasedRLEnv, asset_cfg: SceneEntityCfg) -> torch.Tensor:
-    """Penalize power consumption of the robot."""
-    # extract the used quantities (to enable type-hinting)
-    asset: Articulation = env.scene[asset_cfg.name]
-    return torch.sum(torch.abs(asset.data.applied_torque) * torch.abs(asset.data.joint_vel), dim=1)
-
-def stand_still_penalty(
-    env: ManagerBasedRLEnv, command_name: str, asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
-) -> torch.Tensor:
-    """Penalize the robot for not standing still when velocity commands are small."""
-    asset: RigidObject = env.scene[asset_cfg.name]
-
-    # compute out of limits constraints
-    angle = asset.data.joint_pos[:, asset_cfg.joint_ids] - asset.data.default_joint_pos[:, asset_cfg.joint_ids]
-    # print("stand still velocity penalty: ", zero_penalty, " reach mark: ", reached_mark)
-    rew = torch.sum(torch.abs(angle), dim=1)
-
-    small_commands = torch.norm(env.command_manager.get_command(command_name)[:, :2], dim=-1) < 0.1
-    return rew * small_commands
diff --git a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/viplanner_algo.py b/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/viplanner_algo.py
deleted file mode 100644
index 82b4bc5..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/omni/isaac/viplanner/viplanner/viplanner_algo.py
+++ /dev/null
@@ -1,171 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES, ETH Zurich, and University of Toronto
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""
-This script demonstrates how to use the rigid objects class.
-"""
-
-"""Rest everything follows."""
-
-import os
-
-import carb
-import omni.isaac.lab.utils.math as math_utils
-import torch
-import torchvision.transforms as transforms
-from omni.isaac.debug_draw import _debug_draw
-
-from viplanner.config import TrainCfg
-
-# viplanner
-from viplanner.plannernet import AutoEncoder, DualAutoEncoder
-from viplanner.traj_cost_opt.traj_opt import TrajOpt
-
-"""
-VIPlanner Helpers
-"""
-
-
-class VIPlannerAlgo:
-    def __init__(self, model_dir: str, fear_threshold: float = 0.5):
-        """Apply VIPlanner Algorithm
-
-        Args:
-            model_dir (str): Directory that include model.pt and model.yaml
-        """
-        super().__init__()
-
-        assert os.path.exists(model_dir), "Model directory does not exist"
-        assert os.path.isfile(os.path.join(model_dir, "model.pt")), "Model file does not exist"
-        assert os.path.isfile(os.path.join(model_dir, "model.yaml")), "Model config file does not exist"
-
-        # params
-        self.fear_threshold = fear_threshold
-
-        # load model
-        self.train_config: TrainCfg = None
-        self.load_model(model_dir)
-
-        # get transforms for images
-        self.transform = transforms.Resize(self.train_config.img_input_size, antialias=None)
-
-        # init trajectory optimizer
-        self.traj_generate = TrajOpt()
-
-        # setup waypoint display in Isaac
-        self.draw = _debug_draw.acquire_debug_draw_interface()
-        self.color_fear = [(1.0, 0.4, 0.1, 1.0)]  # red
-        self.color_path = [(0.4, 1.0, 0.1, 1.0)]  # green
-        self.size = [5.0]
-
-    def load_model(self, model_dir: str):
-        # load train config
-        self.train_config: TrainCfg = TrainCfg.from_yaml(os.path.join(model_dir, "model.yaml"))
-        carb.log_info(
-            f"Model loaded using sem: {self.train_config.sem}, rgb: {self.train_config.rgb}, knodes: {self.train_config.knodes}, in_channel: {self.train_config.in_channel}"
-        )
-
-        if isinstance(self.train_config.data_cfg, list):
-            self.max_goal_distance = self.train_config.data_cfg[0].max_goal_distance
-            self.max_depth = self.train_config.data_cfg[0].max_depth
-        else:
-            self.max_goal_distance = self.train_config.data_cfg.max_goal_distance
-            self.max_depth = self.train_config.data_cfg.max_depth
-
-        if self.train_config.sem:
-            self.net = DualAutoEncoder(self.train_config)
-        else:
-            self.net = AutoEncoder(self.train_config.in_channel, self.train_config.knodes)
-
-        # get model and load weights
-        try:
-            model_state_dict, _ = torch.load(os.path.join(model_dir, "model.pt"))
-        except ValueError:
-            model_state_dict = torch.load(os.path.join(model_dir, "model.pt"))
-        self.net.load_state_dict(model_state_dict)
-
-        # inference script = no grad for model
-        self.net.eval()
-
-        # move to GPU if available
-        if torch.cuda.is_available():
-            self.net = self.net.cuda()
-            self.cuda_avail = True
-        else:
-            carb.log_warn("CUDA not available, VIPlanner will run on CPU")
-            self.cuda_avail = False
-        return
-
-    ###
-    # Transformations
-    ###
-
-    def goal_transformer(self, goal: torch.Tensor, cam_pos: torch.Tensor, cam_quat: torch.Tensor) -> torch.Tensor:
-        """transform goal into camera frame"""
-        goal_cam_frame = goal - cam_pos
-        goal_cam_frame[:, 2] = 0  # trained with z difference of 0
-        goal_cam_frame = math_utils.quat_apply(math_utils.quat_inv(cam_quat), goal_cam_frame)
-        return goal_cam_frame
-
-    def path_transformer(
-        self, path_cam_frame: torch.Tensor, cam_pos: torch.Tensor, cam_quat: torch.Tensor
-    ) -> torch.Tensor:
-        """transform path from camera frame to world frame"""
-        return math_utils.quat_apply(
-            cam_quat.unsqueeze(1).repeat(1, path_cam_frame.shape[1], 1), path_cam_frame
-        ) + cam_pos.unsqueeze(1)
-
-    def input_transformer(self, image: torch.Tensor) -> torch.Tensor:
-        # transform images
-        image = self.transform(image)
-        image[image > self.max_depth] = 0.0
-        image[~torch.isfinite(image)] = 0  # set all inf or nan values to 0
-        return image
-
-    ###
-    # Planning
-    ###
-
-    def plan(self, image: torch.Tensor, goal_robot_frame: torch.Tensor) -> tuple:
-        with torch.no_grad():
-            keypoints, fear = self.net(self.input_transformer(image), goal_robot_frame)
-        traj = self.traj_generate.TrajGeneratorFromPFreeRot(keypoints, step=0.1)
-
-        return keypoints, traj, fear
-
-    def plan_dual(self, dep_image: torch.Tensor, sem_image: torch.Tensor, goal_robot_frame: torch.Tensor) -> tuple:
-        # transform input
-        sem_image = self.transform(sem_image) / 255
-        with torch.no_grad():
-            keypoints, fear = self.net(self.input_transformer(dep_image), sem_image, goal_robot_frame)
-        traj = self.traj_generate.TrajGeneratorFromPFreeRot(keypoints, step=0.1)
-
-        return keypoints, traj, fear
-
-    ###
-    # Debug Draw
-    ###
-
-    def debug_draw(self, paths: torch.Tensor, fear: torch.Tensor, goal: torch.Tensor):
-        self.draw.clear_lines()
-        self.draw.clear_points()
-
-        def draw_single_traj(traj, color, size):
-            traj[:, 2] = torch.mean(traj[:, 2])
-            self.draw.draw_lines(traj[:-1].tolist(), traj[1:].tolist(), color * len(traj[1:]), size * len(traj[1:]))
-
-        for idx, curr_path in enumerate(paths):
-            if fear[idx] > self.fear_threshold:
-                draw_single_traj(curr_path, self.color_fear, self.size)
-                self.draw.draw_points(goal.tolist(), self.color_fear * len(goal), self.size * len(goal))
-            else:
-                draw_single_traj(curr_path, self.color_path, self.size)
-                self.draw.draw_points(goal.tolist(), self.color_path * len(goal), self.size * len(goal))
diff --git a/isaaclab/extension/omni.isaac.viplanner/setup.py b/isaaclab/extension/omni.isaac.viplanner/setup.py
deleted file mode 100644
index f12b19b..0000000
--- a/isaaclab/extension/omni.isaac.viplanner/setup.py
+++ /dev/null
@@ -1,37 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Installation script for the 'omni.isaac.viplanner' python package."""
-
-
-from setuptools import setup
-
-# Minimum dependencies required prior to installation
-INSTALL_REQUIRES = [
-    # generic
-    "numpy",
-    "scipy>=1.7.1",
-    # RL
-    "torch>=1.9.0",
-]
-
-# Installation operation
-setup(
-    name="omni-isaac-viplanner",
-    author="Pascal Roth",
-    author_email="rothpa@ethz.ch",
-    version="0.0.1",
-    description="Extension to include ViPlanner: Visual Semantic Imperative Learning for Local Navigation",
-    keywords=["robotics", "rl"],
-    include_package_data=True,
-    python_requires=">=3.7",
-    install_requires=INSTALL_REQUIRES,
-    packages=["omni.isaac.viplanner"],
-    classifiers=["Natural Language :: English", "Programming Language :: Python :: 3.7"],
-    zip_safe=False,
-)
-
-# EOF
diff --git a/isaaclab/extension/omni.waypoints/config/extension.toml b/isaaclab/extension/omni.waypoints/config/extension.toml
deleted file mode 100644
index 0b07282..0000000
--- a/isaaclab/extension/omni.waypoints/config/extension.toml
+++ /dev/null
@@ -1,21 +0,0 @@
-[package]
-version = "0.0.1"
-title = "Waypoint extension"
-description="Extension to extract waypoints in 3D environments."
-authors =["Pascal Roth"]
-repository = "https://gitlab-master.nvidia.com/mmittal/omni_isaac_lab"
-category = "robotics"
-keywords = ["kit", "robotics"]
-readme  = "docs/README.md"
-
-[dependencies]
-"omni.kit.uiapp" = {}
-"omni.isaac.ui" = {}
-"omni.isaac.core" = {}
-
-# Main python module this extension provides.
-[[python.module]]
-name = "omni.isaac.waypoints"
-
-[[python.module]]
-name = "omni.isaac.waypoints.scripts"
diff --git a/isaaclab/extension/omni.waypoints/omni/isaac/waypoints/recorder/__init__.py b/isaaclab/extension/omni.waypoints/omni/isaac/waypoints/recorder/__init__.py
deleted file mode 100644
index ce2f2e5..0000000
--- a/isaaclab/extension/omni.waypoints/omni/isaac/waypoints/recorder/__init__.py
+++ /dev/null
@@ -1,11 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from .recorder import Recorder
-
-__all__ = ["Recorder"]
-
-# EoF
diff --git a/isaaclab/extension/omni.waypoints/omni/isaac/waypoints/recorder/recorder.py b/isaaclab/extension/omni.waypoints/omni/isaac/waypoints/recorder/recorder.py
deleted file mode 100644
index 6893269..0000000
--- a/isaaclab/extension/omni.waypoints/omni/isaac/waypoints/recorder/recorder.py
+++ /dev/null
@@ -1,136 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import json
-
-# python
-import os
-from typing import List
-
-import numpy as np
-
-# omni
-import omni
-
-# isaac-debug
-import omni.isaac.debug_draw._debug_draw as omni_debug_draw
-import scipy.spatial.transform as tf
-
-# isaac-core
-from omni.isaac.core.objects import VisualCuboid
-from pxr import UsdGeom
-
-
-class Recorder:
-    """
-    Record arbitrary number of waypoints and save them as .json file
-    """
-
-    cube_scale = 100  # convert from meters to cm
-
-    def __init__(self) -> None:
-        # init buffers
-        self.start_point: List[float] = [0.0] * 3
-        self.end_point: List[float] = [0.0] * 3
-        self.way_points: List[List[float]] = []
-
-        # init params
-        self.save_path: str = None
-        self.file_name: str = "waypoints"
-
-        # Acquire draw interface
-        self.draw_interface = omni_debug_draw.acquire_debug_draw_interface()
-
-        # cube
-        self.cube = VisualCuboid(
-            prim_path="/Waypoint",  # The prim path of the cube in the USD stage
-            name="waypoint",  # The unique name used to retrieve the object from the scene later on
-            position=np.array([0, 0, 1.0]),  # Using the current stage units which is in meters by default.
-            scale=np.array([0.25, 0.25, 0.25]) * self.cube_scale,  # most arguments accept mainly numpy arrays.
-            size=1.0,
-            color=np.array([1, 0.4, 0]),  # RGB channels, going from 0-1
-        )
-
-        # identfy up axis of the stage (y or z)
-        stage = omni.usd.get_context().get_stage()
-        if UsdGeom.GetStageUpAxis(stage) == UsdGeom.Tokens.y:
-            self.rot_mat = tf.Rotation.from_euler("XYZ", [90, 90, 0], degrees=True).as_matrix()
-        elif UsdGeom.GetStageUpAxis(stage) == UsdGeom.Tokens.z:
-            self.rot_mat = np.eye(3)
-        else:
-            raise ValueError("Stage Up Axis not supported")
-        return
-
-    def set_save_path(self, path: str) -> None:
-        self.save_path = path
-        return
-
-    def set_filename(self, name) -> None:
-        self.file_name = name.get_value_as_string()
-        return
-
-    def set_start_point(self) -> None:
-        # get coordinates of the start
-        start_point = self._get_cube_coords()
-        # save start point with z-up axis
-        self.start_point = np.matmul(self.rot_mat, start_point).tolist()
-        # draw start point
-        self.draw_interface.draw_points([start_point], [(0, 1, 0, 1)], [10])  # green
-        return
-
-    def add_way_point(self) -> None:
-        # get coordinates of the cube
-        way_point = self._get_cube_coords()
-        # save way point with z-up axis
-        self.way_points.append(np.matmul(self.rot_mat, way_point).tolist())
-        # draw start point
-        self.draw_interface.draw_points([way_point], [(0, 0, 1, 1)], [10])  # blue
-        return
-
-    def set_end_point(self) -> None:
-        """
-        Set the end point of the path and save all waypoints as .json file with the following structure:
-        {
-            start: [x, y, z],
-            end: [x, y, z],
-            waypoints: [[x, y, z], [x, y, z], ...]
-        }
-        All points are saved in the z-up axis convention.
-        """
-
-        # get coordinates of the end
-        end_point = self._get_cube_coords()
-        # save end point with z-up axis
-        self.end_point = np.matmul(self.rot_mat, end_point).tolist()
-        # draw start point
-        self.draw_interface.draw_points([end_point], [(1, 0, 0, 1)], [10])  # red
-        # save points
-        if self.file_name.endswith(".json"):
-            file_path = os.path.join(self.save_path, self.file_name)
-        else:
-            file_path = os.path.join(self.save_path, self.file_name + ".json")
-
-        data = {"start": self.start_point, "end": self.end_point, "waypoints": self.way_points}
-        with open(file_path, "w") as file:
-            json.dump(data, file)
-        return
-
-    def reset(self) -> None:
-        self.start_point = [0.0] * 3
-        self.end_point = [0.0] * 3
-        self.way_points = []
-        self.draw_interface.clear_points()
-        return
-
-    """ Helper functions """
-
-    def _get_cube_coords(self) -> np.ndarray:
-        pose = omni.usd.utils.get_world_transform_matrix(self.cube.prim)
-        pose = np.array(pose).T
-        return pose[:3, 3]
-
-
-# EoF
diff --git a/isaaclab/extension/omni.waypoints/omni/isaac/waypoints/scripts/__init__.py b/isaaclab/extension/omni.waypoints/omni/isaac/waypoints/scripts/__init__.py
deleted file mode 100644
index cce8c3d..0000000
--- a/isaaclab/extension/omni.waypoints/omni/isaac/waypoints/scripts/__init__.py
+++ /dev/null
@@ -1,11 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from .recorder_ui import WaypointExtension
-
-__all__ = ["WaypointExtension"]
-
-# EoF
diff --git a/isaaclab/extension/omni.waypoints/omni/isaac/waypoints/scripts/recorder_ui.py b/isaaclab/extension/omni.waypoints/omni/isaac/waypoints/scripts/recorder_ui.py
deleted file mode 100644
index 1ab8bd7..0000000
--- a/isaaclab/extension/omni.waypoints/omni/isaac/waypoints/scripts/recorder_ui.py
+++ /dev/null
@@ -1,209 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import asyncio
-import gc
-
-# python
-import os
-
-import carb
-
-# omni
-import omni
-import omni.client
-import omni.ext
-
-# isaac-core
-import omni.ui as ui
-
-# omni-isaac-ui
-from omni.isaac.ui.ui_utils import btn_builder, get_style, setup_ui_headers, str_builder
-
-# isaac-waypoints
-from omni.isaac.waypoints.recorder import Recorder
-
-EXTENSION_NAME = "Waypoint Recorder"
-
-
-class WaypointExtension(omni.ext.IExt):
-    """Extension to record Waypoints in Isaac Sim"""
-
-    def on_startup(self, ext_id):
-        self._ext_id = ext_id
-        self._usd_context = omni.usd.get_context()
-        self._window = omni.ui.Window(
-            EXTENSION_NAME, width=400, height=500, visible=True, dockPreference=ui.DockPreference.LEFT_BOTTOM
-        )
-
-        # init recorder class and get path to extension
-        self._extension_path = omni.kit.app.get_app().get_extension_manager().get_extension_path(ext_id)
-        self.recorder = Recorder()
-
-        # set additional parameters
-        self._input_fields: dict = {}  # dictionary to store values of buttion, float fields, etc.
-
-        # build ui
-        self.build_ui()
-        return
-
-    ##
-    # UI Build functions
-    ##
-
-    def build_ui(self):
-        with self._window.frame:
-            with ui.VStack(spacing=5, height=0):
-                self._build_info_ui()
-
-                self._build_recorder_ui()
-
-                self._build_display_ui()
-
-        async def dock_window():
-            await omni.kit.app.get_app().next_update_async()
-
-            def dock(space, name, location, pos=0.5):
-                window = omni.ui.Workspace.get_window(name)
-                if window and space:
-                    window.dock_in(space, location, pos)
-                return window
-
-            tgt = ui.Workspace.get_window("Viewport")
-            dock(tgt, EXTENSION_NAME, omni.ui.DockPosition.LEFT, 0.33)
-            await omni.kit.app.get_app().next_update_async()
-
-        self._task = asyncio.ensure_future(dock_window())
-
-    def _build_info_ui(self):
-        title = EXTENSION_NAME
-        doc_link = "https://github.com/leggedrobotics/omni_isaac_lab"
-
-        overview = "Extension to record waypoints in any Environment and export them to a .json file."
-
-        setup_ui_headers(self._ext_id, __file__, title, doc_link, overview)
-        return
-
-    def _build_recorder_ui(self):
-        frame = ui.CollapsableFrame(
-            title="Record Waypoints",
-            height=0,
-            collapsed=False,
-            style=get_style(),
-            style_type_name_override="CollapsableFrame",
-            horizontal_scrollbar_policy=ui.ScrollBarPolicy.SCROLLBAR_AS_NEEDED,
-            vertical_scrollbar_policy=ui.ScrollBarPolicy.SCROLLBAR_ALWAYS_ON,
-        )
-        with frame:
-            with ui.VStack(style=get_style(), spacing=5, height=0):
-                # get save directory
-                kwargs = {
-                    "label": "Save Directory",
-                    "type": "stringfield",
-                    "default_val": "",
-                    "tooltip": "Click the Folder Icon to Set Filepath",
-                    "use_folder_picker": True,
-                }
-                self._input_fields["save_path"] = str_builder(**kwargs)
-                self._input_fields["save_path"].add_value_changed_fn(self._check_save_path)
-
-                kwargs = {
-                    "label": "Save Filename",
-                    "type": "stringfield",
-                    "default_val": "waypoints",
-                }
-                self._input_fields["file_name"] = str_builder(**kwargs)
-                self._input_fields["file_name"].add_value_changed_fn(self.recorder.set_filename)
-
-                self._input_fields["start_point"] = btn_builder(
-                    "Start-Point", text="Record", on_clicked_fn=self._set_start_point
-                )
-                self._input_fields["start_point"].enabled = False
-
-                self._input_fields["way_point"] = btn_builder(
-                    "Intermediate-Point", text="Record", on_clicked_fn=self._set_way_point
-                )
-                self._input_fields["way_point"].enabled = False
-
-                self._input_fields["end_point"] = btn_builder(
-                    "End-Point", text="Record", on_clicked_fn=self._set_end_point
-                )
-                self._input_fields["end_point"].enabled = False
-
-                self._input_fields["reset"] = btn_builder("Reset", text="Reset", on_clicked_fn=self.recorder.reset)
-                self._input_fields["reset"].enabled = True
-        return
-
-    def _build_display_ui(self):
-        frame = ui.CollapsableFrame(
-            title="Waypoint Information",
-            height=0,
-            collapsed=False,
-            style=get_style(),
-            style_type_name_override="CollapsableFrame",
-            horizontal_scrollbar_policy=ui.ScrollBarPolicy.SCROLLBAR_AS_NEEDED,
-            vertical_scrollbar_policy=ui.ScrollBarPolicy.SCROLLBAR_ALWAYS_ON,
-        )
-        with frame:
-            with ui.VStack(style=get_style(), spacing=5, height=0):
-                # control parameters
-                pass
-        return
-
-    ##
-    # Shutdown Helpers
-    ##
-
-    def on_shutdown(self):
-        if self._window:
-            self._window = None
-        gc.collect()
-
-    ##
-    # Recorder Helper
-    ##
-
-    def _check_save_path(self, path):
-        path = path.get_value_as_string()
-
-        if not os.path.isfile(path):
-            self._input_fields["start_point"].enabled = True
-            self.recorder.set_save_path(path=path)
-        else:
-            self._input_fields["start_point"].enabled = False
-            carb.log_warn(f"Directory at save path {path} does not exist!")
-
-        return
-
-    def _set_start_point(self) -> None:
-        # set start point
-        self.recorder.set_start_point()
-
-        # enable intermediate waypoints
-        self._input_fields["start_point"].enabled = False
-        self._input_fields["way_point"].enabled = True
-        return
-
-    def _set_way_point(self) -> None:
-        # add intermediate waypoint to list
-        self.recorder.add_way_point()
-
-        # enable end point
-        self._input_fields["end_point"].enabled = True
-        return
-
-    def _set_end_point(self) -> None:
-        # set end point
-        self.recorder.set_end_point()
-
-        # enable / disable buttons
-        self._input_fields["way_point"].enabled = False
-        self._input_fields["end_point"].enabled = False
-        self._input_fields["start_point"].enabled = True
-        return
-
-
-# EoF
diff --git a/isaaclab/extension/omni.waypoints/setup.py b/isaaclab/extension/omni.waypoints/setup.py
deleted file mode 100644
index 6ba5913..0000000
--- a/isaaclab/extension/omni.waypoints/setup.py
+++ /dev/null
@@ -1,27 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Installation script for the 'omni.isaac.waypoints' python package."""
-
-
-from setuptools import setup
-
-# Installation operation
-setup(
-    name="omni-isaac-waypoints",
-    author="Pascal Roth",
-    author_email="rothpa@ethz.ch",
-    version="0.0.1",
-    description="Extension to extract waypoints in 3D environments.",
-    keywords=["robotics"],
-    include_package_data=True,
-    python_requires="==3.7.*",
-    packages=["omni.isaac.waypoints"],
-    classifiers=["Natural Language :: English", "Programming Language :: Python :: 3.7"],
-    zip_safe=False,
-)
-
-# EOF
diff --git a/isaaclab/standalone/play.py b/isaaclab/standalone/play.py
index 7621d0a..92fb5f4 100644
--- a/isaaclab/standalone/play.py
+++ b/isaaclab/standalone/play.py
@@ -46,8 +46,8 @@ simulation_app = app_launcher.app
 
 import gymnasium as gym
 import os
-import math
 import torch
+import imageio
 
 from rsl_rl.runners import OnPolicyRunner
 
@@ -64,44 +64,8 @@ from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
 )
 
 
-from omni.isaac.viplanner.config import *
-from omni.isaac.viplanner.utils import RslRlVecEnvHistoryWrapper
-
-
-def create_video_with_ffmpeg(frames, output_filename='output.mp4', fps=30):
-    height, width, _ = frames[0].shape
-    
-    # Prepare ffmpeg command
-    ffmpeg_command = [
-        'ffmpeg', 
-        '-y',  # Overwrite output file if it exists
-        '-f', 'rawvideo',
-        '-vcodec', 'rawvideo',
-        '-s', f'{width}x{height}',  # Input size
-        '-pix_fmt', 'rgb24',  # Pixel format
-        '-r', str(fps),  # Frame rate
-        '-i', '-',  # Input from stdin
-        '-c:v', 'libx264',  # H.264 codec
-        '-pix_fmt', 'yuv420p',  # Output pixel format
-        '-preset', 'ultrafast',  # Preset for fast encoding
-        output_filename
-    ]
-    
-    # Start ffmpeg process
-    process = subprocess.Popen(ffmpeg_command, stdin=subprocess.PIPE)
-    
-    try:
-        # Write frames to ffmpeg's stdin
-        for frame in frames:
-            process.stdin.write(frame.tobytes())
-    except Exception as e:
-        print(f"Error while writing frames to ffmpeg: {e}")
-    finally:
-        # Close ffmpeg process
-        process.stdin.close()
-        process.wait()
-
-    print(f'Video saved as {output_filename}')
+from omni.isaac.leggednav.config import *
+from omni.isaac.leggednav.utils import RslRlVecEnvHistoryWrapper
 
 
 def main():
@@ -141,7 +105,6 @@ def main():
     log_root_path = os.path.abspath(log_root_path)
     print(f"[INFO] Loading experiment from directory: {log_root_path}")
     resume_path = get_checkpoint_path(log_root_path, args_cli.load_run, agent_cfg.load_checkpoint)
-    print(f"[INFO]: Loading model checkpoint from: {resume_path}")
 
     # load previously trained model
     ppo_runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=None, device=agent_cfg.device)
@@ -180,7 +143,8 @@ def main():
             # agent stepping
             actions = policy(obs)
             # env stepping
-            obs, _, _, _ = env.step(actions)
+            obs, _, _, infos = env.step(actions)
+            # import pdb; pdb.set_trace()
 
             if args_cli.video and len(frames) < args_cli.video_length:
                 base_env = env.unwrapped
@@ -196,8 +160,10 @@ def main():
             if args_cli.video and len(frames) == args_cli.video_length:
                 break
     
-    if args_cli.video:
-        create_video_with_ffmpeg(frames, output_filename=os.path.join(log_dir, f"{args_cli.load_run}.mp4"), fps=50)
+    writer = imageio.get_writer(os.path.join(log_dir, f"{args_cli.load_run}.mp4"), fps=50)
+    for frame in frames:
+        writer.append_data(frame)
+    writer.close()
 
     # close the simulator
     env.close()
diff --git a/isaaclab/standalone/play_low_matterport_keyboard.py b/isaaclab/standalone/play_low_matterport_keyboard.py
index 1c65d53..7b205a0 100644
--- a/isaaclab/standalone/play_low_matterport_keyboard.py
+++ b/isaaclab/standalone/play_low_matterport_keyboard.py
@@ -55,9 +55,12 @@ from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
     RslRlVecEnvWrapper,
 )
 
-from omni.viplanner.config import *
+from omni.isaac.viplanner.config import *
 from omni.isaac.lab.devices.keyboard import Se2Keyboard
-from omni.viplanner.utils import RslRlVecEnvHistoryWrapper
+from omni.isaac.viplanner.utils import RslRlVecEnvHistoryWrapper
+
+import omni.isaac.lab.sim as sim_utils
+from omni.isaac.lab.markers import VisualizationMarkers, VisualizationMarkersCfg
 
 
 def quat2eulers(q0, q1, q2, q3):
@@ -81,6 +84,19 @@ def quat2eulers(q0, q1, q2, q3):
     return roll, pitch, yaw
 
 
+def define_markers() -> VisualizationMarkers:
+    """Define path markers with various different shapes."""
+    marker_cfg = VisualizationMarkersCfg(
+        prim_path="/Visuals/pathMarkers",
+        markers={
+            "waypoint": sim_utils.SphereCfg(
+                radius=0.1,
+                visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 1.0, 0.0)),
+            ),
+        },
+    )
+    return VisualizationMarkers(marker_cfg)
+
 
 def main():
     """Play with RSL-RL agent."""
@@ -125,15 +141,16 @@ def main():
     # cam_target = (robot_pos_w[0], robot_pos_w[1], 1.0)
     # # set the camera view
     # env.unwrapped.sim.set_camera_view(eye=cam_eye, target=cam_target)
-    robot_pos_w = env.unwrapped.scene["robot"].data.root_pos_w[0].detach().cpu().numpy()
+    # robot_pos_w = env.unwrapped.scene["robot"].data.root_pos_w[0].detach().cpu().numpy()
     # cam_eye = (robot_pos_w[0]-0.0, robot_pos_w[1]-1.5, 1.2)
-    robot_quat_w = env.unwrapped.scene["robot"].data.root_quat_w[0].detach().cpu().numpy()
-    roll, pitch, yaw = quat2eulers(robot_quat_w[0], robot_quat_w[1], robot_quat_w[2], robot_quat_w[3])
-    cam_eye = (robot_pos_w[0] - 0.8 * math.sin(-yaw), robot_pos_w[1] - 0.8 * math.cos(-yaw), robot_pos_w[2] + 0.8)
-    cam_target = (robot_pos_w[0], robot_pos_w[1], robot_pos_w[2])
+    # robot_quat_w = env.unwrapped.scene["robot"].data.root_quat_w[0].detach().cpu().numpy()
+    # roll, pitch, yaw = quat2eulers(robot_quat_w[0], robot_quat_w[1], robot_quat_w[2], robot_quat_w[3])
+    # cam_eye = (robot_pos_w[0] - 0.8 * math.sin(-yaw), robot_pos_w[1] - 0.8 * math.cos(-yaw), robot_pos_w[2] + 0.8)
+    # cam_target = (robot_pos_w[0], robot_pos_w[1], robot_pos_w[2])
 
     # set the camera view
-    env.unwrapped.sim.set_camera_view(eye=cam_eye, target=cam_target)
+    # env.unwrapped.sim.set_camera_view(eye=cam_eye, target=cam_target)
+
     # simulate environment
     while simulation_app.is_running():
         # run everything in inference mode
@@ -143,28 +160,33 @@ def main():
             if num_count % 10 == 0:
                 print("vel_command_keyboard: ", vel_command_keyboard)
             # import pdb; pdb.set_trace()
-            assert torch.allclose(env.unwrapped.command_manager._terms['base_velocity'].vel_command_b, torch.tensor([0., 0., 0.], device = obs.device))
+            # assert torch.allclose(env.unwrapped.command_manager._terms['base_velocity'].vel_command_b, torch.tensor([0., 0., 0.], device = obs.device))
             # env.command_manager._terms['base_velocity'].vel_command_b[0,:] = commands
             # obs[:,9:12] = commands_key
-            # obs[:,9:12] = torch.tensor([vel_command_keyboard[0], vel_command_keyboard[1], vel_command_keyboard[2]], device = obs.device)
+            obs[:,9:12] = torch.tensor([vel_command_keyboard[0], vel_command_keyboard[1], vel_command_keyboard[2]], device = obs.device)
             # agent stepping
             actions = policy(obs)
 
-            # robot_pos_w = env.unwrapped.scene["robot"].data.root_pos_w[0].detach().cpu().numpy()
-            # robot_quat_w = env.unwrapped.scene["robot"].data.root_quat_w[0].detach().cpu().numpy()
+            robot_pos_w = env.unwrapped.scene["robot"].data.root_pos_w[0].detach().cpu().numpy()
+            robot_quat_w = env.unwrapped.scene["robot"].data.root_quat_w[0].detach().cpu().numpy()
+            print("robot quat: ", robot_quat_w)
+            print("robot pos: ", robot_pos_w)
+
             # roll, pitch, yaw = quat2eulers(robot_quat_w[0], robot_quat_w[1], robot_quat_w[2], robot_quat_w[3])
-            # cam_eye = (robot_pos_w[0] - 0.8 * math.sin(-yaw), robot_pos_w[1] - 0.8 * math.cos(-yaw), robot_pos_w[2] + 0.8)
+            # cam_eye = (robot_pos_w[0] + 5.0 * math.cos(-yaw), robot_pos_w[1] + 5.0 * math.sin(yaw), robot_pos_w[2] + 1.0)
             # cam_target = (robot_pos_w[0], robot_pos_w[1], robot_pos_w[2])
             # # set the camera view
             # env.unwrapped.sim.set_camera_view(eye=cam_eye, target=cam_target)
             # env stepping
-            obs, _, _, _ = env.step(actions)
+            obs, _, _, infos = env.step(actions)
+    
 
-            num_count += 1
     # close the simulator
     env.close()
 
 
+
+
 if __name__ == "__main__":
     # run the main function
     main()
diff --git a/isaaclab/standalone/train.py b/isaaclab/standalone/train.py
index 1fa6f1f..53e52c4 100644
--- a/isaaclab/standalone/train.py
+++ b/isaaclab/standalone/train.py
@@ -63,8 +63,8 @@ from omni.isaac.lab_tasks.utils import get_checkpoint_path, parse_env_cfg
 
 # from omni.isaac.viplanner.config import H1RoughEnvCfg, H1BaseRoughEnvCfg, H12DoFRoughEnvCfg, H1VisionRoughEnvCfg, G1VisionRoughEnvCfg
 # from omni.isaac.viplanner.config import H1RoughEnvCfg_PLAY, H1BaseRoughEnvCfg_PLAY, H12DoFRoughEnvCfg_PLAY, H1VisionRoughEnvCfg_PLAY, G1VisionRoughEnvCfg_PLAY
-from omni.isaac.viplanner.config import *
-from omni.isaac.viplanner.utils import RslRlVecEnvHistoryWrapper
+from omni.isaac.leggednav.config import *
+from omni.isaac.leggednav.utils import RslRlVecEnvHistoryWrapper
 
 torch.backends.cuda.matmul.allow_tf32 = True
 torch.backends.cudnn.allow_tf32 = True
diff --git a/pyproject.toml b/pyproject.toml
deleted file mode 100644
index 95ef551..0000000
--- a/pyproject.toml
+++ /dev/null
@@ -1,53 +0,0 @@
-[build-system]
-requires = ["setuptools", "wheel"]
-build-backend = "setuptools.build_meta"
-
-[project]
-name = "viplanner"
-version = "0.1.0"
-description = "Visual Imperative Planner for Legged Robots"
-authors = [{name = "Pascal Roth", email = "rothpa@ethz.ch"}]
-license = {file = "LICENSE.txt"}
-readme = "README.md"
-requires-python = ">=3.7"
-keywords = ["robotics", "planning", "legged-robots"]
-classifiers = [
-    "Development Status :: 3 - Alpha",
-    "Intended Audience :: Science/Research",
-    "License :: OSI Approved :: BSD License",
-    "Programming Language :: Python :: 3.7",
-    "Programming Language :: Python :: 3.8",
-    "Programming Language :: Python :: 3.9",
-]
-dependencies = [
-    "torch",
-    "torchvision",
-    "PyYAML==6.0",
-    "tqdm",
-    "matplotlib",
-    "networkx",
-    "scipy",
-    "open3d==0.17.0",
-    "wandb==0.14.0",
-    "opencv-python-headless",
-]
-
-[project.optional-dependencies]
-inference = [
-    "mmcv==2.0.0",
-    "mmengine",
-    "mmdet",
-]
-standard = [
-    "pypose",
-]
-jetson = [
-    "torch==1.11",
-]
-
-[project.urls]
-homepage = "https://github.com/pascal-roth/viplanner"
-repository = "https://github.com/pascal-roth/viplanner.git"
-
-[tool.setuptools.packages]
-find = {}
\ No newline at end of file
diff --git a/tests/test_history_obs.py b/tests/test_history_obs.py
deleted file mode 100644
index 8a4ea93..0000000
--- a/tests/test_history_obs.py
+++ /dev/null
@@ -1,100 +0,0 @@
-import argparse
-
-from omni.isaac.lab.app import AppLauncher
-
-# local imports
-from isaaclab.standalone import cli_args  # isort: skip
-
-# add argparse arguments
-parser = argparse.ArgumentParser(description="Train an RL agent with RSL-RL.")
-parser.add_argument("--video", action="store_true", default=False, help="Record videos during training.")
-parser.add_argument("--video_length", type=int, default=200, help="Length of the recorded video (in steps).")
-parser.add_argument("--video_interval", type=int, default=2000, help="Interval between video recordings (in steps).")
-parser.add_argument("--cpu", action="store_true", default=False, help="Use CPU pipeline.")
-parser.add_argument(
-    "--disable_fabric", action="store_true", default=False, help="Disable fabric and use USD I/O operations."
-)
-parser.add_argument("--num_envs", type=int, default=None, help="Number of environments to simulate.")
-parser.add_argument("--task", type=str, default=None, help="Name of the task.")
-parser.add_argument("--seed", type=int, default=None, help="Seed used for the environment")
-parser.add_argument("--max_iterations", type=int, default=None, help="RL Policy training iterations.")
-parser.add_argument("--use_cnn", action="store_true", default=None, help="Name of the run folder to resume from.")
-parser.add_argument("--arm_fixed", action="store_true", default=False, help="Fix the robot's arms.")
-parser.add_argument("--use_rnn", action="store_true", default=False, help="Use RNN in the actor-critic model.")
-parser.add_argument("--use_base_setting", action="store_true", default=False, help="Use the base setting for the robot.")
-parser.add_argument("--history_length", default=4, type=int, help="Length of history buffer.")
-
-# append RSL-RL cli arguments
-cli_args.add_rsl_rl_args(parser)
-# append AppLauncher cli args
-AppLauncher.add_app_launcher_args(parser)
-args_cli = parser.parse_args()
-
-# launch omniverse app
-app_launcher = AppLauncher(args_cli)
-simulation_app = app_launcher.app
-
-"""Rest everything follows."""
-
-import gymnasium as gym
-import os
-import math
-import torch
-from datetime import datetime
-
-from rsl_rl.runners import OnPolicyRunnerHistory
-
-import omni.isaac.lab_tasks  # noqa: F401
-from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
-    RslRlOnPolicyRunnerCfg,
-)
-
-from omni.isaac.viplanner.config import H1BaseRoughPPORunnerCfg, H1BaseRoughEnvCfg_PLAY
-from omni.isaac.viplanner.utils import RslRlVecEnvHistoryWrapper
-
-def test_env(env):
-    obs, extras = env.get_observations()
-    assert obs.shape == (env.num_envs, env.history_length * env.observation_space["policy"].shape[1])
-    assert extras["observations"]["policy"].shape == (env.num_envs, env.history_length * env.observation_space["policy"].shape[1])
-    obses = [obs.reshape(env.num_envs, env.history_length, -1)]
-    for _ in range(100):
-        action = env.action_space.sample()
-        obs, rew, done, info = env.step(torch.from_numpy(action))
-        obs_reshaped = obs.view(env.num_envs, env.history_length, -1)
-        obses.append(obs_reshaped)
-    print("Env Test Passed!")
-    return obses
-
-
-def main():
-    """Play with RSL-RL agent."""
-    # parse configuration
-    env_cfg = H1BaseRoughEnvCfg_PLAY()
-    # env_cfg.curriculum = None
-    if args_cli.num_envs:
-        env_cfg.scene.num_envs = args_cli.num_envs 
-    agent_cfg: RslRlOnPolicyRunnerCfg = H1BaseRoughPPORunnerCfg()
-
-    # specify directory for logging experiments
-    log_root_path = os.path.join("logs", "rsl_rl", agent_cfg.experiment_name)
-    log_root_path = os.path.abspath(log_root_path)
-    print(f"[INFO] Logging experiment in directory: {log_root_path}")
-    # specify directory for logging runs: {time-stamp}_{run_name}
-    log_dir = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
-    if agent_cfg.run_name:
-        log_dir += f"_{agent_cfg.run_name}"
-    log_dir = os.path.join(log_root_path, log_dir)
-
-    # max iterations for training
-    if args_cli.max_iterations:
-        agent_cfg.max_iterations = args_cli.max_iterations
-
-    # create isaac environment
-    env = gym.make(args_cli.task, cfg=env_cfg, render_mode="rgb_array" if args_cli.video else None)
-    env = RslRlVecEnvHistoryWrapper(env, history_length=10)
-
-    test_env(env)
-
-
-if __name__ == "__main__":
-    main()
\ No newline at end of file
diff --git a/viplanner/__init__.py b/viplanner/__init__.py
deleted file mode 100644
index 890101b..0000000
--- a/viplanner/__init__.py
+++ /dev/null
@@ -1,9 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# This line will be programmatically read/write by setup.py.
-# Leave them at the bottom of this file and don't touch them.
-__version__ = "0.1"
diff --git a/viplanner/config/__init__.py b/viplanner/config/__init__.py
deleted file mode 100644
index 7de7173..0000000
--- a/viplanner/config/__init__.py
+++ /dev/null
@@ -1,34 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from .coco_sem_meta import _COCO_MAPPING, get_class_for_id
-from .costmap_cfg import (
-    CostMapConfig,
-    GeneralCostMapConfig,
-    ReconstructionCfg,
-    SemCostMapConfig,
-    TsdfCostMapConfig,
-)
-from .learning_cfg import DataCfg, TrainCfg
-from .viplanner_sem_meta import OBSTACLE_LOSS, VIPlannerSemMetaHandler
-
-__all__ = [
-    # configs
-    "ReconstructionCfg",
-    "SemCostMapConfig",
-    "TsdfCostMapConfig",
-    "CostMapConfig",
-    "GeneralCostMapConfig",
-    "TrainCfg",
-    "DataCfg",
-    # mapping
-    "VIPlannerSemMetaHandler",
-    "OBSTACLE_LOSS",
-    "get_class_for_id",
-    "_COCO_MAPPING",
-]
-
-# EoF
diff --git a/viplanner/config/coco_sem_meta.py b/viplanner/config/coco_sem_meta.py
deleted file mode 100644
index 5f53351..0000000
--- a/viplanner/config/coco_sem_meta.py
+++ /dev/null
@@ -1,374 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# Modified from https://github.com/google-research/deeplab2/blob/main/data/coco_constants.py
-# File containing the meta info of all classes from the COCO dataset.
-
-COCO_CATEGORIES = [
-    {"color": [220, 20, 60], "isthing": 1, "id": 1, "name": "person"},
-    {"color": [119, 11, 32], "isthing": 1, "id": 2, "name": "bicycle"},
-    {"color": [0, 0, 142], "isthing": 1, "id": 3, "name": "car"},
-    {"color": [0, 0, 230], "isthing": 1, "id": 4, "name": "motorcycle"},
-    {"color": [106, 0, 228], "isthing": 1, "id": 5, "name": "airplane"},
-    {"color": [0, 60, 100], "isthing": 1, "id": 6, "name": "bus"},
-    {"color": [0, 80, 100], "isthing": 1, "id": 7, "name": "train"},
-    {"color": [0, 0, 70], "isthing": 1, "id": 8, "name": "truck"},
-    {"color": [0, 0, 192], "isthing": 1, "id": 9, "name": "boat"},
-    {"color": [250, 170, 30], "isthing": 1, "id": 10, "name": "traffic light"},
-    {"color": [100, 170, 30], "isthing": 1, "id": 11, "name": "fire hydrant"},
-    {"color": [220, 220, 0], "isthing": 1, "id": 13, "name": "stop sign"},
-    {"color": [175, 116, 175], "isthing": 1, "id": 14, "name": "parking meter"},
-    {"color": [250, 0, 30], "isthing": 1, "id": 15, "name": "bench"},
-    {"color": [165, 42, 42], "isthing": 1, "id": 16, "name": "bird"},
-    {"color": [255, 77, 255], "isthing": 1, "id": 17, "name": "cat"},
-    {"color": [0, 226, 252], "isthing": 1, "id": 18, "name": "dog"},
-    {"color": [182, 182, 255], "isthing": 1, "id": 19, "name": "horse"},
-    {"color": [0, 82, 0], "isthing": 1, "id": 20, "name": "sheep"},
-    {"color": [120, 166, 157], "isthing": 1, "id": 21, "name": "cow"},
-    {"color": [110, 76, 0], "isthing": 1, "id": 22, "name": "elephant"},
-    {"color": [174, 57, 255], "isthing": 1, "id": 23, "name": "bear"},
-    {"color": [199, 100, 0], "isthing": 1, "id": 24, "name": "zebra"},
-    {"color": [72, 0, 118], "isthing": 1, "id": 25, "name": "giraffe"},
-    {"color": [255, 179, 240], "isthing": 1, "id": 27, "name": "backpack"},
-    {"color": [0, 125, 92], "isthing": 1, "id": 28, "name": "umbrella"},
-    {"color": [209, 0, 151], "isthing": 1, "id": 31, "name": "handbag"},
-    {"color": [188, 208, 182], "isthing": 1, "id": 32, "name": "tie"},
-    {"color": [0, 220, 176], "isthing": 1, "id": 33, "name": "suitcase"},
-    {"color": [255, 99, 164], "isthing": 1, "id": 34, "name": "frisbee"},
-    {"color": [92, 0, 73], "isthing": 1, "id": 35, "name": "skis"},
-    {"color": [133, 129, 255], "isthing": 1, "id": 36, "name": "snowboard"},
-    {"color": [78, 180, 255], "isthing": 1, "id": 37, "name": "sports ball"},
-    {"color": [0, 228, 0], "isthing": 1, "id": 38, "name": "kite"},
-    {"color": [174, 255, 243], "isthing": 1, "id": 39, "name": "baseball bat"},
-    {"color": [45, 89, 255], "isthing": 1, "id": 40, "name": "baseball glove"},
-    {"color": [134, 134, 103], "isthing": 1, "id": 41, "name": "skateboard"},
-    {"color": [145, 148, 174], "isthing": 1, "id": 42, "name": "surfboard"},
-    {"color": [255, 208, 186], "isthing": 1, "id": 43, "name": "tennis racket"},
-    {"color": [197, 226, 255], "isthing": 1, "id": 44, "name": "bottle"},
-    {"color": [171, 134, 1], "isthing": 1, "id": 46, "name": "wine glass"},
-    {"color": [109, 63, 54], "isthing": 1, "id": 47, "name": "cup"},
-    {"color": [207, 138, 255], "isthing": 1, "id": 48, "name": "fork"},
-    {"color": [151, 0, 95], "isthing": 1, "id": 49, "name": "knife"},
-    {"color": [9, 80, 61], "isthing": 1, "id": 50, "name": "spoon"},
-    {"color": [84, 105, 51], "isthing": 1, "id": 51, "name": "bowl"},
-    {"color": [74, 65, 105], "isthing": 1, "id": 52, "name": "banana"},
-    {"color": [166, 196, 102], "isthing": 1, "id": 53, "name": "apple"},
-    {"color": [208, 195, 210], "isthing": 1, "id": 54, "name": "sandwich"},
-    {"color": [255, 109, 65], "isthing": 1, "id": 55, "name": "orange"},
-    {"color": [0, 143, 149], "isthing": 1, "id": 56, "name": "broccoli"},
-    {"color": [179, 0, 194], "isthing": 1, "id": 57, "name": "carrot"},
-    {"color": [209, 99, 106], "isthing": 1, "id": 58, "name": "hot dog"},
-    {"color": [5, 121, 0], "isthing": 1, "id": 59, "name": "pizza"},
-    {"color": [227, 255, 205], "isthing": 1, "id": 60, "name": "donut"},
-    {"color": [147, 186, 208], "isthing": 1, "id": 61, "name": "cake"},
-    {"color": [153, 69, 1], "isthing": 1, "id": 62, "name": "chair"},
-    {"color": [3, 95, 161], "isthing": 1, "id": 63, "name": "couch"},
-    {"color": [163, 255, 0], "isthing": 1, "id": 64, "name": "potted plant"},
-    {"color": [119, 0, 170], "isthing": 1, "id": 65, "name": "bed"},
-    {"color": [0, 182, 199], "isthing": 1, "id": 67, "name": "dining table"},
-    {"color": [0, 165, 120], "isthing": 1, "id": 70, "name": "toilet"},
-    {"color": [183, 130, 88], "isthing": 1, "id": 72, "name": "tv"},
-    {"color": [95, 32, 0], "isthing": 1, "id": 73, "name": "laptop"},
-    {"color": [130, 114, 135], "isthing": 1, "id": 74, "name": "mouse"},
-    {"color": [110, 129, 133], "isthing": 1, "id": 75, "name": "remote"},
-    {"color": [166, 74, 118], "isthing": 1, "id": 76, "name": "keyboard"},
-    {"color": [219, 142, 185], "isthing": 1, "id": 77, "name": "cell phone"},
-    {"color": [79, 210, 114], "isthing": 1, "id": 78, "name": "microwave"},
-    {"color": [178, 90, 62], "isthing": 1, "id": 79, "name": "oven"},
-    {"color": [65, 70, 15], "isthing": 1, "id": 80, "name": "toaster"},
-    {"color": [127, 167, 115], "isthing": 1, "id": 81, "name": "sink"},
-    {"color": [59, 105, 106], "isthing": 1, "id": 82, "name": "refrigerator"},
-    {"color": [142, 108, 45], "isthing": 1, "id": 84, "name": "book"},
-    {"color": [196, 172, 0], "isthing": 1, "id": 85, "name": "clock"},
-    {"color": [95, 54, 80], "isthing": 1, "id": 86, "name": "vase"},
-    {"color": [128, 76, 255], "isthing": 1, "id": 87, "name": "scissors"},
-    {"color": [201, 57, 1], "isthing": 1, "id": 88, "name": "teddy bear"},
-    {"color": [246, 0, 122], "isthing": 1, "id": 89, "name": "hair drier"},
-    {"color": [191, 162, 208], "isthing": 1, "id": 90, "name": "toothbrush"},
-    {"color": [255, 255, 128], "isthing": 0, "id": 92, "name": "banner"},
-    {"color": [147, 211, 203], "isthing": 0, "id": 93, "name": "blanket"},
-    {"color": [150, 100, 100], "isthing": 0, "id": 95, "name": "bridge"},
-    {"color": [168, 171, 172], "isthing": 0, "id": 100, "name": "cardboard"},
-    {"color": [146, 112, 198], "isthing": 0, "id": 107, "name": "counter"},
-    {"color": [210, 170, 100], "isthing": 0, "id": 109, "name": "curtain"},
-    {"color": [92, 136, 89], "isthing": 0, "id": 112, "name": "door-stuff"},
-    {"color": [218, 88, 184], "isthing": 0, "id": 118, "name": "floor-wood"},
-    {"color": [241, 129, 0], "isthing": 0, "id": 119, "name": "flower"},
-    {"color": [217, 17, 255], "isthing": 0, "id": 122, "name": "fruit"},
-    {"color": [124, 74, 181], "isthing": 0, "id": 125, "name": "gravel"},
-    {"color": [70, 70, 70], "isthing": 0, "id": 128, "name": "house"},
-    {"color": [255, 228, 255], "isthing": 0, "id": 130, "name": "light"},
-    {"color": [154, 208, 0], "isthing": 0, "id": 133, "name": "mirror-stuff"},
-    {"color": [193, 0, 92], "isthing": 0, "id": 138, "name": "net"},
-    {"color": [76, 91, 113], "isthing": 0, "id": 141, "name": "pillow"},
-    {"color": [255, 180, 195], "isthing": 0, "id": 144, "name": "platform"},
-    {"color": [106, 154, 176], "isthing": 0, "id": 145, "name": "playingfield"},
-    {"color": [230, 150, 140], "isthing": 0, "id": 147, "name": "railroad"},
-    {"color": [60, 143, 255], "isthing": 0, "id": 148, "name": "river"},
-    {"color": [128, 64, 128], "isthing": 0, "id": 149, "name": "road"},
-    {"color": [92, 82, 55], "isthing": 0, "id": 151, "name": "roof"},
-    {"color": [254, 212, 124], "isthing": 0, "id": 154, "name": "sand"},
-    {"color": [73, 77, 174], "isthing": 0, "id": 155, "name": "sea"},
-    {"color": [255, 160, 98], "isthing": 0, "id": 156, "name": "shelf"},
-    {"color": [255, 255, 255], "isthing": 0, "id": 159, "name": "snow"},
-    {"color": [104, 84, 109], "isthing": 0, "id": 161, "name": "stairs"},
-    {"color": [169, 164, 131], "isthing": 0, "id": 166, "name": "tent"},
-    {"color": [225, 199, 255], "isthing": 0, "id": 168, "name": "towel"},
-    {"color": [137, 54, 74], "isthing": 0, "id": 171, "name": "wall-brick"},
-    {"color": [135, 158, 223], "isthing": 0, "id": 175, "name": "wall-stone"},
-    {"color": [7, 246, 231], "isthing": 0, "id": 176, "name": "wall-tile"},
-    {"color": [107, 255, 200], "isthing": 0, "id": 177, "name": "wall-wood"},
-    {"color": [58, 41, 149], "isthing": 0, "id": 178, "name": "water-other"},
-    {"color": [183, 121, 142], "isthing": 0, "id": 180, "name": "window-blind"},
-    {"color": [255, 73, 97], "isthing": 0, "id": 181, "name": "window-other"},
-    {"color": [107, 142, 35], "isthing": 0, "id": 184, "name": "tree-merged"},
-    {"color": [190, 153, 153], "isthing": 0, "id": 185, "name": "fence-merged"},
-    {"color": [146, 139, 141], "isthing": 0, "id": 186, "name": "ceiling-merged"},
-    {"color": [70, 130, 180], "isthing": 0, "id": 187, "name": "sky-other-merged"},
-    {"color": [134, 199, 156], "isthing": 0, "id": 188, "name": "cabinet-merged"},
-    {"color": [209, 226, 140], "isthing": 0, "id": 189, "name": "table-merged"},
-    {"color": [96, 36, 108], "isthing": 0, "id": 190, "name": "floor-other-merged"},
-    {"color": [96, 96, 96], "isthing": 0, "id": 191, "name": "pavement-merged"},
-    {"color": [64, 170, 64], "isthing": 0, "id": 192, "name": "mountain-merged"},
-    {"color": [152, 251, 152], "isthing": 0, "id": 193, "name": "grass-merged"},
-    {"color": [208, 229, 228], "isthing": 0, "id": 194, "name": "dirt-merged"},
-    {"color": [206, 186, 171], "isthing": 0, "id": 195, "name": "paper-merged"},
-    {"color": [152, 161, 64], "isthing": 0, "id": 196, "name": "food-other-merged"},
-    {"color": [116, 112, 0], "isthing": 0, "id": 197, "name": "building-other-merged"},
-    {"color": [0, 114, 143], "isthing": 0, "id": 198, "name": "rock-merged"},
-    {"color": [102, 102, 156], "isthing": 0, "id": 199, "name": "wall-other-merged"},
-    {"color": [250, 141, 255], "isthing": 0, "id": 200, "name": "rug-merged"},
-]
-
-_COCO_MAPPING = {
-    "road": ["road"],
-    "sidewalk": [
-        "pavement-merged",
-    ],
-    "floor": [
-        "floor-other-merged",
-        "floor-wood",
-        "platform",
-        "playingfield",
-        "rug-merged",
-    ],
-    "gravel": [
-        "gravel",
-    ],
-    "stairs": [
-        "stairs",
-    ],
-    "sand": [
-        "sand",
-    ],
-    "snow": [
-        "snow",
-    ],
-    "person": ["person"],
-    "anymal": [
-        "bird",
-        "cat",
-        "dog",
-        "horse",
-        "sheep",
-        "cow",
-        "elephant",
-        "bear",
-        "zebra",
-        "giraffe",
-    ],
-    "vehicle": [
-        "car",
-        "bus",
-        "truck",
-        "boat",
-    ],
-    "on_rails": [
-        "train",
-        "railroad",
-    ],
-    "motorcycle": [
-        "motorcycle",
-    ],
-    "bicycle": [
-        "bicycle",
-    ],
-    "building": [
-        "building-other-merged",
-        "house",
-        "roof",
-    ],
-    "wall": [
-        "wall-other-merged",
-        "curtain",
-        "mirror-stuff",
-        "wall-brick",
-        "wall-stone",
-        "wall-tile",
-        "wall-wood",
-        "window-blind",
-        "window-other",
-    ],
-    "fence": [
-        "fence-merged",
-    ],
-    "bridge": [
-        "bridge",
-    ],
-    "pole": [
-        "fire hydrant",
-        "parking meter",
-    ],
-    "traffic_sign": [
-        "stop sign",
-    ],
-    "traffic_light": [
-        "traffic light",
-    ],
-    "bench": [
-        "bench",
-    ],
-    "vegetation": [
-        "potted plant",
-        "flower",
-        "tree-merged",
-        "mountain-merged",
-        "rock-merged",
-    ],
-    "terrain": [
-        "grass-merged",
-        "dirt-merged",
-    ],
-    "water_surface": [
-        "river",
-        "sea",
-        "water-other",
-    ],
-    "sky": [
-        "sky-other-merged",
-        "airplane",
-    ],
-    "dynamic": [
-        "backpack",
-        "umbrella",
-        "handbag",
-        "tie",
-        "suitcase",
-        "book",
-        # sports
-        "frisbee",
-        "skis",
-        "snowboard",
-        "sports ball",
-        "kite",
-        "baseball bat",
-        "baseball glove",
-        "skateboard",
-        "surfboard",
-        "tennis racket",
-        # kitchen
-        "bottle",
-        "wine glass",
-        "cup",
-        "fork",
-        "knife",
-        "spoon",
-        "bowl",
-        "microwave",
-        "oven",
-        "toaster",
-        "sink",
-        "refrigerator",
-        # food
-        "banana",
-        "sandwich",
-        "orange",
-        "broccoli",
-        "carrot",
-        "hot dog",
-        "pizza",
-        "donut",
-        "cake",
-        "fruit",
-        "food-other-merged",
-        "apple",
-        # computer hardware
-        "mouse",
-        "remote",
-        "keyboard",
-        "cell phone",
-        "laptop",
-        # other
-        "scissors",
-        "teddy bear",
-        "hair drier",
-        "toothbrush",
-        "net",
-        "paper-merged",
-    ],
-    "static": [
-        "banner",
-        "cardboard",
-        "light",
-        "tent",
-        "unknown",
-    ],
-    "furniture": [
-        "chair",
-        "couch",
-        "bed",
-        "dining table",
-        "toilet",
-        "clock",
-        "vase",
-        "blanket",
-        "pillow",
-        "shelf",
-        "cabinet",
-        "table-merged",
-        "counter",
-        "tv",
-    ],
-    "door": [
-        "door-stuff",
-    ],
-    "ceiling": ["ceiling-merged"],
-    "indoor_soft": [
-        "towel",
-    ],
-}
-
-
-def get_class_for_id():
-    id_to_class = {}
-    for idx, id_dict in enumerate(COCO_CATEGORIES):
-        success = False
-        for class_name, keywords in _COCO_MAPPING.items():
-            if any(keyword in id_dict["name"] for keyword in keywords):
-                id_to_class[idx] = class_name
-                success = True
-                break
-        if not success:
-            print("No mapping found for {}".format(id_dict["name"]))
-    return id_to_class
-
-
-def get_class_for_id_mmdet(class_list: list):
-    id_to_class = {}
-    for idx, coco_class_name in enumerate(class_list):
-        success = False
-        for class_name, keywords in _COCO_MAPPING.items():
-            if any(keyword in coco_class_name for keyword in keywords):
-                id_to_class[idx] = class_name
-                success = True
-                break
-        if not success:
-            print("No mapping found for {}".format(coco_class_name["name"]))
-    return id_to_class
-
-
-if __name__ == "__main__":
-    print(get_class_for_id())
diff --git a/viplanner/config/costmap_cfg.py b/viplanner/config/costmap_cfg.py
deleted file mode 100644
index ceb0539..0000000
--- a/viplanner/config/costmap_cfg.py
+++ /dev/null
@@ -1,197 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# python
-import os
-from dataclasses import dataclass
-from typing import Optional
-
-import yaml
-
-
-class Loader(yaml.SafeLoader):
-    pass
-
-
-def construct_GeneralCostMapConfig(loader, node):
-    return GeneralCostMapConfig(**loader.construct_mapping(node))
-
-
-Loader.add_constructor(
-    "tag:yaml.org,2002:python/object:viplanner.config.costmap_cfg.GeneralCostMapConfig",
-    construct_GeneralCostMapConfig,
-)
-
-
-def construct_ReconstructionCfg(loader, node):
-    return ReconstructionCfg(**loader.construct_mapping(node))
-
-
-Loader.add_constructor(
-    "tag:yaml.org,2002:python/object:viplanner.config.costmap_cfg.ReconstructionCfg",
-    construct_ReconstructionCfg,
-)
-
-
-def construct_SemCostMapConfig(loader, node):
-    return SemCostMapConfig(**loader.construct_mapping(node))
-
-
-Loader.add_constructor(
-    "tag:yaml.org,2002:python/object:viplanner.config.costmap_cfg.SemCostMapConfig",
-    construct_SemCostMapConfig,
-)
-
-
-def construct_TsdfCostMapConfig(loader, node):
-    return TsdfCostMapConfig(**loader.construct_mapping(node))
-
-
-Loader.add_constructor(
-    "tag:yaml.org,2002:python/object:viplanner.config.costmap_cfg.TsdfCostMapConfig",
-    construct_TsdfCostMapConfig,
-)
-
-
-@dataclass
-class ReconstructionCfg:
-    """
-    Arguments for 3D reconstruction using depth maps
-    """
-
-    # directory where the environment with the depth (and semantic) images is located
-    data_dir: str = "${USER_PATH_TO_DATA}"
-    # environment name
-    env: str = "town01"
-    # image suffix
-    depth_suffix = "_cam0"
-    sem_suffix = "_cam1"
-    # higher resolution depth images available for reconstruction  (meaning that the depth images are also taked by the semantic camera)
-    high_res_depth: bool = False
-
-    # reconstruction parameters
-    voxel_size: float = 0.05  # [m] 0.05 for matterport 0.1 for carla
-    start_idx: int = 0  # start index for reconstruction
-    max_images: Optional[int] = 1000  # maximum number of images to reconstruct, if None, all images are used
-    depth_scale: float = 1000.0  # depth scale factor
-    # semantic reconstruction
-    semantics: bool = True
-
-    # speed vs. memory trade-off parameters
-    point_cloud_batch_size: int = (
-        200  # 3d points of nbr images added to point cloud at once (higher values use more memory but faster)
-    )
-
-    """ Internal functions """
-
-    def get_data_path(self) -> str:
-        return os.path.join(self.data_dir, self.env)
-
-    def get_out_path(self) -> str:
-        return os.path.join(self.out_dir, self.env)
-
-
-@dataclass
-class SemCostMapConfig:
-    """Configuration for the semantic cost map"""
-
-    # point-cloud filter parameters
-    ground_height: Optional[float] = -0.5  # None for matterport  -0.5 for carla  -1.0 for nomoko
-    robot_height: float = 0.70
-    robot_height_factor: float = 3.0
-    nb_neighbors: int = 100
-    std_ratio: float = 2.0  # keep high, otherwise ground will be removed
-    downsample: bool = False
-    # smoothing
-    nb_neigh: int = 15
-    change_decimal: int = 3
-    conv_crit: float = (
-        0.45  # ration of points that have to change by at least the #change_decimal decimal value to converge
-    )
-    nb_tasks: Optional[int] = 10  # number of tasks for parallel processing, if None, all available cores are used
-    sigma_smooth: float = 2.5
-    max_iterations: int = 1
-    # obstacle threshold  (multiplied with highest loss value defined for a semantic class)
-    obstacle_threshold: float = 0.8  # 0.5/ 0.6 for matterport, 0.8 for carla
-    # negative reward for space with smallest cost (introduces a gradient in area with smallest loss value, steering towards center)
-    # NOTE: at the end cost map is elevated by that amount to ensure that the smallest cost is 0
-    negative_reward: float = 0.5
-    # loss values rounded up to decimal #round_decimal_traversable equal to 0.0 are selected and the traversable gradient is determined based on them
-    round_decimal_traversable: int = 2
-    # compute height map
-    compute_height_map: bool = False  # false for matterport, true for carla and nomoko
-
-
-@dataclass
-class TsdfCostMapConfig:
-    """Configuration for the tsdf cost map"""
-
-    # offset of the point cloud
-    offset_z: float = 0.0
-    # filter parameters
-    ground_height: float = 0.35
-    robot_height: float = 0.70
-    robot_height_factor: float = 2.0
-    nb_neighbors: int = 50
-    std_ratio: float = 0.2
-    filter_outliers: bool = True
-    # dilation parameters
-    sigma_expand: float = 2.0
-    obstacle_threshold: float = 0.01
-    free_space_threshold: float = 0.5
-
-
-@dataclass
-class GeneralCostMapConfig:
-    """General Cost Map Configuration"""
-
-    # path to point cloud
-    root_path: str = "town01"
-    ply_file: str = "cloud.ply"
-    # resolution of the cost map
-    resolution: float = 0.1  # [m]  (0.04 for matterport, 0.1 for carla)
-    # map parameters
-    clear_dist: float = 1.0  # cost map expansion over the point cloud space (prevent paths to go out of the map)
-    # smoothing parameters
-    sigma_smooth: float = 3.0
-    # cost map expansion
-    x_min: Optional[float] = -8.05
-    # [m] if None, the minimum of the point cloud is used None (carla town01:  -8.05   matterport: None)
-    y_min: Optional[float] = -8.05
-    # [m] if None, the minimum of the point cloud is used None (carla town01:  -8.05   matterport: None)
-    x_max: Optional[float] = 346.22
-    # [m] if None, the maximum of the point cloud is used None (carla town01:  346.22  matterport: None)
-    y_max: Optional[float] = 336.65
-    # [m] if None, the maximum of the point cloud is used None (carla town01:  336.65  matterport: None)
-
-
-@dataclass
-class CostMapConfig:
-    """General Cost Map Configuration"""
-
-    # cost map domains
-    semantics: bool = True
-    geometry: bool = False
-
-    # name
-    map_name: str = "cost_map_sem"
-
-    # general cost map configuration
-    general: GeneralCostMapConfig = GeneralCostMapConfig()
-
-    # individual cost map configurations
-    sem_cost_map: SemCostMapConfig = SemCostMapConfig()
-    tsdf_cost_map: TsdfCostMapConfig = TsdfCostMapConfig()
-
-    # visualize cost map
-    visualize: bool = True
-
-    # FILLED BY CODE -> DO NOT CHANGE ###
-    x_start: float = None
-    y_start: float = None
-
-
-# EoF
diff --git a/viplanner/config/learning_cfg.py b/viplanner/config/learning_cfg.py
deleted file mode 100644
index 416b64d..0000000
--- a/viplanner/config/learning_cfg.py
+++ /dev/null
@@ -1,245 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import os
-
-# python
-from dataclasses import dataclass, field
-from typing import List, Optional, Tuple, Union
-
-import yaml
-
-
-# define own loader class to include DataCfg
-class Loader(yaml.SafeLoader):
-    pass
-
-
-def construct_datacfg(loader, node):
-    add_dicts = {}
-    for node_entry in node.value:
-        if isinstance(node_entry[1], yaml.MappingNode):
-            add_dicts[node_entry[0].value] = loader.construct_mapping(node_entry[1])
-            node.value.remove(node_entry)
-
-    return DataCfg(**loader.construct_mapping(node), **add_dicts)
-
-
-Loader.add_constructor(
-    "tag:yaml.org,2002:python/object:viplanner.config.learning_cfg.DataCfg",
-    construct_datacfg,
-)
-
-
-@dataclass
-class DataCfg:
-    """Config for data loading"""
-
-    # real world data used --> images have to be rotated by 180 degrees
-    real_world_data: bool = False
-    # from carla dataset (exclude certain spaces)
-    carla: bool = False
-
-    # identification suffix of the cameras for semantic and depth images
-    depth_suffix = "_cam0"
-    sem_suffix = "_cam1"
-
-    # data processing
-    max_depth: float = 15.0
-    "maximum depth for depth image"
-
-    # odom (=start) point selection
-    max_goal_distance: float = 15.0
-    min_goal_distance: float = 0.5
-    "maximum and minimum distance between odom and goal"
-    distance_scheme: dict = field(default_factory=lambda: {1: 0.2, 3: 0.35, 5: 0.25, 7.5: 0.15, 10: 0.05})
-    # select goal points for the samples according to the scheme:
-    # {distance: percentage of goals}, distances have to be increasing
-    # and max distance has to be equal to max_goal_distance
-    obs_cost_height: float = 1.5
-    "all odom points with cost of more than obs_cost_height are discarded (negative cost of cost_map will be automatically added)"
-    fov_scale: float = 1.0
-    "scaling of the field of view (only goals within fov are considered)"
-    depth_scale: float = 1000.0
-    "scaling of the depth image"
-
-    # train val split
-    ratio: float = 0.9
-    "ratio between train and val dataset"
-    max_train_pairs: Optional[int] = None
-    pairs_per_image: int = 4
-    "maximum number of train pairs (can be used to limit training time) can be set, otherwise number of recorded images times pairs_per_image is used"
-    ratio_fov_samples: float = 1.0
-    ratio_front_samples: float = 0.0
-    ratio_back_samples: float = 0.0
-    "samples distribution -> either within the robots fov, in front of the robot but outside the fov or behind the robot"
-
-    # edge blur (real world RealSense difficulties along edges)  --> will be also visible in rgb/sem images due to warp
-    noise_edges: bool = False  # not activate for CARLA yet
-    edge_threshold: int = 100
-    extend_kernel_size: Tuple[int, int] = field(default_factory=lambda: [5, 5])
-
-    # noise augmentation --> will be applied to a scaled image with range between [0, 1]
-    depth_salt_pepper: Optional[float] = None  # Proportion of image pixels to replace with noise on range [0, 1]
-    depth_gaussian: Optional[float] = None  # Standard deviation of the noise to add (no clipping applied)
-    depth_random_polygons_nb: Optional[int] = None  # Number of random polygons to add
-    depth_random_polygon_size: int = 10  # Size of the random polygons in pixels
-
-    sem_rgb_pepper: Optional[float] = None  # Proportion of pixels to randomly set to 0
-    sem_rgb_black_img: Optional[float] = None  # Randomly set this proportion of images to complete black images  -->
-    sem_rgb_random_polygons_nb: Optional[int] = None  # Number of random polygons to add
-    sem_rgb_random_polygon_size: int = 20  # Size of the random polygons in pixels
-
-
-@dataclass
-class TrainCfg:
-    """Config for multi environment training"""
-
-    # high level configurations
-    sem: bool = True
-    rgb: bool = False
-    "use semantic/ rgb image"
-    file_name: Optional[str] = None
-    "appendix to the model filename if needed"
-    seed: int = 0
-    "random seed"
-    gpu_id: int = 0
-    "GPU id"
-    file_path: str = "${USER_PATH_TO_MODEL_DATA}"
-    "file path to models and data directory, can be overwritten by environment variable EXPERIMENT_DIRECTORY (e.g. for cluster)"
-    # NOTE: since the environment variable is intended for cluster usage, some visualizations will be automatically switched off
-
-    # data and dataloader configurations
-    cost_map_name: str = "cost_map_sem"  # "cost_map_sem"
-    "cost map name"
-    env_list: List[str] = field(
-        default_factory=lambda: [
-            "2azQ1b91cZZ",
-            "JeFG25nYj2p",
-            "Vvot9Ly1tCj",
-            "ur6pFq6Qu1A",
-            "B6ByNegPMKs",
-            "8WUmhLawc2A",
-            "E9uDoFAP3SH",
-            "QUCTc6BB5sX",
-            "YFuZgdQ5vWj",
-            "2n8kARJN3HM",
-        ]
-    )
-    test_env_id: int = 9
-    "the test env id in the id list"
-    data_cfg: Union[DataCfg, List[DataCfg]] = DataCfg()
-    "further data configuration (can be individualized for every environment)"
-    multi_epoch_dataloader: bool = False
-    "load all samples into RAM s.t. do not have to be reloaded for each epoch"
-    num_workers: int = 4
-    "number of workers for dataloader"
-    load_in_ram: bool = False
-    "if true, all samples will be loaded into RAM s.t. do not have to be reloaded for each epoch"
-
-    # loss configurations
-    fear_ahead_dist: float = 2.5
-    "fear lookahead distance"
-    w_obs: float = 0.25
-    w_height: float = 1.0
-    w_motion: float = 1.5
-    w_goal: float = 4.0
-    "weights for the loss components"
-    obstacle_thread: float = 1.2
-    "obstacle threshold to decide if fear path or not (neg reward for semantic cost-maps is added automatically)"
-
-    # network configurations
-    img_input_size: Tuple[int, int] = field(default_factory=lambda: [360, 640])
-    "image size (will be cropped if larger or resized if smaller)"
-    in_channel: int = 16
-    "goal input channel numbers"
-    knodes: int = 5
-    "number of max waypoints predicted"
-    pre_train_sem: bool = True
-    pre_train_cfg: Optional[str] = "m2f_model/coco/panoptic/maskformer2_R50_bs16_50ep.yaml"
-    pre_train_weights: Optional[str] = "m2f_model/coco/panoptic/model_final_94dc52.pkl"
-    pre_train_freeze: bool = True
-    "loading of a pre-trained rgb encoder from mask2former (possible is ResNet 50 or 101)"
-    # NOTE: `pre_train_cfg` and `pre_train_weights` are assumed to be found under `file_path/models` (see above)
-    decoder_small: bool = False
-    "small decoder with less parameters"
-
-    # training configurations
-    resume: bool = False
-    "resume training"
-    epochs: int = 100
-    "number of training epochs"
-    batch_size: int = 64
-    "number of minibatch size"
-    hierarchical: bool = False
-    hierarchical_step: int = 50
-    hierarchical_front_step_ratio: float = 0.02
-    hierarchical_back_step_ratio: float = 0.01
-    "hierarchical training with an adjusted data structure"
-
-    # optimizer and scheduler configurations
-    lr: float = 2e-3
-    "learning rate"
-    factor: float = 0.5
-    "ReduceLROnPlateau factor"
-    min_lr: float = 1e-5
-    "minimum lr for ReduceLROnPlateau"
-    patience: int = 3
-    "patience of epochs for ReduceLROnPlateau"
-    optimizer: str = "sgd"  # either adam or sgd
-    "optimizer"
-    momentum: float = 0.1
-    "momentum of the optimizer"
-    w_decay: float = 1e-4
-    "weight decay of the optimizer"
-
-    # visualization configurations
-    camera_tilt: float = 0.15
-    "camera tilt angle for visualization only"
-    n_visualize: int = 15
-    "number of trajectories that are visualized"
-
-    # logging configurations
-    wb_project: str = "Matterport"
-    wb_entity: str = "viplanner"
-    wb_api_key: str = "enter_your_key_here"
-
-    # functions
-    def get_model_save(self, epoch: Optional[int] = None):
-        input_domain = "DepSem" if self.sem else "Dep"
-        cost_name = "Geom" if self.cost_map_name == "cost_map_geom" else "Sem"
-        optim = "SGD" if self.optimizer == "sgd" else "Adam"
-        name = f"_{self.file_name}" if self.file_name is not None else ""
-        epoch = epoch if epoch is not None else self.epochs
-        hierarch = "_hierarch" if self.hierarchical else ""
-        return f"plannernet_env{self.env_list[0]}_ep{epoch}_input{input_domain}_cost{cost_name}_optim{optim}{hierarch}{name}"
-
-    @property
-    def all_model_dir(self):
-        return os.path.join(os.getenv("EXPERIMENT_DIRECTORY", self.file_path), "models")
-
-    @property
-    def curr_model_dir(self):
-        return os.path.join(self.all_model_dir, self.get_model_save())
-
-    @property
-    def data_dir(self):
-        return os.path.join(os.getenv("EXPERIMENT_DIRECTORY", self.file_path), "data")
-
-    @property
-    def log_dir(self):
-        return os.path.join(os.getenv("EXPERIMENT_DIRECTORY", self.file_path), "logs")
-
-    @classmethod
-    def from_yaml(cls, yaml_path: str):
-        # open yaml file and load config
-        with open(yaml_path) as f:
-            cfg_dict = yaml.load(f, Loader=Loader)
-
-        return cls(**cfg_dict["config"])
-
-
-# EoF
diff --git a/viplanner/config/viplanner_sem_meta.py b/viplanner/config/viplanner_sem_meta.py
deleted file mode 100644
index 1f56cfc..0000000
--- a/viplanner/config/viplanner_sem_meta.py
+++ /dev/null
@@ -1,360 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-OBSTACLE_LOSS = 2.0
-TRAVERSABLE_INTENDED_LOSS = 0
-TRAVERSABLE_UNINTENDED_LOSS = 0.5
-ROAD_LOSS = 1.5
-TERRAIN_LOSS = 1.0
-# NOTE: only obstacle loss should be over obscale_loss defined in costmap_cfg.py
-
-# original coco meta
-VIPLANNER_SEM_META = [
-    # TRAVERSABLE SPACE ###
-    # traversable intended
-    {
-        "name": "sidewalk",
-        "loss": TRAVERSABLE_INTENDED_LOSS,
-        "color": [0, 255, 0],
-        "ground": True,
-    },
-    {
-        "name": "crosswalk",
-        "loss": TRAVERSABLE_INTENDED_LOSS,
-        "color": [0, 102, 0],
-        "ground": True,
-    },
-    {
-        "name": "floor",
-        "loss": TRAVERSABLE_INTENDED_LOSS,
-        "color": [0, 204, 0],
-        "ground": True,
-    },
-    {
-        "name": "stairs",
-        "loss": TRAVERSABLE_INTENDED_LOSS,
-        "color": [0, 153, 0],
-        "ground": True,
-    },
-    # traversable not intended
-    {
-        "name": "gravel",
-        "loss": TRAVERSABLE_UNINTENDED_LOSS,
-        "color": [204, 255, 0],
-        "ground": True,
-    },
-    {
-        "name": "sand",
-        "loss": TRAVERSABLE_UNINTENDED_LOSS,
-        "color": [153, 204, 0],
-        "ground": True,
-    },
-    {
-        "name": "snow",
-        "loss": TRAVERSABLE_UNINTENDED_LOSS,
-        "color": [204, 102, 0],
-        "ground": True,
-    },
-    {
-        "name": "indoor_soft",  # human made thing, can be walked on
-        "color": [102, 153, 0],
-        "loss": TERRAIN_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "terrain",
-        "color": [255, 255, 0],
-        "loss": TERRAIN_LOSS,
-        "ground": True,
-    },
-    {
-        "name": "road",
-        "loss": ROAD_LOSS,
-        "color": [255, 128, 0],
-        "ground": True,
-    },
-    # OBSTACLES ###
-    # human
-    {
-        "name": "person",
-        "color": [255, 0, 0],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "anymal",
-        "color": [204, 0, 0],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    # vehicle
-    {
-        "name": "vehicle",
-        "color": [153, 0, 0],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "on_rails",
-        "color": [51, 0, 0],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "motorcycle",
-        "color": [102, 0, 0],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "bicycle",
-        "color": [102, 0, 0],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    # construction
-    {
-        "name": "building",
-        "loss": OBSTACLE_LOSS,
-        "color": [127, 0, 255],
-        "ground": False,
-    },
-    {
-        "name": "wall",
-        "color": [102, 0, 204],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "fence",
-        "color": [76, 0, 153],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "bridge",
-        "color": [51, 0, 102],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "tunnel",
-        "color": [51, 0, 102],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    # object
-    {
-        "name": "pole",
-        "color": [0, 0, 255],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "traffic_sign",
-        "color": [0, 0, 153],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "traffic_light",
-        "color": [0, 0, 204],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "bench",
-        "color": [0, 0, 102],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    # nature
-    {
-        "name": "vegetation",
-        "color": [153, 0, 153],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "water_surface",
-        "color": [204, 0, 204],
-        "loss": OBSTACLE_LOSS,
-        "ground": True,
-    },
-    # sky
-    {
-        "name": "sky",
-        "color": [102, 0, 51],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "background",
-        "color": [102, 0, 51],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    # void outdoor
-    {
-        "name": "dynamic",
-        "color": [32, 0, 32],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "static",  # also everything unknown
-        "color": [0, 0, 0],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    # indoor
-    {
-        "name": "furniture",
-        "color": [0, 0, 51],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "door",
-        "color": [153, 153, 0],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-    {
-        "name": "ceiling",
-        "color": [25, 0, 51],
-        "loss": OBSTACLE_LOSS,
-        "ground": False,
-    },
-]
-
-
-class VIPlannerSemMetaHandler:
-    """Useful functions for handling VIPlanner semantic meta data."""
-
-    def __init__(self) -> None:
-        # meta config
-        self.meta = VIPLANNER_SEM_META
-
-        # class loss dict
-        self.class_loss: dict = self._get_class_loss_dict()
-        self.class_color: dict = self._get_class_color_dict()
-        self.class_ground: dict = self._get_class_ground_dict()
-        self.class_id: dict = self._get_class_id_dict()
-        return
-
-    def get_colors_for_names(self, name_list: list) -> list:
-        """Get list of colors for a list of names."""
-        colors = []
-        name_to_color = {nc["name"]: nc["color"] for nc in self.meta}
-        for name in name_list:
-            if name in name_to_color:
-                colors.append(name_to_color[name])
-        return colors
-
-    def _get_class_loss_dict(self) -> dict:
-        """Get class loss dict."""
-        return {nc["name"]: nc["loss"] for nc in self.meta}
-
-    def _get_class_color_dict(self) -> dict:
-        """Get class color dict."""
-        return {nc["name"]: nc["color"] for nc in self.meta}
-
-    def _get_class_ground_dict(self) -> dict:
-        """Get class ground dict."""
-        return {nc["name"]: nc["ground"] for nc in self.meta}
-
-    def _get_class_id_dict(self) -> dict:
-        """Get class id dict."""
-        return {nc["name"]: i for i, nc in enumerate(self.meta)}
-
-    @property
-    def colors(self) -> list:
-        """Get list of colors."""
-        return list(self.class_color.values())
-
-    @property
-    def losses(self) -> list:
-        """Get list of losses."""
-        return list(self.class_loss.values())
-
-    @property
-    def names(self) -> list:
-        """Get list of names."""
-        return list(self.class_loss.keys())
-
-    @property
-    def ground(self) -> list:
-        """Get list of ground."""
-        return list(self.class_ground.values())
-
-
-"""CLASS COLOR VISUALIZATION"""
-
-if __name__ == "__main__":
-    import matplotlib.pyplot as plt
-
-    # init meta handler
-    meta_handler = VIPlannerSemMetaHandler()
-
-    # class ordering array
-    cls_order = [
-        ["sky", "background", "ceiling", "dynamic", "static"],
-        [
-            "building",
-            "wall",
-            "fence",
-            "vegetation",
-            "water_surface",
-        ],  # 'bridge',
-        [
-            "pole",
-            "traffic_light",
-            "traffic_sign",
-            "bench",
-            "furniture",
-            "door",
-        ],
-        ["gravel", "sand", "indoor_soft", "terrain", "snow", "road"],
-        ["sidewalk", "floor", "stairs", "crosswalk"],
-        ["person", "anymal", "vehicle", "motorcycle", "bicycle", "on_rails"],
-    ]
-
-    # Create the 8x8 grid of subplots
-    fig, axs = plt.subplots(nrows=6, ncols=6, figsize=(10, 10))
-
-    # Loop over each subplot and plot the data
-    for i in range(6):
-        for j in range(6):
-            ax = axs[i][j]
-
-            # Remove the axis, axis ticks, border, ...
-            ax.spines["top"].set_visible(False)
-            ax.spines["right"].set_visible(False)
-            ax.spines["bottom"].set_visible(False)
-            ax.spines["left"].set_visible(False)
-            ax.set_xticks([])
-            ax.set_yticks([])
-
-            # plot color
-            if j >= len(cls_order[i]):
-                continue
-            ax.imshow([[tuple(meta_handler.class_color[cls_order[i][j]])]])
-            ax.set_title(cls_order[i][j], fontsize=16)
-            ax.set_xlabel(meta_handler.class_color[cls_order[i][j]], fontsize=12)
-
-    # Set the overall title of the plot
-    fig.suptitle("VIPlanner Semantic Classes Color Scheme", fontsize=22)
-
-    # Adjust the spacing between subplots
-    plt.subplots_adjust(wspace=0.4, hspace=0.4)
-
-    plt.tight_layout()
-    plt.savefig("/home/{$USER}/viplanner_semantic_classes_color_scheme.png", dpi=300)
-    # Show the plot
-    plt.show()
-
-# EoF
diff --git a/viplanner/cost_builder.py b/viplanner/cost_builder.py
deleted file mode 100644
index 59345c9..0000000
--- a/viplanner/cost_builder.py
+++ /dev/null
@@ -1,53 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# imperative-cost-map
-from viplanner.config import CostMapConfig
-from viplanner.cost_maps import CostMapPCD, SemCostMap, TsdfCostMap
-
-
-def main(cfg: CostMapConfig, final_viz: bool = True):
-    assert any([cfg.semantics, cfg.geometry]), "no cost map type selected"
-
-    # create semantic cost map
-    if cfg.semantics:
-        print("============ Creating Semantic Map from cloud ===============")
-        sem_cost_map = SemCostMap(cfg.general, cfg.sem_cost_map, visualize=cfg.visualize)
-        sem_cost_map.pcd_init()
-        data, coord = sem_cost_map.create_costmap()
-    # create tsdf cost map
-    elif cfg.geometry:
-        print("============== Creating tsdf Map from cloud =================")
-        tsdf_cost_map = TsdfCostMap(cfg.general, cfg.tsdf_cost_map)
-        tsdf_cost_map.ReadPointFromFile()
-        data, coord = tsdf_cost_map.CreateTSDFMap()
-        (tsdf_cost_map.VizCloud(tsdf_cost_map.obs_pcd) if cfg.visualize else None)
-    else:
-        raise ValueError("no cost map type selected")
-
-    # set coords in costmap config
-    cfg.x_start, cfg.y_start = coord
-
-    # construct final cost map as pcd and save parameters
-    print("======== Generate and Save costmap as Point-Cloud ===========")
-    cost_mapper = CostMapPCD(
-        cfg=cfg,
-        tsdf_array=data[0],
-        viz_points=data[1],
-        ground_array=data[2],
-        load_from_file=False,
-    )
-    cost_mapper.SaveTSDFMap()
-    if final_viz:
-        cost_mapper.ShowTSDFMap(cost_map=True)
-    return
-
-
-if __name__ == "__main__":
-    cfg = CostMapConfig()
-    main(cfg)
-
-# EoF
diff --git a/viplanner/cost_maps/__init__.py b/viplanner/cost_maps/__init__.py
deleted file mode 100644
index 2eb6b7c..0000000
--- a/viplanner/cost_maps/__init__.py
+++ /dev/null
@@ -1,13 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from .cost_to_pcd import CostMapPCD
-from .sem_cost_map import SemCostMap
-from .tsdf_cost_map import TsdfCostMap
-
-__all__ = ["TsdfCostMap", "SemCostMap", "CostMapPCD"]
-
-# EoF
diff --git a/viplanner/cost_maps/cost_to_pcd.py b/viplanner/cost_maps/cost_to_pcd.py
deleted file mode 100644
index 2f2081a..0000000
--- a/viplanner/cost_maps/cost_to_pcd.py
+++ /dev/null
@@ -1,235 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import argparse
-import os
-from typing import Optional, Union
-
-import numpy as np
-import open3d as o3d
-import pypose as pp
-import torch
-import yaml
-
-# viplanner
-from viplanner.config.costmap_cfg import CostMapConfig, Loader
-
-torch.set_default_dtype(torch.float32)
-
-
-class CostMapPCD:
-    def __init__(
-        self,
-        cfg: CostMapConfig,
-        tsdf_array: np.ndarray,
-        viz_points: np.ndarray,
-        ground_array: np.ndarray,
-        gpu_id: Optional[int] = 0,
-        load_from_file: Optional[bool] = False,
-    ):
-        # determine device
-        if torch.cuda.is_available() and gpu_id is not None:
-            self.device = torch.device("cuda:" + str(gpu_id))
-        else:
-            self.device = torch.device("cpu")
-
-        # args
-        self.cfg: CostMapConfig = cfg
-        self.load_from_file: bool = load_from_file
-        self.tsdf_array: torch.Tensor = torch.tensor(tsdf_array, device=self.device)
-        self.viz_points: np.ndarray = viz_points
-        self.ground_array: torch.Tensor = torch.tensor(ground_array, device=self.device)
-
-        # init flag
-        self.map_init = False
-
-        # init pointclouds
-        self.pcd_tsdf = o3d.geometry.PointCloud()
-        self.pcd_viz = o3d.geometry.PointCloud()
-
-        # execute setup
-        self.num_x: int = 0
-        self.num_y: int = 0
-        self.setup()
-        return
-
-    def setup(self):
-        # expand of cost map
-        self.num_x, self.num_y = self.tsdf_array.shape
-        # visualization points
-        self.pcd_viz.points = o3d.utility.Vector3dVector(self.viz_points)
-        # set cost map
-        self.SetUpCostArray()
-        # update pcd instance
-        xv, yv = np.meshgrid(
-            np.linspace(0, self.num_x * self.cfg.general.resolution, self.num_x),
-            np.linspace(0, self.num_y * self.cfg.general.resolution, self.num_y),
-            indexing="ij",
-        )
-        T = np.concatenate((np.expand_dims(xv, axis=0), np.expand_dims(yv, axis=0)), axis=0)
-        T = np.concatenate(
-            (
-                T,
-                np.expand_dims(self.cost_array.cpu().detach().numpy(), axis=0),
-            ),
-            axis=0,
-        )
-        if self.load_from_file:
-            wps = T.reshape(3, -1).T + np.array([self.cfg.x_start, self.cfg.y_start, 0.0])
-            self.pcd_tsdf.points = o3d.utility.Vector3dVector(wps)
-        else:
-            self.pcd_tsdf.points = o3d.utility.Vector3dVector(T.reshape(3, -1).T)
-
-        self.map_init = True
-        return
-
-    def ShowTSDFMap(self, cost_map=True):  # not run with cuda
-        if not self.map_init:
-            print("Error: cannot show map, map has not been init yet!")
-            return
-        if cost_map:
-            o3d.visualization.draw_geometries([self.pcd_tsdf])
-        else:
-            o3d.visualization.draw_geometries([self.pcd_viz])
-        return
-
-    def Pos2Ind(self, points: Union[torch.Tensor, pp.LieTensor]):
-        # points [torch shapes [num_p, 3]]
-        start_xy = torch.tensor(
-            [self.cfg.x_start, self.cfg.y_start],
-            dtype=torch.float64,
-            device=points.device,
-        ).expand(1, 1, -1)
-        if isinstance(points, pp.LieTensor):
-            H = (points.tensor()[:, :, 0:2] - start_xy) / self.cfg.general.resolution
-        else:
-            H = (points[:, :, 0:2] - start_xy) / self.cfg.general.resolution
-        mask = torch.logical_and(
-            (H > 0).all(axis=2),
-            (H < torch.tensor([self.num_x, self.num_y], device=points.device)[None, None, :]).all(axis=2),
-        )
-        return self.NormInds(H), H[mask, :]
-
-    def NormInds(self, H):
-        norm_matrix = torch.tensor(
-            [self.num_x / 2.0, self.num_y / 2.0],
-            dtype=torch.float64,
-            device=H.device,
-        )
-        H = (H - norm_matrix) / norm_matrix
-        return H
-
-    def DeNormInds(self, NH):
-        norm_matrix = torch.tensor(
-            [self.num_x / 2.0, self.num_y / 2.0],
-            dtype=torch.float64,
-            device=NH.device,
-        )
-        NH = NH * norm_matrix + norm_matrix
-        return NH
-
-    def SaveTSDFMap(self):
-        if not self.map_init:
-            print("Error: map has not been init yet!")
-            return
-
-        # make directories
-        os.makedirs(
-            os.path.join(self.cfg.general.root_path, "maps", "data"),
-            exist_ok=True,
-        )
-        os.makedirs(
-            os.path.join(self.cfg.general.root_path, "maps", "cloud"),
-            exist_ok=True,
-        )
-        os.makedirs(
-            os.path.join(self.cfg.general.root_path, "maps", "params"),
-            exist_ok=True,
-        )
-
-        map_path = os.path.join(
-            self.cfg.general.root_path,
-            "maps",
-            "data",
-            self.cfg.map_name + "_map.txt",
-        )
-        ground_path = os.path.join(
-            self.cfg.general.root_path,
-            "maps",
-            "data",
-            self.cfg.map_name + "_ground.txt",
-        )
-        cloud_path = os.path.join(
-            self.cfg.general.root_path,
-            "maps",
-            "cloud",
-            self.cfg.map_name + "_cloud.txt",
-        )
-        # save data
-        np.savetxt(map_path, self.tsdf_array.cpu())
-        np.savetxt(ground_path, self.ground_array.cpu())
-        np.savetxt(cloud_path, self.viz_points)
-        # save config parameters
-        yaml_path = os.path.join(
-            self.cfg.general.root_path,
-            "maps",
-            "params",
-            f"config_{self.cfg.map_name}.yaml",
-        )
-        with open(yaml_path, "w+") as file:
-            yaml.dump(
-                vars(self.cfg),
-                file,
-                allow_unicode=True,
-                default_flow_style=False,
-            )
-
-        print("TSDF Map saved.")
-        return
-
-    def SetUpCostArray(self):
-        self.cost_array = self.tsdf_array
-        return
-
-    @classmethod
-    def ReadTSDFMap(cls, root_path: str, map_name: str, gpu_id: Optional[int] = None):
-        # read config
-        with open(os.path.join(root_path, "maps", "params", f"config_{map_name}.yaml")) as f:
-            cfg: CostMapConfig = CostMapConfig(**yaml.load(f, Loader))
-
-        # load data
-        tsdf_array = np.loadtxt(os.path.join(root_path, "maps", "data", map_name + "_map.txt"))
-        viz_points = np.loadtxt(os.path.join(root_path, "maps", "cloud", map_name + "_cloud.txt"))
-        ground_array = np.loadtxt(os.path.join(root_path, "maps", "data", map_name + "_ground.txt"))
-
-        return cls(
-            cfg=cfg,
-            tsdf_array=tsdf_array,
-            viz_points=viz_points,
-            ground_array=ground_array,
-            gpu_id=gpu_id,
-            load_from_file=True,
-        )
-
-
-if __name__ == "__main__":
-    # parse environment directory and cost_map name
-    parser = argparse.ArgumentParser(prog="Show Costmap", description="Show Costmap")
-    parser.add_argument(
-        "-e",
-        "--env",
-        type=str,
-        help="path to the environment directory",
-        required=True,
-    )
-    parser.add_argument("-m", "--map", type=str, help="name of the cost_map", required=True)
-    args = parser.parse_args()
-
-    # show costmap
-    map = CostMapPCD.ReadTSDFMap(args.env, args.map)
-    map.ShowTSDFMap()
-
-# EoF
diff --git a/viplanner/cost_maps/sem_cost_map.py b/viplanner/cost_maps/sem_cost_map.py
deleted file mode 100644
index c98cbd6..0000000
--- a/viplanner/cost_maps/sem_cost_map.py
+++ /dev/null
@@ -1,573 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import math
-import multiprocessing as mp
-
-# python
-import os
-from functools import partial
-from typing import Tuple
-
-import matplotlib.pyplot as plt
-import numpy as np
-import open3d as o3d
-import scipy
-
-# imperative-cost-map
-from viplanner.config import (
-    OBSTACLE_LOSS,
-    GeneralCostMapConfig,
-    SemCostMapConfig,
-    VIPlannerSemMetaHandler,
-)
-
-
-class SemCostMap:
-    """
-    Cost Map based on semantic information
-    """
-
-    def __init__(
-        self,
-        cfg_general: GeneralCostMapConfig,
-        cfg: SemCostMapConfig,
-        visualize: bool = True,
-    ):
-        self._cfg_general = cfg_general
-        self._cfg_sem = cfg
-        self.visualize = visualize
-
-        # init VIPlanner Semantic Class Meta Handler
-        self.sem_meta = VIPlannerSemMetaHandler()
-
-        # cost map init parameters
-        self.pcd: o3d.geometry.PointCloud = None
-        self.pcd_filtered: o3d.geometry.PointCloud = None
-        self.height_map: np.ndarray = None
-        self._num_x: int = 0.0
-        self._num_y: int = 0.0
-        self._start_x: float = 0.0
-        self._start_y: float = 0.0
-        self._init_done: bool = False
-
-        # cost map
-        self.grid_cell_loss: np.ndarray = None
-        return
-
-    def pcd_init(self) -> None:
-        # load pcd and filter it
-        print("COST-MAP INIT START")
-        print("start loading and filtering point cloud from:" f" {self._cfg_general.ply_file}")
-        pc_path = os.path.join(self._cfg_general.root_path, self._cfg_general.ply_file)
-        assert os.path.exists(pc_path), f"point cloud file does not exist: {pc_path}"
-        self.pcd = o3d.io.read_point_cloud(pc_path)
-
-        # filter for x and y coordinates
-        if any(
-            [
-                self._cfg_general.x_max,
-                self._cfg_general.x_min,
-                self._cfg_general.y_max,
-                self._cfg_general.y_min,
-            ]
-        ):
-            pts = np.asarray(self.pcd.points)
-            pts_x_idx_upper = (
-                (pts[:, 0] < self._cfg_general.x_max)
-                if self._cfg_general.x_max is not None
-                else np.ones(pts.shape[0], dtype=bool)
-            )
-            pts_x_idx_lower = (
-                (pts[:, 0] > self._cfg_general.x_min)
-                if self._cfg_general.x_min is not None
-                else np.ones(pts.shape[0], dtype=bool)
-            )
-            pts_y_idx_upper = (
-                (pts[:, 1] < self._cfg_general.y_max)
-                if self._cfg_general.y_max is not None
-                else np.ones(pts.shape[0], dtype=bool)
-            )
-            pts_y_idx_lower = (
-                (pts[:, 1] > self._cfg_general.y_min)
-                if self._cfg_general.y_min is not None
-                else np.ones(pts.shape[0], dtype=bool)
-            )
-            self.pcd = self.pcd.select_by_index(
-                np.where(
-                    np.vstack(
-                        (
-                            pts_x_idx_lower,
-                            pts_x_idx_upper,
-                            pts_y_idx_upper,
-                            pts_y_idx_lower,
-                        )
-                    ).all(axis=0)
-                )[0]
-            )
-
-        # set parameters
-        self._set_map_parameters(self.pcd)
-
-        # get ground height map
-        if self._cfg_sem.compute_height_map:
-            self.height_map = self._pcd_ground_height_map(self.pcd)
-        else:
-            self.height_map = np.zeros((self._num_x, self._num_y))
-
-        # filter point cloud depending on height
-        self.pcd_filtered = self._pcd_filter()
-
-        # update init flag
-        self._init_done = True
-        print("COST-MAP INIT DONE")
-        return
-
-    def create_costmap(self) -> Tuple[list, list]:
-        assert self._init_done, "cost map not initialized, call pcd_init() first"
-        print("COST-MAP CREATION START")
-
-        # get the loss for each grid cell
-        grid_loss = self._get_grid_loss()
-
-        # make grid loss differentiable
-        grid_loss = self._dense_grid_loss(grid_loss)
-
-        print("COST-MAP CREATION DONE")
-        return [grid_loss, self.pcd_filtered.points, self.height_map], [
-            float(self._start_x),
-            float(self._start_y),
-        ]
-
-    """Helper functions"""
-
-    def _pcd_ground_height_map(self, pcd: o3d.geometry.PointCloud) -> np.ndarray:
-        "Start building height map"
-        # for each grid cell, get the point with the highest z value
-        pts = np.asarray(pcd.points)
-        pts_grid_idx_red, pts_idx = self._get_unqiue_grid_idx(pts)
-
-        # ground height of human constructed things (buildings, bench, etc.) should be equal to the ground height of the surrounding terrain/ street
-        # --> classify the selected points and change depending on the class
-        # get colors
-        color = np.asarray(pcd.colors)[pts_idx] * 255.0
-        # pts to class idx array
-        pts_ground = np.zeros(color.shape[0], dtype=bool)
-        # assign each point to a class
-        color = color.astype(int)
-        for class_name, class_color in self.sem_meta.class_color.items():
-            pts_idx_of_class = (color == class_color).all(axis=1).nonzero()[0]
-            pts_ground[pts_idx_of_class] = self.sem_meta.class_ground[class_name]
-
-        # filter outliers
-        pts_ground_idx = pts_idx[pts_ground]
-        if False:
-            pcd_ground = pcd.select_by_index(pts_ground_idx)
-            _, ind = pcd_ground.remove_radius_outlier(nb_points=5, radius=5 * self._cfg_general.resolution)
-            pts_ground_idx = pts_ground_idx[ind]
-            pts_ground_red = np.zeros(pts_grid_idx_red.shape[0], dtype=bool)
-            pts_ground_red[np.where(pts_ground)[0][ind]] = True
-            pts_ground = pts_ground_red
-
-        # fit kdtree to the points on the ground and assign ground height to all other points based on the nearest neighbor
-        pts_ground_location = pts[pts_ground_idx]
-        ground_kdtree = scipy.spatial.KDTree(pts_ground_location)
-        _, non_ground_neighbor_idx = ground_kdtree.query(pts[pts_idx[~pts_ground]], workers=-1)
-
-        # init height map and assign ground height to all points on the ground
-        height_pts_ground = np.zeros(pts_grid_idx_red.shape[0])
-        height_pts_ground[pts_ground] = pts_ground_location[:, 2]
-        height_pts_ground[~pts_ground] = pts_ground_location[non_ground_neighbor_idx, 2]
-
-        # fill the holes
-        height_map = np.full((self._num_x, self._num_y), np.nan)
-        height_map[pts_grid_idx_red[:, 0], pts_grid_idx_red[:, 1]] = height_pts_ground
-        hole_idx = np.vstack(np.where(np.isnan(height_map))).T
-
-        kdtree_grid = scipy.spatial.KDTree(pts_grid_idx_red)
-        distance, neighbor_idx = kdtree_grid.query(hole_idx, k=3, workers=-1)
-        weights = distance / np.sum(distance, axis=1)[:, None]
-        height_map[hole_idx[:, 0], hole_idx[:, 1]] = np.sum(height_pts_ground[neighbor_idx] * weights, axis=1)
-
-        if self.visualize:
-            # visualize the height map
-            plt.imshow(height_map)
-            plt.colorbar()
-            plt.show()
-
-        print("Done building height map")
-        return height_map
-
-    def _pcd_filter(self) -> o3d.geometry.PointCloud:
-        """remove points above the robot height, under the ground and filter for outliers"""
-        pts = np.asarray(self.pcd.points)
-
-        if self.height_map is not None:
-            pts_grid_idx = (
-                np.round((pts[:, :2] - np.array([self._start_x, self._start_y])) / self._cfg_general.resolution)
-            ).astype(int)
-            pts[:, 2] -= self.height_map[pts_grid_idx[:, 0], pts_grid_idx[:, 1]]
-
-        pts_ceil_idx = pts[:, 2] < self._cfg_sem.robot_height * self._cfg_sem.robot_height_factor
-        pts_ground_idx = (
-            pts[:, 2] > self._cfg_sem.ground_height
-            if self._cfg_sem.ground_height is not None
-            else np.ones(pts.shape[0], dtype=bool)
-        )
-        pcd_height_filtered = self.pcd.select_by_index(
-            np.where(np.vstack((pts_ceil_idx, pts_ground_idx)).all(axis=0))[0]
-        )
-
-        # downsampling
-        if self._cfg_sem.downsample:
-            pcd_height_filtered = pcd_height_filtered.voxel_down_sample(self._cfg_general.resolution)
-            print("Voxel Downsampling applied")
-
-        # remove statistical outliers
-        pcd_filtered, _ = pcd_height_filtered.remove_statistical_outlier(
-            nb_neighbors=self._cfg_sem.nb_neighbors,
-            std_ratio=self._cfg_sem.std_ratio,
-        )
-
-        return pcd_filtered
-
-    def _set_map_parameters(self, pcd: o3d.geometry.PointCloud) -> None:
-        """Define the size and start position of the cost map"""
-        pts = np.asarray(pcd.points)
-        assert pts.shape[0] > 0, "No points received."
-
-        # get max and minimum of cost map
-        max_x, max_y, _ = np.amax(pts, axis=0) + self._cfg_general.clear_dist
-        min_x, min_y, _ = np.amin(pts, axis=0) - self._cfg_general.clear_dist
-
-        prev_param = (
-            self._num_x,
-            self._num_y,
-            round(self._start_x, 3),
-            round(self._start_y, 3),
-        )
-        self._num_x = np.ceil((max_x - min_x) / self._cfg_general.resolution / 10).astype(int) * 10
-        self._num_y = np.ceil((max_y - min_y) / self._cfg_general.resolution / 10).astype(int) * 10
-        self._start_x = (max_x + min_x) / 2.0 - self._num_x / 2.0 * self._cfg_general.resolution
-        self._start_y = (max_y + min_y) / 2.0 - self._num_y / 2.0 * self._cfg_general.resolution
-
-        print(f"cost map size set to: {self._num_x} x {self._num_y}")
-        if prev_param != (
-            self._num_x,
-            self._num_y,
-            round(self._start_x, 3),
-            round(self._start_y, 3),
-        ):
-            print("Map parameters changed!")
-            return True
-
-        return False
-
-    def _class_mapping(self) -> np.ndarray:
-        # get colors
-        color = np.asarray(self.pcd_filtered.colors) * 255.0
-
-        # pts to class idx array
-        pts_class_idx = np.ones(color.shape[0], dtype=int) * -1
-
-        # assign each point to a class
-        color = color.astype(int)
-        for class_idx, class_color in enumerate(self.sem_meta.colors):
-            pts_idx_of_class = (color == class_color).all(axis=1).nonzero()[0]
-            pts_class_idx[pts_idx_of_class] = class_idx
-
-        # identify points with unknown classes --> remove from point cloud
-        known_idx = np.where(pts_class_idx != -1)[0]
-        self.pcd_filtered = self.pcd_filtered.select_by_index(known_idx)
-        print(f"Class of {len(known_idx)} points identified" f" ({len(known_idx) / len(color)} %).")
-
-        return pts_class_idx[known_idx]
-
-    @staticmethod
-    def _smoother(
-        pts_idx: np.ndarray,
-        pts_grid: np.ndarray,
-        pts_loss: np.ndarray,
-        conv_crit: float,
-        nb_neigh: int,
-        change_decimal: int,
-        max_iterations: int,
-    ) -> np.ndarray:
-        # get grid idx for each point
-        print(f"Process {mp.current_process().name} started")
-
-        lock.acquire()  # do not access the same memort twice
-        pts_loss_local = pts_loss[pts_idx].copy()
-        pts_grid_local = pts_grid[pts_idx].copy()
-        lock.release()
-
-        print(f"Process {mp.current_process().name} data loaded")
-
-        # fit kd-tree to available points
-        kd_tree = scipy.spatial.KDTree(pts_grid_local)
-        pt_dist, pt_neigh_idx = kd_tree.query(pts_grid_local, k=nb_neigh + 1)
-        pt_dist = pt_dist[:, 1:]  # filter the point itself
-        pt_neigh_idx = pt_neigh_idx[:, 1:]  # filter the point itself
-
-        # turn distance into weight
-        # pt_dist_weighted = pt_dist * np.linspace(1, 0.01, nb_neigh)
-        pt_dist_inv = 1.0 / pt_dist
-        pt_dist_inv[
-            ~np.isfinite(pt_dist_inv)
-        ] = 0.0  # set inf to 0 (inf or nan values when closest point at the same position)
-        pt_weights = scipy.special.softmax(pt_dist_inv, axis=1)
-
-        # smooth losses
-        counter = 0
-        pts_loss_smooth = pts_loss_local.copy()
-        while counter < max_iterations:
-            counter += 1
-            pts_loss_smooth = np.sum(pts_loss_smooth[pt_neigh_idx] * pt_weights, axis=1)
-
-            conv_rate = (
-                np.sum(np.round(pts_loss_smooth, change_decimal) != np.round(pts_loss_local, change_decimal))
-                / pts_loss_local.shape[0]
-            )
-
-            if conv_rate > conv_crit:
-                print(
-                    f"Process {mp.current_process().name} converged with"
-                    f" {np.round(conv_rate * 100, decimals=2)} % of changed"
-                    f" points after {counter} iterations."
-                )
-                break
-
-        return pts_loss_smooth
-
-    @staticmethod
-    def _smoother_init(l_local: mp.Lock) -> None:
-        global lock
-        lock = l_local
-        return
-
-    def _get_grid_loss(self) -> np.ndarray:
-        """convert points to grid"""
-        # get class mapping --> execute first because pcd are filtered
-        class_idx = self._class_mapping()
-
-        # update map parameters --> has to be done after mapping because last step where points are removed
-        changed = self._set_map_parameters(self.pcd_filtered)
-        if changed and self._cfg_sem.compute_height_map:
-            print("Recompute heightmap map due to changed parameters")
-            self.height_map = self._pcd_ground_height_map(self.pcd_filtered)
-        elif changed:
-            self.height_map = np.zeros((self._num_x, self._num_y))
-
-        # get points
-        pts = np.asarray(self.pcd_filtered.points)
-        pts_grid = (pts[:, :2] - np.array([self._start_x, self._start_y])) / self._cfg_general.resolution
-
-        # get loss for each point
-        pts_loss = np.zeros(class_idx.shape[0])
-        for sem_class in range(len(self.sem_meta.losses)):
-            pts_loss[class_idx == sem_class] = self.sem_meta.losses[sem_class]
-
-        # split task index
-        num_tasks = self._cfg_sem.nb_tasks if self._cfg_sem.nb_tasks else mp.cpu_count()
-        pts_task_idx = np.array_split(np.random.permutation(pts_loss.shape[0]), num_tasks)
-
-        # create pool with lock
-        lock_local = mp.Lock()
-        pool = mp.pool.Pool(processes=num_tasks, initializer=self._smoother_init, initargs=(lock_local,))
-        loss_array = pool.map(
-            partial(
-                self._smoother,
-                pts_grid=pts_grid,
-                pts_loss=pts_loss,
-                conv_crit=self._cfg_sem.conv_crit,
-                nb_neigh=self._cfg_sem.nb_neigh,
-                change_decimal=self._cfg_sem.change_decimal,
-                max_iterations=self._cfg_sem.max_iterations,
-            ),
-            pts_task_idx,
-        )
-        pool.close()
-        pool.join()
-
-        # reassemble loss array
-        smooth_loss = np.zeros_like(pts_loss)
-        for process_idx in range(num_tasks):
-            smooth_loss[pts_task_idx[process_idx]] = loss_array[process_idx]
-
-        if False:  # self.visualize:
-            plt.scatter(pts[:, 0], pts[:, 1], c=smooth_loss, cmap="jet")
-            plt.show()
-
-        return smooth_loss
-
-    def _distance_based_gradient(
-        self,
-        loss_level_idx: np.ndarray,
-        loss_min: float,
-        loss_max: float,
-        log_scaling: bool,
-    ) -> np.ndarray:
-        grid = np.zeros((self._num_x, self._num_y))
-
-        # distance transform
-        grid[loss_level_idx] = 1
-        grid = scipy.ndimage.distance_transform_edt(grid)
-
-        # loss scaling
-        if log_scaling:
-            grid[grid > 0.0] = np.log(grid[grid > 0.0] + math.e)
-        else:
-            grid = (grid - np.min(grid)) / (np.max(grid) - np.min(grid))
-            grid = grid * (loss_max - loss_min) + loss_min
-
-        return grid[loss_level_idx]
-
-    def _dense_grid_loss(self, smooth_loss: np.ndarray) -> None:
-        # get grid idx of all classified points
-        pts = np.asarray(self.pcd_filtered.points)
-        pts_grid_idx_red, pts_idx = self._get_unqiue_grid_idx(pts)
-
-        grid_loss = np.ones((self._num_x, self._num_y)) * -10
-        grid_loss[pts_grid_idx_red[:, 0], pts_grid_idx_red[:, 1]] = smooth_loss[pts_idx]
-
-        # get grid idx of all (non-) classified points
-        non_classified_idx = np.where(grid_loss == -10)
-        non_classified_idx = np.vstack((non_classified_idx[0], non_classified_idx[1])).T
-
-        kdtree = scipy.spatial.KDTree(pts_grid_idx_red)
-        distances, idx = kdtree.query(non_classified_idx, k=1)
-
-        # only use points within the mesh, i.e. distance to nearest neighbor smaller than 10 cells
-        within_mesh = distances < 10
-
-        # assign each point its neighbor loss
-        grid_loss[
-            non_classified_idx[within_mesh, 0],
-            non_classified_idx[within_mesh, 1],
-        ] = grid_loss[
-            pts_grid_idx_red[idx[within_mesh], 0],
-            pts_grid_idx_red[idx[within_mesh], 1],
-        ]
-
-        # apply smoothing for filter missclassified points
-        grid_loss[
-            non_classified_idx[~within_mesh, 0],
-            non_classified_idx[~within_mesh, 1],
-        ] = OBSTACLE_LOSS
-        grid_loss = scipy.ndimage.gaussian_filter(grid_loss, sigma=self._cfg_sem.sigma_smooth)
-
-        # get different loss levels
-        loss_levels = np.unique(self.sem_meta.losses)
-        assert round(loss_levels[0], 3) == 0.0, f"Lowest loss level should be 0.0, instead found {loss_levels[0]}."
-        if round(loss_levels[-1], 3) == 1.0:
-            print("WARNING: Highest loss level should be 1.0, instead found" f" {loss_levels[-1]}.")
-
-        # intended traversable area is best traversed with maximum distance to any area with higher cost
-        # apply distance transform to nearest obstacle to enforce smallest loss when distance is max
-        traversable_idx = np.where(
-            np.round(grid_loss, decimals=self._cfg_sem.round_decimal_traversable) == loss_levels[0]
-        )
-        grid_loss[traversable_idx] = (
-            self._distance_based_gradient(
-                traversable_idx,
-                loss_levels[0],
-                abs(self._cfg_sem.negative_reward),
-                False,
-            )
-            * -1
-        )
-
-        # outside of the mesh is an obstacle and all points over obstacle threshold of grid loss are obstacles
-        obs_within_mesh_idx = np.where(grid_loss > self._cfg_sem.obstacle_threshold * loss_levels[-1])
-        obs_idx = (
-            np.hstack((obs_within_mesh_idx[0], non_classified_idx[~within_mesh, 0])),
-            np.hstack((obs_within_mesh_idx[1], non_classified_idx[~within_mesh, 1])),
-        )
-        grid_loss[obs_idx] = self._distance_based_gradient(obs_idx, None, None, True)
-
-        # repeat distance transform for intermediate loss levels
-        for i in range(1, len(loss_levels) - 1):
-            loss_level_idx = np.where(
-                np.round(grid_loss, decimals=self._cfg_sem.round_decimal_traversable) == loss_levels[i]
-            )
-            grid_loss[loss_level_idx] = self._distance_based_gradient(
-                loss_level_idx, loss_levels[i], loss_levels[i + 1], False
-            )
-
-        assert not (grid_loss == -10).any(), "There are still grid cells without a loss value."
-
-        # elevate grid_loss to avoid negative values due to negative reward in area with smallest loss level
-        if np.min(grid_loss) < 0:
-            grid_loss = grid_loss + np.abs(np.min(grid_loss))
-
-        # smooth loss again
-        loss_smooth = scipy.ndimage.gaussian_filter(grid_loss, sigma=self._cfg_general.sigma_smooth)
-
-        # plot grid classes and losses
-        if self.visualize:
-            fig, axs = plt.subplots(2, 2)
-            axs[0, 0].set_title("grid loss")
-            axs[0, 0].imshow(grid_loss, cmap="jet")
-            axs[0, 1].set_title("loss smooth")
-            axs[0, 1].imshow(loss_smooth, cmap="jet")
-            axs[1, 0].set_title("grid loss x-grad")
-            axs[1, 0].imshow(
-                np.log(np.abs(scipy.ndimage.sobel(grid_loss, axis=0, mode="constant")) + math.e) - 1,
-                cmap="jet",
-            )
-            axs[1, 1].set_title("grid loss y-grad")
-            axs[1, 1].imshow(
-                np.log(np.abs(scipy.ndimage.sobel(grid_loss, axis=1, mode="constant")) + math.e) - 1,
-                cmap="jet",
-            )
-            plt.show()
-
-        return loss_smooth
-
-    def _get_unqiue_grid_idx(self, pts):
-        """
-        Will select the points that are unique in their grid position and have the highest z location
-        """
-        pts_grid_idx = (
-            np.round((pts[:, :2] - np.array([self._start_x, self._start_y])) / self._cfg_general.resolution)
-        ).astype(int)
-
-        # convert pts_grid_idx to 1d array
-        pts_grid_idx_1d = pts_grid_idx[:, 0] * self._num_x + pts_grid_idx[:, 1]
-
-        # get index of all points mapped to the same grid location --> take highest value to avoid local minima in e.g. cars
-        # following solution given at: https://stackoverflow.com/questions/30003068/how-to-get-a-list-of-all-indices-of-repeated-elements-in-a-numpy-array
-        # creates an array of indices, sorted by unique element
-        idx_sort = np.argsort(pts_grid_idx_1d)
-        # sorts pts_grid_idx_1d so all unique elements are together
-        pts_grid_idx_1d_sorted = pts_grid_idx_1d[idx_sort]
-        # returns the unique values, the index of the first occurrence of a value, and the count for each element
-        vals, idx_start, count = np.unique(pts_grid_idx_1d_sorted, return_counts=True, return_index=True)
-        # splits the indices into separate arrays
-        pts_grid_location_map = np.split(idx_sort, idx_start[1:])
-
-        # filter for points with more than one occurrence
-        pts_grid_location_map = np.array(pts_grid_location_map, dtype=object)
-        pts_grid_location_map_multiple = pts_grid_location_map[count > 1]
-
-        # get index with maximum z value for all points mapped to the same grid location
-        pts_grid_location_map_multiple_idx = np.array(
-            [
-                pts_grid_location_map_multiple[idx][np.argmax(pts[pts_idx, 2])]
-                for idx, pts_idx in enumerate(pts_grid_location_map_multiple)
-            ]
-        )
-
-        # combine indices to get for every grid location the index of the point with the highest z value
-        grid_idx = np.zeros(len(pts_grid_location_map), dtype=int)
-        grid_idx[count > 1] = pts_grid_location_map_multiple_idx
-        grid_idx[count == 1] = pts_grid_location_map[count == 1]
-        pts_grid_idx_red = pts_grid_idx[grid_idx]
-
-        return pts_grid_idx_red, grid_idx
-
-
-# EoF
diff --git a/viplanner/cost_maps/tsdf_cost_map.py b/viplanner/cost_maps/tsdf_cost_map.py
deleted file mode 100644
index b8bf41c..0000000
--- a/viplanner/cost_maps/tsdf_cost_map.py
+++ /dev/null
@@ -1,187 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import math
-
-# python
-import os
-
-import numpy as np
-import open3d as o3d
-from scipy import ndimage
-from scipy.ndimage import gaussian_filter
-
-# imperative-cost-map
-from viplanner.config import GeneralCostMapConfig, TsdfCostMapConfig
-
-
-class TsdfCostMap:
-    """
-    Cost Map based on geometric information
-    """
-
-    def __init__(self, cfg_general: GeneralCostMapConfig, cfg_tsdf: TsdfCostMapConfig):
-        self._cfg_general = cfg_general
-        self._cfg_tsdf = cfg_tsdf
-        # set init flag
-        self.is_map_ready = False
-        # init point clouds
-        self.obs_pcd = o3d.geometry.PointCloud()
-        self.free_pcd = o3d.geometry.PointCloud()
-        return
-
-    def UpdatePCDwithPs(self, P_obs, P_free, is_downsample=False):
-        self.obs_pcd.points = o3d.utility.Vector3dVector(P_obs)
-        self.free_pcd.points = o3d.utility.Vector3dVector(P_free)
-        if is_downsample:
-            self.obs_pcd = self.obs_pcd.voxel_down_sample(self._cfg_general.resolution)
-            self.free_pcd = self.free_pcd.voxel_down_sample(self._cfg_general.resolution * 0.85)
-
-        self.obs_points = np.asarray(self.obs_pcd.points)
-        self.free_points = np.asarray(self.free_pcd.points)
-        print("number of obs points: %d, free points: %d" % (self.obs_points.shape[0], self.free_points.shape[0]))
-
-    def ReadPointFromFile(self):
-        pcd_load = o3d.io.read_point_cloud(os.path.join(self._cfg_general.root_path, self._cfg_general.ply_file))
-        obs_p, free_p = self.TerrainAnalysis(np.asarray(pcd_load.points))
-        self.UpdatePCDwithPs(obs_p, free_p, is_downsample=True)
-        if self._cfg_tsdf.filter_outliers:
-            obs_p = self.FilterCloud(self.obs_points)
-            free_p = self.FilterCloud(self.free_points, outlier_filter=False)
-            self.UpdatePCDwithPs(obs_p, free_p)
-        self.UpdateMapParams()
-        return
-
-    def TerrainAnalysis(self, input_points):
-        obs_points = np.zeros(input_points.shape)
-        free_poins = np.zeros(input_points.shape)
-        obs_idx = 0
-        free_idx = 0
-        # naive approach with z values
-        for p in input_points:
-            p_height = p[2] + self._cfg_tsdf.offset_z
-            if (p_height > self._cfg_tsdf.ground_height * 1.2) and (
-                p_height < self._cfg_tsdf.robot_height * self._cfg_tsdf.robot_height_factor
-            ):  # remove ground and ceiling
-                obs_points[obs_idx, :] = p
-                obs_idx = obs_idx + 1
-            elif p_height < self._cfg_tsdf.ground_height and p_height > -self._cfg_tsdf.ground_height:
-                free_poins[free_idx, :] = p
-                free_idx = free_idx + 1
-        return obs_points[:obs_idx, :], free_poins[:free_idx, :]
-
-    def UpdateMapParams(self):
-        if self.obs_points.shape[0] == 0:
-            print("No points received.")
-            return
-        max_x, max_y, _ = np.amax(self.obs_points, axis=0) + self._cfg_general.clear_dist
-        min_x, min_y, _ = np.amin(self.obs_points, axis=0) - self._cfg_general.clear_dist
-
-        self.num_x = np.ceil((max_x - min_x) / self._cfg_general.resolution / 10).astype(int) * 10
-        self.num_y = np.ceil((max_y - min_y) / self._cfg_general.resolution / 10).astype(int) * 10
-        self.start_x = (max_x + min_x) / 2.0 - self.num_x / 2.0 * self._cfg_general.resolution
-        self.start_y = (max_y + min_y) / 2.0 - self.num_y / 2.0 * self._cfg_general.resolution
-
-        print("tsdf map initialized, with size: %d, %d" % (self.num_x, self.num_y))
-        self.is_map_ready = True
-
-    def CreateTSDFMap(self):
-        if not self.is_map_ready:
-            raise ValueError("create tsdf map fails, no points received.")
-        free_map = np.ones([self.num_x, self.num_y])
-        obs_map = np.zeros([self.num_x, self.num_y])
-        free_I = self.IndexArrayOfPs(self.free_points)
-        obs_I = self.IndexArrayOfPs(self.obs_points)
-        # create free place map
-        for i in obs_I:
-            obs_map[i[0], i[1]] = 1.0
-        obs_map = gaussian_filter(obs_map, sigma=self._cfg_tsdf.sigma_expand)
-        for i in free_I:
-            if i[0] < self.num_x and i[1] < self.num_y:
-                free_map[i[0], i[1]] = 0
-        free_map = gaussian_filter(free_map, sigma=self._cfg_tsdf.sigma_expand)
-        free_map[free_map < self._cfg_tsdf.free_space_threshold] = 0
-        # assign obstacles
-        free_map[obs_map > self._cfg_tsdf.obstacle_threshold] = 1.0
-
-        print("occupancy map generation completed.")
-        # Distance Transform
-        tsdf_array = ndimage.distance_transform_edt(free_map)
-        tsdf_array[tsdf_array > 0.0] = np.log(tsdf_array[tsdf_array > 0.0] + math.e)
-        tsdf_array = gaussian_filter(tsdf_array, sigma=self._cfg_general.sigma_smooth)
-
-        viz_points = np.concatenate((self.obs_points, self.free_points), axis=0)
-
-        # TODO: Using true terrain analysis module
-        ground_array = np.ones([self.num_x, self.num_y]) * 0.0
-        return [tsdf_array, viz_points, ground_array], [
-            float(self.start_x),
-            float(self.start_y),
-        ]
-
-    def IndexArrayOfPs(self, points):
-        indexes = points[:, :2] - np.array([self.start_x, self.start_y])
-        indexes = (np.round(indexes / self._cfg_general.resolution)).astype(int)
-        return indexes
-
-    def FilterCloud(self, points, outlier_filter=True):
-        # crop points
-        if any(
-            [
-                self._cfg_general.x_max,
-                self._cfg_general.x_min,
-                self._cfg_general.y_max,
-                self._cfg_general.y_min,
-            ]
-        ):
-            points_x_idx_upper = (
-                (points[:, 0] < self._cfg_general.x_max)
-                if self._cfg_general.x_max is not None
-                else np.ones(points.shape[0], dtype=bool)
-            )
-            points_x_idx_lower = (
-                (points[:, 0] > self._cfg_general.x_min)
-                if self._cfg_general.x_min is not None
-                else np.ones(points.shape[0], dtype=bool)
-            )
-            points_y_idx_upper = (
-                (points[:, 1] < self._cfg_general.y_max)
-                if self._cfg_general.y_max is not None
-                else np.ones(points.shape[0], dtype=bool)
-            )
-            points_y_idx_lower = (
-                (points[:, 1] > self._cfg_general.y_min)
-                if self._cfg_general.y_min is not None
-                else np.ones(points.shape[0], dtype=bool)
-            )
-            points = points[
-                np.vstack(
-                    (
-                        points_x_idx_lower,
-                        points_x_idx_upper,
-                        points_y_idx_upper,
-                        points_y_idx_lower,
-                    )
-                ).all(axis=0)
-            ]
-
-        if outlier_filter:
-            # Filter outlier in points
-            pcd = o3d.geometry.PointCloud()
-            pcd.points = o3d.utility.Vector3dVector(points)
-            cl, _ = pcd.remove_statistical_outlier(
-                nb_neighbors=self._cfg_tsdf.nb_neighbors,
-                std_ratio=self._cfg_tsdf.std_ratio,
-            )
-            points = np.asarray(cl.points)
-
-        return points
-
-    def VizCloud(self, pcd):
-        o3d.visualization.draw_geometries([pcd])  # visualize point cloud
-
-
-# EoF
diff --git a/viplanner/depth_reconstruct.py b/viplanner/depth_reconstruct.py
deleted file mode 100644
index 22c149c..0000000
--- a/viplanner/depth_reconstruct.py
+++ /dev/null
@@ -1,371 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# python
-import os
-
-import cv2
-import numpy as np
-import open3d as o3d
-import scipy.spatial.transform as tf
-from tqdm import tqdm
-
-# imperative-cost-map
-from viplanner.config import ReconstructionCfg, VIPlannerSemMetaHandler
-
-
-class DepthReconstruction:
-    """
-    Reconstruct 3D Map with depth images, assumes the ground truth camera odom is known
-    Config parameters can be set in ReconstructionCfg
-
-    Expects following datastructure:
-
-    - env_name
-        - camera_extrinsic.txt  (format: x y z qx qy qz qw)
-        - intrinsics.txt (expects ROS CameraInfo format --> P-Matrix)
-        - depth  (either png and/ or npy)
-            - xxxx.png  (images should be named with 4 digits, e.g. 0000.png, 0001.png, etc.)
-            - xxxx.npy  (arrays should be named with 4 digits, e.g. 0000.npy, 0001.npy, etc.)
-        - semantics (optional)
-            - xxxx.png  (images should be named with 4 digits, e.g. 0000.png, 0001.png, etc., RGB images)
-
-    when both depth and semantic images are available, then define sem_suffic and depth_suffix in ReconstructionCfg to differentiate between the two with the following structure:
-
-    - env_name
-        - camera_extrinsic{depth_suffix}.txt  (format: x y z qx qy qz qw)
-        - camera_extrinsic{sem_suffix}.txt  (format: x y z qx qy qz qw)
-        - intrinsics.txt (expects ROS CameraInfo format --> P-Matrix) (contains both intrinsics for depth and semantic images)
-        - depth (either png and/ or npy)
-            - xxxx{depth_suffix}.png  (images should be named with 4 digits, e.g. 0000.png, 0001.png, etc.)
-            - xxxx{depth_suffix}.npy  (arrays should be named with 4 digits, e.g. 0000.npy, 0001.npy, etc.)
-        - semantics (optional)
-            - xxxx{sem_suffix}.png  (images should be named with 4 digits, e.g. 0000.png, 0001.png, etc., RGB Images)
-
-    in the case of high resolution depth images for the reconstruction, the following additional directory is expected:
-        - depth_high_res (either png and/ or npy)
-            - xxxx{depth_suffix}.png  (images should be named with 4 digits, e.g. 0000.png, 0001.png, etc.)
-            - xxxx{depth_suffix}.npy  (arrays should be named with 4 digits, e.g. 0000.npy, 0001.npy, etc.)
-    """
-
-    debug = False
-
-    def __init__(self, cfg: ReconstructionCfg):
-        # get config
-        self._cfg: ReconstructionCfg = cfg
-        # read camera params and odom
-        self.K_depth: np.ndarray = None
-        self.K_sem: np.ndarray = None
-        self._read_intrinsic()
-        self.extrinsics_depth: np.ndarray = None
-        self.extrinsics_sem: np.ndarray = None
-        self._read_extrinsic()
-        # semantic classes for viplanner
-        self.sem_handler = VIPlannerSemMetaHandler()
-        # control flag if point-cloud has been loaded
-        self._is_constructed = False
-
-        # variables
-        self._pcd: o3d.geometry.PointCloud = None
-
-        print("Ready to read depth data.")
-
-    # public methods
-    def depth_reconstruction(self):
-        # identify start and end image idx for the reconstruction
-        N = len(self.extrinsics_depth)
-        if self._cfg.max_images:
-            self._start_idx = self._cfg.start_idx
-            self._end_idx = min(self._cfg.start_idx + self._cfg.max_images, N)
-        else:
-            self._start_idx = 0
-            self._end_idx = N
-
-        if self._cfg.point_cloud_batch_size > self._end_idx - self._start_idx:
-            print(
-                "[WARNING] batch size must be smaller or equal than number of"
-                " images to reconstruct, now set to max value"
-                f" {self._end_idx - self._start_idx}"
-            )
-            self._cfg.point_cloud_batch_size = self._end_idx - self._start_idx
-
-        print("total number of images for reconstruction:" f" {int(self._end_idx - self._start_idx)}")
-
-        # get pixel tensor for reprojection
-        pixels = self._computePixelTensor()
-
-        # init point-cloud
-        self._pcd = o3d.geometry.PointCloud()  # point size (n, 3)
-        first_batch = True
-
-        # init lists
-        points_all = []
-        if self._cfg.semantics:
-            sem_map_all = []
-
-        for img_counter, img_idx in enumerate(
-            tqdm(
-                range(self._end_idx - self._start_idx),
-                desc="Reconstructing 3D Points",
-            )
-        ):
-            im = self._load_depth_image(img_idx)
-            extrinsics = self.extrinsics_depth[img_idx + self._start_idx]
-
-            # project points in world frame
-            rot = tf.Rotation.from_quat(extrinsics[3:]).as_matrix()
-            points = im.reshape(-1, 1) * (rot @ pixels.T).T
-            # filter points with 0 depth --> otherwise obstacles at camera position
-            non_zero_idx = np.where(points.any(axis=1))[0]
-
-            points_final = points[non_zero_idx] + extrinsics[:3]
-
-            if self._cfg.semantics and self._cfg.high_res_depth:
-                img_path = os.path.join(
-                    self._cfg.get_data_path(),
-                    "semantics",
-                    str(self._start_idx + img_idx).zfill(4) + self._cfg.sem_suffix + ".png",
-                )
-                sem_image = cv2.imread(img_path)  # load in BGR format
-                sem_image = cv2.cvtColor(sem_image, cv2.COLOR_BGR2RGB)
-                sem_points = sem_image.reshape(-1, 3)[non_zero_idx]
-                points_all.append(points_final)
-                sem_map_all.append(sem_points)
-            elif self._cfg.semantics:
-                sem_annotation, filter_idx = self._get_semantic_image(points_final, img_idx)
-                points_all.append(points_final[filter_idx])
-                sem_map_all.append(sem_annotation)
-            else:
-                points_all.append(points_final)
-
-            # update point cloud
-            if img_counter % self._cfg.point_cloud_batch_size == 0:
-                print("updating open3d geometry point cloud with" f" {self._cfg.point_cloud_batch_size} images ...")
-
-                if first_batch:
-                    self._pcd.points = o3d.utility.Vector3dVector(np.vstack(points_all))
-                    if self._cfg.semantics:
-                        self._pcd.colors = o3d.utility.Vector3dVector(np.vstack(sem_map_all) / 255.0)
-                    first_batch = False
-
-                else:
-                    self._pcd.points.extend(np.vstack(points_all))
-                    if self._cfg.semantics:
-                        self._pcd.colors.extend(np.vstack(sem_map_all) / 255.0)
-
-                # reset buffer lists
-                del points_all
-                points_all = []
-                if self._cfg.semantics:
-                    del sem_map_all
-                    sem_map_all = []
-
-                # apply downsampling
-                print("downsampling point cloud with voxel size" f" {self._cfg.voxel_size} ...")
-                self._pcd = self._pcd.voxel_down_sample(self._cfg.voxel_size)
-
-        # add last batch
-        if len(points_all) > 0:
-            print("updating open3d geometry point cloud with last images ...")
-            self._pcd.points.extend(np.vstack(points_all))
-            points_all = None
-            if self._cfg.semantics:
-                self._pcd.colors.extend(np.vstack(sem_map_all) / 255.0)
-                sem_map_all = None
-
-            # apply downsampling
-            print(f"downsampling point cloud with voxel size {self._cfg.voxel_size} ...")
-            self._pcd = self._pcd.voxel_down_sample(self._cfg.voxel_size)
-
-        # update flag
-        self._is_constructed = True
-        print("construction completed.")
-
-        return
-
-    def show_pcd(self):
-        if not self._is_constructed:
-            print("no reconstructed cloud")
-            return
-        origin = o3d.geometry.TriangleMesh.create_coordinate_frame(
-            size=1.0, origin=np.min(np.asarray(self._pcd.points), axis=0)
-        )
-        o3d.visualization.draw_geometries([self._pcd, origin], mesh_show_wireframe=True)  # visualize point cloud
-        return
-
-    def save_pcd(self):
-        if not self._is_constructed:
-            print("save points failed, no reconstructed cloud!")
-
-        print("save output files to: " + os.path.join(self._cfg.data_dir, self._cfg.env))
-
-        # pre-create the folder for the mapping
-        os.makedirs(
-            os.path.join(
-                os.path.join(self._cfg.data_dir, self._cfg.env),
-                "maps",
-                "cloud",
-            ),
-            exist_ok=True,
-        )
-        os.makedirs(
-            os.path.join(os.path.join(self._cfg.data_dir, self._cfg.env), "maps", "data"),
-            exist_ok=True,
-        )
-        os.makedirs(
-            os.path.join(
-                os.path.join(self._cfg.data_dir, self._cfg.env),
-                "maps",
-                "params",
-            ),
-            exist_ok=True,
-        )
-
-        # save clouds
-        o3d.io.write_point_cloud(
-            os.path.join(self._cfg.data_dir, self._cfg.env, "cloud.ply"),
-            self._pcd,
-        )  # save point cloud
-
-        print("saved point cloud to ply file.")
-
-    @property
-    def pcd(self):
-        return self._pcd
-
-    """helper functions"""
-
-    def _read_extrinsic(self) -> None:
-        if self._cfg.semantics:
-            extrinsic_path = os.path.join(
-                self._cfg.get_data_path(),
-                "camera_extrinsic" + self._cfg.sem_suffix + ".txt",
-            )
-            self.extrinsics_sem = np.loadtxt(extrinsic_path, delimiter=",")
-        if self._cfg.high_res_depth:
-            assert self._cfg.semantics, (
-                "high res depth requires semantic images since depth should be" " recorded with semantic camera"
-            )
-            self.extrinsics_depth = self.extrinsics_sem
-        else:
-            extrinsic_path = os.path.join(
-                self._cfg.get_data_path(),
-                "camera_extrinsic" + self._cfg.depth_suffix + ".txt",
-            )
-            self.extrinsics_depth = np.loadtxt(extrinsic_path, delimiter=",")
-        return
-
-    def _read_intrinsic(self) -> None:
-        intrinsic_path = os.path.join(self._cfg.get_data_path(), "intrinsics.txt")
-        P = np.loadtxt(intrinsic_path, delimiter=",")  # assumes ROS P matrix
-        self._intrinsic = list(P)
-        if self._cfg.semantics:
-            self.K_depth = P[0].reshape(3, 4)[:3, :3]
-            self.K_sem = P[1].reshape(3, 4)[:3, :3]
-        else:
-            self.K_depth = P.reshape(3, 4)[:3, :3]
-
-        if self._cfg.high_res_depth:
-            self.K_depth = self.K_sem
-        return
-
-    def _load_depth_image(self, idx: int) -> np.ndarray:
-        # get path to images
-        if self._cfg.high_res_depth:
-            dir_path = os.path.join(self._cfg.get_data_path(), "depth_high_res")
-        else:
-            dir_path = os.path.join(self._cfg.get_data_path(), "depth")
-
-        if os.path.isfile(
-            os.path.join(
-                dir_path,
-                str(idx + self._start_idx).zfill(4) + self._cfg.depth_suffix + ".npy",
-            )
-        ):
-            img_array = (
-                np.load(
-                    os.path.join(
-                        dir_path,
-                        str(idx + self._start_idx).zfill(4) + self._cfg.depth_suffix + ".npy",
-                    )
-                )
-                / self._cfg.depth_scale
-            )
-        else:
-            img_path = os.path.join(
-                dir_path,
-                str(idx + self._start_idx).zfill(4) + self._cfg.depth_suffix + ".png",
-            )
-            img_array = cv2.imread(img_path, cv2.IMREAD_ANYDEPTH) / self._cfg.depth_scale
-
-        img_array[~np.isfinite(img_array)] = 0
-        return img_array
-
-    def _computePixelTensor(self):
-        depth_img = self._load_depth_image(0)
-
-        # get image plane mesh grid
-        pix_u = np.arange(0, depth_img.shape[1])
-        pix_v = np.arange(0, depth_img.shape[0])
-        grid = np.meshgrid(pix_u, pix_v)
-        pixels = np.vstack(list(map(np.ravel, grid))).T
-        pixels = np.hstack([pixels, np.ones((len(pixels), 1))])  # add ones for 3D coordinates
-
-        # transform to camera frame
-        k_inv = np.linalg.inv(self.K_depth)
-        pix_cam_frame = np.matmul(k_inv, pixels.T)
-        # reorder to be in "robotics" axis order (x forward, y left, z up)
-        return pix_cam_frame[[2, 0, 1], :].T * np.array([1, -1, -1])
-
-    def _get_semantic_image(self, points, idx):
-        # load semantic image and pose
-        img_path = os.path.join(
-            self._cfg.get_data_path(),
-            "semantics",
-            str(self._start_idx + idx).zfill(4) + self._cfg.sem_suffix + ".png",
-        )
-        sem_image = cv2.imread(img_path)  # loads in bgr order
-        sem_image = cv2.cvtColor(sem_image, cv2.COLOR_BGR2RGB)
-        pose_sem = self.extrinsics_sem[idx + self._cfg.start_idx]
-        # transform points to semantic camera frame
-        points_sem_cam_frame = (tf.Rotation.from_quat(pose_sem[3:]).as_matrix().T @ (points - pose_sem[:3]).T).T
-        # normalize points
-        points_sem_cam_frame_norm = points_sem_cam_frame / points_sem_cam_frame[:, 0][:, np.newaxis]
-        # reorder points be camera convention (z-forward)
-        points_sem_cam_frame_norm = points_sem_cam_frame_norm[:, [1, 2, 0]] * np.array([-1, -1, 1])
-        # transform points to pixel coordinates
-        pixels = (self.K_sem @ points_sem_cam_frame_norm.T).T
-        # filter points outside of image
-        filter_idx = (
-            (pixels[:, 0] >= 0)
-            & (pixels[:, 0] < sem_image.shape[1])
-            & (pixels[:, 1] >= 0)
-            & (pixels[:, 1] < sem_image.shape[0])
-        )
-        # get semantic annotation
-        sem_annotation = sem_image[
-            pixels[filter_idx, 1].astype(int),
-            pixels[filter_idx, 0].astype(int),
-        ]
-        # remove all pixels that have no semantic annotation
-        non_classified_idx = np.all(sem_annotation == self.sem_handler.class_color["static"], axis=1)
-        sem_annotation = sem_annotation[~non_classified_idx]
-        filter_idx[np.where(filter_idx)[0][non_classified_idx]] = False
-
-        return sem_annotation, filter_idx
-
-
-if __name__ == "__main__":
-    cfg = ReconstructionCfg()
-
-    # start depth reconstruction
-    depth_constructor = DepthReconstruction(cfg)
-    depth_constructor.depth_reconstruction()
-
-    depth_constructor.save_pcd()
-    depth_constructor.show_pcd()
-
-# EoF
diff --git a/viplanner/plannernet/PlannerNet.py b/viplanner/plannernet/PlannerNet.py
deleted file mode 100644
index 39809e2..0000000
--- a/viplanner/plannernet/PlannerNet.py
+++ /dev/null
@@ -1,172 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import torch.nn as nn
-
-
-def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):
-    """3x3 convolution with padding"""
-    return nn.Conv2d(
-        in_planes,
-        out_planes,
-        kernel_size=3,
-        stride=stride,
-        padding=dilation,
-        groups=groups,
-        bias=False,
-        dilation=dilation,
-    )
-
-
-def conv1x1(in_planes, out_planes, stride=1):
-    """1x1 convolution"""
-    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)
-
-
-class BasicBlock(nn.Module):
-    expansion = 1
-
-    def __init__(
-        self,
-        inplanes,
-        planes,
-        stride=1,
-        downsample=None,
-        groups=1,
-        base_width=64,
-        dilation=1,
-    ):
-        super().__init__()
-        if groups != 1 or base_width != 64:
-            raise ValueError("BasicBlock only supports groups=1 and base_width=64")
-        if dilation > 1:
-            raise NotImplementedError("Dilation > 1 not supported in BasicBlock")
-        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
-        self.conv1 = conv3x3(inplanes, planes, stride)
-        self.relu = nn.ReLU(inplace=True)
-        self.conv2 = conv3x3(planes, planes)
-        self.downsample = downsample
-        self.stride = stride
-
-    def forward(self, x):
-        identity = x
-
-        out = self.conv1(x)
-        out = self.relu(out)
-
-        out = self.conv2(out)
-
-        if self.downsample is not None:
-            identity = self.downsample(x)
-
-        out += identity
-        out = self.relu(out)
-
-        return out
-
-
-class PlannerNet(nn.Module):
-    def __init__(
-        self,
-        layers,
-        block=BasicBlock,
-        groups=1,
-        width_per_group=64,
-        replace_stride_with_dilation=None,
-    ) -> None:
-        super().__init__()
-        self.inplanes = 64
-        self.dilation = 1
-
-        if replace_stride_with_dilation is None:
-            # each element in the tuple indicates if we should replace
-            # the 2x2 stride with a dilated convolution instead
-            replace_stride_with_dilation = [False, False, False]
-        if len(replace_stride_with_dilation) != 3:
-            raise ValueError(
-                "replace_stride_with_dilation should be None "
-                "or a 3-element tuple, got {}".format(replace_stride_with_dilation)
-            )
-        self.groups = groups
-        self.base_width = width_per_group
-        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
-        self.relu = nn.ReLU(inplace=True)
-        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
-        self.layer1 = self._make_layer(block, 64, layers[0])
-        self.layer2 = self._make_layer(
-            block,
-            128,
-            layers[1],
-            stride=2,
-            dilate=replace_stride_with_dilation[0],
-        )
-        self.layer3 = self._make_layer(
-            block,
-            256,
-            layers[2],
-            stride=2,
-            dilate=replace_stride_with_dilation[1],
-        )
-        self.layer4 = self._make_layer(
-            block,
-            512,
-            layers[3],
-            stride=2,
-            dilate=replace_stride_with_dilation[2],
-        )
-
-    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):
-        downsample = None
-        previous_dilation = self.dilation
-        if dilate:
-            self.dilation *= stride
-            stride = 1
-        if stride != 1 or self.inplanes != planes * block.expansion:
-            downsample = nn.Sequential(
-                conv1x1(self.inplanes, planes * block.expansion, stride),
-            )
-
-        layers = []
-        layers.append(
-            block(
-                self.inplanes,
-                planes,
-                stride,
-                downsample,
-                self.groups,
-                self.base_width,
-                previous_dilation,
-            )
-        )
-        self.inplanes = planes * block.expansion
-        for _ in range(1, blocks):
-            layers.append(
-                block(
-                    self.inplanes,
-                    planes,
-                    groups=self.groups,
-                    base_width=self.base_width,
-                    dilation=self.dilation,
-                )
-            )
-
-        return nn.Sequential(*layers)
-
-    def _forward_impl(self, x):
-        # See note [TorchScript super()]
-        x = self.conv1(x)
-        x = self.relu(x)
-        x = self.maxpool(x)
-
-        x = self.layer1(x)
-        x = self.layer2(x)
-        x = self.layer3(x)
-        x = self.layer4(x)
-
-        return x
-
-    def forward(self, x):
-        return self._forward_impl(x)
diff --git a/viplanner/plannernet/__init__.py b/viplanner/plannernet/__init__.py
deleted file mode 100644
index 0b62eaa..0000000
--- a/viplanner/plannernet/__init__.py
+++ /dev/null
@@ -1,17 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from .autoencoder import AutoEncoder, DualAutoEncoder
-from .rgb_encoder import PRE_TRAIN_POSSIBLE, get_m2f_cfg
-
-__all__ = [
-    "AutoEncoder",
-    "DualAutoEncoder",
-    "get_m2f_cfg",
-    "PRE_TRAIN_POSSIBLE",
-]
-
-# EoF
diff --git a/viplanner/plannernet/autoencoder.py b/viplanner/plannernet/autoencoder.py
deleted file mode 100644
index 8491438..0000000
--- a/viplanner/plannernet/autoencoder.py
+++ /dev/null
@@ -1,159 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from typing import Optional
-
-import torch
-import torch.nn as nn
-
-from viplanner.config import TrainCfg
-
-# visual-imperative-planner
-from .PlannerNet import PlannerNet
-from .rgb_encoder import PRE_TRAIN_POSSIBLE, RGBEncoder
-
-
-class AutoEncoder(nn.Module):
-    def __init__(self, encoder_channel=64, k=5):
-        super().__init__()
-        self.encoder = PlannerNet(layers=[2, 2, 2, 2])
-        self.decoder = Decoder(512, encoder_channel, k)
-
-    def forward(self, x: torch.Tensor, goal: torch.Tensor):
-        x = x.expand(-1, 3, -1, -1)
-        x = self.encoder(x)
-        x, c = self.decoder(x, goal)
-        return x, c
-
-
-class DualAutoEncoder(nn.Module):
-    def __init__(
-        self,
-        train_cfg: TrainCfg,
-        m2f_cfg=None,
-        weight_path: Optional[str] = None,
-    ):
-        super().__init__()
-        self.encoder_depth = PlannerNet(layers=[2, 2, 2, 2])
-        if train_cfg.rgb and train_cfg.pre_train_sem and PRE_TRAIN_POSSIBLE:
-            self.encoder_sem = RGBEncoder(m2f_cfg, weight_path, freeze=train_cfg.pre_train_freeze)
-        else:
-            self.encoder_sem = PlannerNet(layers=[2, 2, 2, 2])
-
-        if train_cfg.decoder_small:
-            self.decoder = DecoderS(1024, train_cfg.in_channel, train_cfg.knodes)
-        else:
-            self.decoder = Decoder(1024, train_cfg.in_channel, train_cfg.knodes)
-        return
-
-    def forward(self, x_depth: torch.Tensor, x_sem: torch.Tensor, goal: torch.Tensor):
-        # encode depth
-        x_depth = x_depth.expand(-1, 3, -1, -1)
-        x_depth = self.encoder_depth(x_depth)
-        # encode sem
-        x_sem = self.encoder_sem(x_sem)
-        # concat
-        x = torch.cat((x_depth, x_sem), dim=1)  # x.size = (N, 1024, 12, 20)
-        # decode
-        x, c = self.decoder(x, goal)
-        return x, c
-
-
-class Decoder(nn.Module):
-    def __init__(self, in_channels, goal_channels, k=5):
-        super().__init__()
-        self.k = k
-        self.relu = nn.ReLU(inplace=True)
-        self.fg = nn.Linear(3, goal_channels)
-        self.sigmoid = nn.Sigmoid()
-
-        self.conv1 = nn.Conv2d(
-            (in_channels + goal_channels),
-            512,
-            kernel_size=5,
-            stride=1,
-            padding=1,
-        )
-        self.conv2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=0)
-
-        self.fc1 = nn.Linear(256 * 128, 1024)
-        self.fc2 = nn.Linear(1024, 512)
-        self.fc3 = nn.Linear(512, k * 3)
-
-        self.frc1 = nn.Linear(1024, 128)
-        self.frc2 = nn.Linear(128, 1)
-
-    def forward(self, x, goal):
-        # compute goal encoding
-        goal = self.fg(goal[:, 0:3])
-        goal = goal[:, :, None, None].expand(-1, -1, x.shape[2], x.shape[3])
-        # cat x with goal in channel dim
-        x = torch.cat((x, goal), dim=1)
-        # compute x
-        x = self.relu(self.conv1(x))  # size = (N, 512, x.H/32, x.W/32)
-        x = self.relu(self.conv2(x))  # size = (N, 512, x.H/60, x.W/60)
-        x = torch.flatten(x, 1)
-
-        f = self.relu(self.fc1(x))
-
-        x = self.relu(self.fc2(f))
-        x = self.fc3(x)
-        x = x.reshape(-1, self.k, 3)
-
-        c = self.relu(self.frc1(f))
-        c = self.sigmoid(self.frc2(c))
-
-        return x, c
-
-
-class DecoderS(nn.Module):
-    def __init__(self, in_channels, goal_channels, k=5):
-        super().__init__()
-        self.k = k
-        self.relu = nn.ReLU(inplace=True)
-        self.fg = nn.Linear(3, goal_channels)
-        self.sigmoid = nn.Sigmoid()
-
-        self.conv1 = nn.Conv2d(
-            (in_channels + goal_channels),
-            512,
-            kernel_size=5,
-            stride=1,
-            padding=1,
-        )
-        self.conv2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=0)
-        self.conv3 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=0)
-        self.conv4 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=0)
-
-        self.fc1 = nn.Linear(64 * 48, 256)  # --> in that setting 33 million parameters
-        self.fc2 = nn.Linear(256, k * 3)
-
-        self.frc1 = nn.Linear(256, 1)
-
-    def forward(self, x, goal):
-        # compute goal encoding
-        goal = self.fg(goal[:, 0:3])
-        goal = goal[:, :, None, None].expand(-1, -1, x.shape[2], x.shape[3])
-        # cat x with goal in channel dim
-        x = torch.cat((x, goal), dim=1)  # x.size = (N, 1024+16, 12, 20)
-        # compute x
-        x = self.relu(self.conv1(x))  # size = (N, 512, x.H/32, x.W/32)  --> (N, 512, 10, 18),
-        x = self.relu(self.conv2(x))  # size = (N, 512, x.H/60, x.W/60)  --> (N, 256, 8, 16)
-        x = self.relu(self.conv3(x))  # size = (N, 512, x.H/90, x.W/90)  --> (N, 128, 6, 14)
-        x = self.relu(self.conv4(x))  # size = (N, 512, x.H/120, x.W/120) --> (N, 64, 4, 12)
-        x = torch.flatten(x, 1)
-
-        f = self.relu(self.fc1(x))
-
-        x = self.fc2(f)
-        x = x.reshape(-1, self.k, 3)
-
-        c = self.sigmoid(self.frc1(f))
-
-        return x, c
-
-
-# EoF
diff --git a/viplanner/plannernet/rgb_encoder.py b/viplanner/plannernet/rgb_encoder.py
deleted file mode 100644
index 4134500..0000000
--- a/viplanner/plannernet/rgb_encoder.py
+++ /dev/null
@@ -1,80 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import argparse
-import pickle
-from typing import Optional
-
-import torch
-import torch.nn as nn
-
-# detectron2 and mask2former (used to load pre-trained models from Mask2Former)
-try:
-    from detectron2.config import get_cfg
-    from detectron2.modeling.backbone import build_resnet_backbone
-    from detectron2.projects.deeplab import add_deeplab_config
-
-    PRE_TRAIN_POSSIBLE = True
-except ImportError:
-    PRE_TRAIN_POSSIBLE = False
-    print("[Warning] Pre-trained ResNet50 models cannot be used since detectron2" " not found")
-
-try:
-    from viplanner.third_party.mask2former.mask2former import add_maskformer2_config
-except ImportError:
-    PRE_TRAIN_POSSIBLE = False
-    print("[Warning] Pre-trained ResNet50 models cannot be used since" " mask2former not found")
-
-
-def get_m2f_cfg(cfg_path: str):  # -> CfgNode:
-    # load config from file
-    cfg = get_cfg()
-    add_deeplab_config(cfg)
-    add_maskformer2_config(cfg)
-    cfg.merge_from_file(cfg_path)
-    cfg.freeze()
-    return cfg
-
-
-class RGBEncoder(nn.Module):
-    def __init__(self, cfg, weight_path: Optional[str] = None, freeze: bool = True) -> None:
-        super().__init__()
-
-        # load pre-trained resnet
-        input_shape = argparse.Namespace()
-        input_shape.channels = 3
-        self.backbone = build_resnet_backbone(cfg, input_shape)
-
-        # load weights
-        if weight_path is not None:
-            with open(weight_path, "rb") as file:
-                model_file = pickle.load(file, encoding="latin1")
-
-            model_file["model"] = {k.replace("backbone.", ""): torch.tensor(v) for k, v in model_file["model"].items()}
-
-            missing_keys, unexpected_keys = self.backbone.load_state_dict(model_file["model"], strict=False)
-            if len(missing_keys) != 0:
-                print(f"[WARNING] Missing keys: {missing_keys}")
-                print(f"[WARNING] Unexpected keys: {unexpected_keys}")
-            print(f"[INFO] Loaded pre-trained backbone from {weight_path}")
-
-        # freeze network
-        if freeze:
-            for param in self.backbone.parameters():
-                param.requires_grad = False
-
-        # layers to get correct output shape --> modifiable
-        self.conv1 = nn.Conv2d(2048, 512, kernel_size=3, stride=1, padding=1)
-
-        return
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        x = self.backbone(x)["res5"]  # size = (N, 2048, 12, 20) (height and width same as ResNet18)
-        x = self.conv1(x)  # size = (N, 512,  12, 20)
-        return x
-
-
-# EoF
diff --git a/viplanner/train.py b/viplanner/train.py
deleted file mode 100644
index 0aceb5d..0000000
--- a/viplanner/train.py
+++ /dev/null
@@ -1,44 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# python
-import torch
-
-torch.set_default_dtype(torch.float32)
-
-# imperative-planning-learning
-from viplanner.config import DataCfg, TrainCfg
-from viplanner.utils.trainer import Trainer
-
-if __name__ == "__main__":
-    env_list_combi = [
-        "2azQ1b91cZZ",  # matterport mesh
-        "JeFG25nYj2p",  # matterport mesh
-        "Vvot9Ly1tCj",  # matterport mesh
-        "town01",  # carla mesh
-        "ur6pFq6Qu1A",  # matterport mesh
-        "B6ByNegPMKs",  # matterport mesh
-        "8WUmhLawc2A",  # matterport mesh
-        "town01",  # carla mesh
-        "2n8kARJN3HM",  # matterport mesh
-    ]
-    carla: TrainCfg = TrainCfg(
-        sem=True,
-        cost_map_name="cost_map_sem",
-        env_list=env_list_combi,
-        test_env_id=8,
-        file_name="combi_more_data",
-        data_cfg=DataCfg(
-            max_goal_distance=10.0,
-        ),
-        n_visualize=128,
-        wb_project="viplanner",
-    )
-    trainer = Trainer(carla)
-    trainer.train()
-    trainer.test()
-    trainer.save_config()
-    torch.cuda.empty_cache()
diff --git a/viplanner/traj_cost_opt/__init__.py b/viplanner/traj_cost_opt/__init__.py
deleted file mode 100644
index 1f6d43e..0000000
--- a/viplanner/traj_cost_opt/__init__.py
+++ /dev/null
@@ -1,20 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from .traj_opt import CubicSplineTorch, TrajOpt
-
-# for deployment in omniverse, pypose module is not available
-try:
-    import pypose as pp
-
-    from .traj_cost import TrajCost
-    from .traj_viz import TrajViz
-
-    __all__ = ["TrajCost", "TrajOpt", "TrajViz", "CubicSplineTorch"]
-except ModuleNotFoundError:
-    __all__ = ["TrajOpt", "CubicSplineTorch"]
-
-# EoF
diff --git a/viplanner/traj_cost_opt/traj_cost.py b/viplanner/traj_cost_opt/traj_cost.py
deleted file mode 100644
index f59fc61..0000000
--- a/viplanner/traj_cost_opt/traj_cost.py
+++ /dev/null
@@ -1,332 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from typing import Optional, Tuple
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-torch.set_default_dtype(torch.float32)
-
-from viplanner.cost_maps import CostMapPCD
-
-# visual-imperative-planning
-from .traj_opt import TrajOpt
-
-try:
-    import pypose as pp  # only used for training
-    import wandb  # only used for training
-except ModuleNotFoundError or ImportError:  # eval in issac sim  # TODO: check if all can be installed in Isaac Sim
-    print("[Warning] pypose or wandb not found, only use for evaluation")
-
-
-class TrajCost:
-    debug = False
-
-    def __init__(
-        self,
-        gpu_id: Optional[int] = 0,
-        log_data: bool = False,
-        w_obs: float = 0.25,
-        w_height: float = 1.0,
-        w_motion: float = 1.5,
-        w_goal: float = 2.0,
-        obstalce_thread: float = 0.75,
-        robot_width: float = 0.6,
-        robot_max_moving_distance: float = 0.15,
-    ) -> None:
-        # init map and optimizer
-        self.gpu_id = gpu_id
-        self.cost_map: CostMapPCD = None
-        self.opt = TrajOpt()
-        self.is_map = False
-        self.neg_reward: torch.Tensor = None
-
-        # loss weights
-        self.w_obs = w_obs
-        self.w_height = w_height
-        self.w_motion = w_motion
-        self.w_goal = w_goal
-
-        # fear label threshold value
-        self.obstalce_thread = obstalce_thread
-
-        # footprint radius
-        self.robot_width = robot_width
-        self.robot_max_moving_distance = robot_max_moving_distance
-
-        # logging
-        self.log_data = log_data
-        return
-
-    @staticmethod
-    def TransformPoints(odom, points):
-        batch_size, num_p, _ = points.shape
-        world_ps = pp.identity_SE3(
-            batch_size,
-            num_p,
-            device=points.device,
-            requires_grad=points.requires_grad,
-        )
-        world_ps.tensor()[:, :, 0:3] = points
-        world_ps = pp.SE3(odom[:, None, :]) @ pp.SE3(world_ps)
-        return world_ps
-
-    def SetMap(self, root_path, map_name):
-        self.cost_map = CostMapPCD.ReadTSDFMap(root_path, map_name, self.gpu_id)
-        self.is_map = True
-
-        # get negative reward of cost-map
-        self.neg_reward = torch.zeros(7, device=self.cost_map.device)
-        if self.cost_map.cfg.semantics:
-            self.neg_reward[2] = self.cost_map.cfg.sem_cost_map.negative_reward
-
-        return
-
-    def CostofTraj(
-        self,
-        waypoints: torch.Tensor,
-        odom: torch.Tensor,
-        goal: torch.Tensor,
-        fear: torch.Tensor,
-        log_step: int,
-        ahead_dist: float,
-        dataset: str = "train",
-    ):
-        batch_size, num_p, _ = waypoints.shape
-
-        assert self.is_map, "Map has to be set for cost calculation"
-        world_ps = self.TransformPoints(odom, waypoints).tensor()
-
-        # Obstacle loss
-        oloss_M = self._compute_oloss(world_ps, batch_size)
-        oloss = torch.mean(torch.sum(oloss_M, axis=1))
-
-        # Terrian Height loss
-        norm_inds, _ = self.cost_map.Pos2Ind(world_ps)
-        height_grid = self.cost_map.ground_array.T.expand(batch_size, 1, -1, -1)
-        hloss_M = (
-            F.grid_sample(
-                height_grid,
-                norm_inds[:, None, :, :],
-                mode="bicubic",
-                padding_mode="border",
-                align_corners=False,
-            )
-            .squeeze(1)
-            .squeeze(1)
-        )
-        hloss_M = torch.abs(world_ps[:, :, 2] - odom[:, None, 2] - hloss_M).to(
-            torch.float32
-        )  # world_ps - odom to have them on the ground to be comparable to the height map
-        hloss_M = torch.sum(hloss_M, axis=1)
-        hloss = torch.mean(hloss_M)
-
-        # Goal Cost - Control Cost
-        gloss_M = torch.norm(goal[:, :3] - waypoints[:, -1, :], dim=1)
-        # gloss = torch.mean(gloss_M)
-        gloss = torch.mean(torch.log(gloss_M + 1.0))
-
-        # Moving Loss - punish staying
-        desired_wp = self.opt.TrajGeneratorFromPFreeRot(goal[:, None, 0:3], step=1.0 / (num_p - 1))
-        desired_ds = torch.norm(desired_wp[:, 1:num_p, :] - desired_wp[:, 0 : num_p - 1, :], dim=2)
-        wp_ds = torch.norm(waypoints[:, 1:num_p, :] - waypoints[:, 0 : num_p - 1, :], dim=2)
-        mloss = torch.abs(desired_ds - wp_ds)
-        mloss = torch.sum(mloss, axis=1)
-        mloss = torch.mean(mloss)
-
-        # Complete Trajectory Loss
-        trajectory_loss = self.w_obs * oloss + self.w_height * hloss + self.w_motion * mloss + self.w_goal * gloss
-
-        # Fear labels
-        goal_dists = torch.cumsum(wp_ds, dim=1, dtype=wp_ds.dtype)
-        goal_dists = torch.vstack([goal_dists] * 3)
-        floss_M = torch.clone(oloss_M)
-        floss_M[goal_dists > ahead_dist] = 0.0
-        fear_labels = torch.max(floss_M, 1, keepdim=True)[0]
-        # fear_labels = nn.Sigmoid()(fear_labels-obstalce_thread)
-        fear_labels = fear_labels > self.obstalce_thread + self.neg_reward[2]
-        fear_labels = torch.any(fear_labels.reshape(3, batch_size).T, dim=1, keepdim=True).to(torch.float32)
-        # Fear loss
-        collision_probabilty_loss = nn.BCELoss()(fear, fear_labels.float())
-
-        # log
-        if self.log_data:
-            try:
-                wandb.log(
-                    {f"Height Loss {dataset}": self.w_height * hloss},
-                    step=log_step,
-                )
-                wandb.log(
-                    {f"Obstacle Loss {dataset}": self.w_obs * oloss},
-                    step=log_step,
-                )
-                wandb.log(
-                    {f"Goal Loss {dataset}": self.w_goal * gloss},
-                    step=log_step,
-                )
-                wandb.log(
-                    {f"Motion Loss {dataset}": self.w_motion * mloss},
-                    step=log_step,
-                )
-                wandb.log(
-                    {f"Trajectory Loss {dataset}": trajectory_loss},
-                    step=log_step,
-                )
-                wandb.log(
-                    {f"Collision Loss {dataset}": collision_probabilty_loss},
-                    step=log_step,
-                )
-            except:  # noqa: E722
-                print("wandb log failed")
-
-        # TODO: kinodynamics cost
-        return collision_probabilty_loss + trajectory_loss
-
-    def obs_cost_eval(self, odom: torch.Tensor, waypoints: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
-        """Compute Obstacle Loss for eval_sim_static script!
-
-        Args:
-            odom (torch.Tensor): Current odometry
-            waypoints (torch.Tensor): waypoints in camera frame
-
-        Returns:
-            tuple: mean obstacle loss for each trajectory, max obstacle loss for each trajectory
-        """
-        assert self.is_map, "Map has to be loaded for evaluation"
-
-        # compute obstacle loss
-        world_ps = self.TransformPoints(odom, waypoints).tensor()
-        oloss_M = self._compute_oloss(world_ps, waypoints.shape[0])
-        # account for negative reward
-        oloss_M = oloss_M - self.neg_reward[2]
-        oloss_M[oloss_M < 0] = 0.0
-        oloss_M = oloss_M.reshape(-1, waypoints.shape[0], oloss_M.shape[1])
-        return torch.mean(oloss_M, axis=[0, 2]), torch.amax(oloss_M, dim=[0, 2])
-
-    def cost_of_recorded_path(
-        self,
-        waypoints: torch.Tensor,
-    ) -> torch.Tensor:
-        """Cost of recorded path - for evaluation only
-
-        Args:
-            waypoints (torch.Tensor): Path coordinates in world frame
-        """
-        assert self.is_map, "Map has to be loaded for evaluation"
-        oloss_M = self._compute_oloss(waypoints.unsqueeze(0), 1)
-        return torch.max(oloss_M)
-
-    def _compute_oloss(self, world_ps, batch_size):
-        if world_ps.shape[1] == 1:  # special case when evaluating cost of a recorded path
-            world_ps_inflated = world_ps
-        else:
-            # include robot dimension as square
-            tangent = world_ps[:, 1:, 0:2] - world_ps[:, :-1, 0:2]  # get tangent vector
-            tangent = tangent / torch.norm(tangent, dim=2, keepdim=True)  # normalize normals vector
-            normals = tangent[:, :, [1, 0]] * torch.tensor(
-                [-1, 1], dtype=torch.float32, device=world_ps.device
-            )  # get normal vector
-            world_ps_inflated = torch.vstack([world_ps[:, :-1, :]] * 3)  # duplicate points
-            world_ps_inflated[:, :, 0:2] = torch.vstack(
-                [
-                    # movement corners
-                    world_ps[:, :-1, 0:2] + normals * self.robot_width / 2,  # front_right
-                    world_ps[:, :-1, 0:2],  # center
-                    world_ps[:, :-1, 0:2] - normals * self.robot_width / 2,  # front_left
-                ]
-            )
-
-        norm_inds, cost_idx = self.cost_map.Pos2Ind(world_ps_inflated)
-
-        # Obstacle Cost
-        cost_grid = self.cost_map.cost_array.T.expand(world_ps_inflated.shape[0], 1, -1, -1)
-        oloss_M = (
-            F.grid_sample(
-                cost_grid,
-                norm_inds[:, None, :, :],
-                mode="bicubic",
-                padding_mode="border",
-                align_corners=False,
-            )
-            .squeeze(1)
-            .squeeze(1)
-        )
-        oloss_M = oloss_M.to(torch.float32)
-
-        if self.debug:
-            # add negative reward for cost-map
-            world_ps_inflated = world_ps_inflated + self.neg_reward
-
-            import numpy as np
-
-            # indexes in the cost map
-            start_xy = torch.tensor(
-                [self.cost_map.cfg.x_start, self.cost_map.cfg.y_start],
-                dtype=torch.float64,
-                device=world_ps_inflated.device,
-            ).expand(1, 1, -1)
-            H = (world_ps_inflated[:, :, 0:2] - start_xy) / self.cost_map.cfg.general.resolution
-            cost_values = self.cost_map.cost_array[
-                H[[0, batch_size, batch_size * 2], :, 0].reshape(-1).detach().cpu().numpy().astype(np.int64),
-                H[[0, batch_size, batch_size * 2], :, 1].reshape(-1).detach().cpu().numpy().astype(np.int64),
-            ]
-
-            import matplotlib.pyplot as plt
-
-            _, (ax1, ax2, ax3) = plt.subplots(1, 3)
-            sc1 = ax1.scatter(
-                world_ps_inflated[[0, batch_size, batch_size * 2], :, 0].reshape(-1).detach().cpu().numpy(),
-                world_ps_inflated[[0, batch_size, batch_size * 2], :, 1].reshape(-1).detach().cpu().numpy(),
-                c=oloss_M[[0, batch_size, batch_size * 2]].reshape(-1).detach().cpu().numpy(),
-                cmap="rainbow",
-                vmin=0,
-                vmax=torch.max(cost_grid).item(),
-            )
-            ax1.set_aspect("equal", adjustable="box")
-            ax2.scatter(
-                H[[0, batch_size, batch_size * 2], :, 0].reshape(-1).detach().cpu().numpy(),
-                H[[0, batch_size, batch_size * 2], :, 1].reshape(-1).detach().cpu().numpy(),
-                c=cost_values.cpu().numpy(),
-                cmap="rainbow",
-                vmin=0,
-                vmax=torch.max(cost_grid).item(),
-            )
-            ax2.set_aspect("equal", adjustable="box")
-            cost_array = self.cost_map.cost_array.cpu().numpy()
-            max_cost = torch.max(self.cost_map.cost_array).item()
-            scale_factor = [1.4, 1.8]
-            for idx, run_idx in enumerate([0, batch_size, batch_size * 2]):
-                _, cost_idx = self.cost_map.Pos2Ind(world_ps_inflated[run_idx, :, :].unsqueeze(0))
-                cost_array[
-                    cost_idx.to(torch.int32).cpu().numpy()[:, 0],
-                    cost_idx.to(torch.int32).cpu().numpy()[:, 1],
-                ] = (
-                    max_cost * scale_factor[idx]
-                )
-            ax3.imshow(cost_array)
-
-            plt.figure()
-            plt.title("cost_map")
-            plt.imshow(cost_array)
-
-            import open3d as o3d
-
-            pcd = o3d.geometry.PointCloud()
-            pcd.points = o3d.utility.Vector3dVector(
-                world_ps_inflated[[0, batch_size, batch_size * 2], :, :3].reshape(-1, 3).detach().cpu().numpy()
-            )
-            pcd.colors = o3d.utility.Vector3dVector(
-                sc1.to_rgba(oloss_M[[0, batch_size, batch_size * 2]].reshape(-1).detach().cpu().numpy())[:, :3]
-            )
-            # pcd.colors = o3d.utility.Vector3dVector(sc2.to_rgba(cost_values[0].cpu().numpy())[:, :3])
-            o3d.visualization.draw_geometries([self.cost_map.pcd_tsdf, pcd])
-
-        return oloss_M
-
-
-# EoF
diff --git a/viplanner/traj_cost_opt/traj_opt.py b/viplanner/traj_cost_opt/traj_opt.py
deleted file mode 100644
index 3a205d8..0000000
--- a/viplanner/traj_cost_opt/traj_opt.py
+++ /dev/null
@@ -1,119 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import torch
-
-torch.set_default_dtype(torch.float32)
-
-
-class CubicSplineTorch:
-    # Reference: https://stackoverflow.com/questions/61616810/how-to-do-cubic-spline-interpolation-and-integration-in-pytorch
-    def __init__(self):
-        self.init_m = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float32)
-
-    def h_poly(self, t):
-        alpha = torch.arange(4, device=t.device, dtype=t.dtype)
-        tt = t[:, None, :] ** alpha[None, :, None]
-        A = torch.tensor(
-            [[1, 0, -3, 2], [0, 1, -2, 1], [0, 0, 3, -2], [0, 0, -1, 1]],
-            dtype=t.dtype,
-            device=t.device,
-        )
-        return A @ tt
-
-    def interp(self, x, y, xs):
-        m = (y[:, 1:, :] - y[:, :-1, :]) / torch.unsqueeze(x[:, 1:] - x[:, :-1], 2)
-        m = torch.cat([m[:, None, 0], (m[:, 1:] + m[:, :-1]) / 2, m[:, None, -1]], 1)
-        idxs = torch.searchsorted(x[0, 1:], xs[0, :])
-        dx = x[:, idxs + 1] - x[:, idxs]
-        hh = self.h_poly((xs - x[:, idxs]) / dx)
-        hh = torch.transpose(hh, 1, 2)
-        out = hh[:, :, 0:1] * y[:, idxs, :]
-        out = out + hh[:, :, 1:2] * m[:, idxs] * dx[:, :, None]
-        out = out + hh[:, :, 2:3] * y[:, idxs + 1, :]
-        out = out + hh[:, :, 3:4] * m[:, idxs + 1] * dx[:, :, None]
-        return out
-
-
-class TrajOpt:
-    debug = False
-
-    def __init__(self):
-        self.cs_interp = CubicSplineTorch()
-
-    def TrajGeneratorFromPFreeRot(self, preds, step):
-        # Points is in se3
-        batch_size, num_p, dims = preds.shape
-        points_preds = torch.cat(
-            (
-                torch.zeros(
-                    batch_size,
-                    1,
-                    dims,
-                    device=preds.device,
-                    requires_grad=preds.requires_grad,
-                ),
-                preds,
-            ),
-            axis=1,
-        )
-        num_p = num_p + 1
-        xs = torch.arange(0, num_p - 1 + step, step, device=preds.device)
-        xs = xs.repeat(batch_size, 1)
-        x = torch.arange(num_p, device=preds.device, dtype=preds.dtype)
-        x = x.repeat(batch_size, 1)
-        waypoints = self.cs_interp.interp(x, points_preds, xs)
-
-        if self.debug:
-            import matplotlib.pyplot as plt  # for plotting
-
-            plt.scatter(
-                points_preds[0, :, 0].cpu().numpy(),
-                points_preds[0, :, 1].cpu().numpy(),
-                label="Samples",
-                color="purple",
-            )
-            plt.plot(
-                waypoints[0, :, 0].cpu().numpy(),
-                waypoints[0, :, 1].cpu().numpy(),
-                label="Interpolated curve",
-                color="blue",
-            )
-            plt.legend()
-            plt.show()
-
-        return waypoints  # R3
-
-    def interpolate_waypoints(self, preds):
-        shape = list(preds.shape)
-        out_shape = [50, shape[2]]
-        waypoints = torch.nn.functional.interpolate(
-            preds.unsqueeze(1),
-            size=tuple(out_shape),
-            mode="bilinear",
-            align_corners=True,
-        )
-        waypoints = waypoints.squeeze(1)
-
-        if self.debug:
-            import matplotlib.pyplot as plt  # for plotting
-
-            plt.scatter(
-                preds[0, :, 0].detach().cpu().numpy(),
-                preds[0, :, 1].detach().cpu().numpy(),
-                label="Samples",
-                color="purple",
-            )
-            plt.plot(
-                waypoints[0, :, 0].detach().cpu().numpy(),
-                waypoints[0, :, 1].detach().cpu().numpy(),
-                label="Interpolated curve",
-                color="blue",
-            )
-            plt.legend()
-            plt.show()
-
-        return waypoints
diff --git a/viplanner/traj_cost_opt/traj_viz.py b/viplanner/traj_cost_opt/traj_viz.py
deleted file mode 100644
index 598de59..0000000
--- a/viplanner/traj_cost_opt/traj_viz.py
+++ /dev/null
@@ -1,329 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# python
-import copy
-from typing import Optional
-
-import cv2
-import matplotlib.pyplot as plt
-import numpy as np
-import open3d as o3d
-import open3d.visualization.rendering as rendering
-import pypose as pp
-import scipy.spatial.transform as tf
-import torch
-
-# visual-planning-learning
-from viplanner.cost_maps import CostMapPCD
-
-from .traj_cost import TrajCost
-
-
-class TrajViz:
-    def __init__(
-        self,
-        intrinsics: np.ndarray,
-        cam_resolution: tuple = (360, 640),
-        camera_tilt: float = 0.0,
-        cost_map: Optional[CostMapPCD] = None,
-    ):
-        # get parameters
-        self._cam_resolution = cam_resolution
-        self._intrinsics = intrinsics
-        self._cost_map = cost_map
-        self._camera_tilt = camera_tilt
-
-        # init camera
-        self.set_camera()
-
-    def set_camera(self):
-        self.camera = o3d.camera.PinholeCameraIntrinsic(
-            self._cam_resolution[1],  # width
-            self._cam_resolution[0],  # height
-            self._intrinsics[0, 0],  # fx
-            self._intrinsics[1, 1],  # fy
-            self._intrinsics[0, 2],  # cx  (width/2)
-            self._intrinsics[1, 2],  # cy  (height/2)
-        )
-        return
-
-    def VizTrajectory(
-        self,
-        preds: torch.Tensor,
-        waypoints: torch.Tensor,
-        odom: torch.Tensor,
-        goal: torch.Tensor,
-        fear: torch.Tensor,
-        augment_viz: torch.Tensor,
-        cost_map: bool = True,
-        visual_height: float = 0.5,
-        mesh_size: float = 0.5,
-        fov_angle: float = 0.0,
-    ) -> None:
-        """Visualize the trajectory within the costmap
-
-        Args:
-            preds (torch.Tensor): predicted keypoints
-            waypoints (torch.Tensor): waypoints
-            odom (torch.Tensor): odom tensor
-            goal (torch.Tensor): goal tensor
-            fear (torch.Tensor): if trajectory is risky
-            augment_viz (torch.Tensor): if input has been augmented
-            cost_map (bool, optional): visualize costmap. Defaults to True.
-            visual_height (float, optional): visual height of the keypoints. Defaults to 0.5.
-            mesh_size (float, optional): size of the mesh. Defaults to 0.5.
-            fov_angle (float, optional): field of view angle. Defaults to 0.0.
-        """
-        # transform to map frame
-        if not isinstance(self._cost_map, CostMapPCD):
-            print("Cost map is missing.")
-            return
-        batch_size = len(waypoints)
-        # transform to world frame
-        preds_ws = TrajCost.TransformPoints(odom, preds).tensor().cpu().detach().numpy()
-        wp_ws = TrajCost.TransformPoints(odom, waypoints).tensor().cpu().detach().numpy()
-        goal_ws = pp.SE3(odom) @ pp.SE3(goal)
-        # convert to positions
-        goal_ws = goal_ws.tensor()[:, 0:3].numpy()
-        visual_list = []
-        if cost_map:
-            visual_list.append(self._cost_map.pcd_tsdf)
-        else:
-            visual_list.append(self._cost_map.pcd_viz)
-            visual_height = visual_height / 5.0
-
-        # visualize and trajs
-        traj_pcd = o3d.geometry.PointCloud()
-        wp_ws = np.concatenate(wp_ws, axis=0)
-        wp_ws[:, 2] = wp_ws[:, 2] + visual_height
-        traj_pcd.points = o3d.utility.Vector3dVector(wp_ws[:, 0:3])
-        traj_pcd.paint_uniform_color([0.99, 0.1, 0.1])
-        visual_list.append(traj_pcd)
-        # start and goal marks
-        mesh_sphere = o3d.geometry.TriangleMesh.create_sphere(mesh_size / 1.5)  # start points
-        mesh_sphere_augment = o3d.geometry.TriangleMesh.create_sphere(mesh_size / 1.5)  # start points
-        small_sphere = o3d.geometry.TriangleMesh.create_sphere(mesh_size / 3.0)  # successful trajectory points
-        small_sphere_fear = o3d.geometry.TriangleMesh.create_sphere(mesh_size / 3.0)  # unsuccessful trajectory points
-        mesh_box = o3d.geometry.TriangleMesh.create_box(mesh_size, mesh_size, mesh_size)  # end points
-        # set mesh colors
-        mesh_box.paint_uniform_color([1.0, 0.64, 0.0])
-        small_sphere.paint_uniform_color([0.4, 1.0, 0.1])
-        small_sphere_fear.paint_uniform_color([1.0, 0.4, 0.1])
-        mesh_sphere_augment.paint_uniform_color([0.0, 0.0, 1.0])
-        # field of view visualization
-        fov_vis_length = 0.75  # length of the fov visualization plane in meters
-        fov_vis_pt_right = pp.SE3(odom) @ pp.SE3(
-            [
-                fov_vis_length * np.cos(fov_angle / 2),
-                fov_vis_length * np.sin(fov_angle / 2),
-                0,
-                0,
-                0,
-                0,
-                1,
-            ]
-        )
-        fov_vis_pt_left = pp.SE3(odom) @ pp.SE3(
-            [
-                fov_vis_length * np.cos(fov_angle / 2),
-                -fov_vis_length * np.sin(fov_angle / 2),
-                0,
-                0,
-                0,
-                0,
-                1,
-            ]
-        )
-        fov_vis_pt_right = fov_vis_pt_right.numpy()[:, 0:3]
-        fov_vis_pt_right[:, 2] += visual_height
-        fov_vis_pt_left = fov_vis_pt_left.numpy()[:, 0:3]
-        fov_vis_pt_left[:, 2] += visual_height
-
-        lines = []
-        points = []
-        for i in range(batch_size):
-            lines.append([2 * i, 2 * i + 1])
-            gp = goal_ws[i, :]
-            op = odom.numpy()[i, :]
-            op[2] = op[2] + visual_height
-            gp[2] = gp[2] + visual_height
-            points.append(gp[:3].tolist())
-            points.append(op[:3].tolist())
-            # add fov visualization
-            fov_mesh = o3d.geometry.TriangleMesh(
-                vertices=o3d.utility.Vector3dVector(np.array([op[:3], fov_vis_pt_right[i], fov_vis_pt_left[i]])),
-                triangles=o3d.utility.Vector3iVector(np.array([[2, 1, 0]])),
-            )
-            fov_mesh.paint_uniform_color([1.0, 0.5, 0.0])
-            visual_list.append(fov_mesh)
-            # add visualization
-            if augment_viz[i]:
-                visual_list.append(copy.deepcopy(mesh_sphere_augment).translate((op[0], op[1], op[2])))
-            else:
-                visual_list.append(copy.deepcopy(mesh_sphere).translate((op[0], op[1], op[2])))
-            visual_list.append(
-                copy.deepcopy(mesh_box).translate(
-                    (
-                        gp[0] - mesh_size / 2.0,
-                        gp[1] - mesh_size / 2.0,
-                        gp[2] - mesh_size / 2.0,
-                    )
-                )
-            )
-            for j in range(preds_ws[i].shape[0]):
-                kp = preds_ws[i][j, :]
-                if fear[i, :] > 0.5:
-                    visual_list.append(
-                        copy.deepcopy(small_sphere_fear).translate((kp[0], kp[1], kp[2] + visual_height))
-                    )
-                else:
-                    visual_list.append(copy.deepcopy(small_sphere).translate((kp[0], kp[1], kp[2] + visual_height)))
-        # set line from odom to goal
-        colors = [[0.99, 0.99, 0.1] for i in range(len(lines))]
-        line_set = o3d.geometry.LineSet(
-            o3d.utility.Vector3dVector(points),
-            o3d.utility.Vector2iVector(lines),
-        )
-        line_set.colors = o3d.utility.Vector3dVector(colors)
-        visual_list.append(line_set)
-        o3d.visualization.draw_geometries(visual_list)
-        return
-
-    def VizImages(
-        self,
-        preds: torch.Tensor,
-        waypoints: torch.Tensor,
-        odom: torch.Tensor,
-        goal: torch.Tensor,
-        fear,
-        images: torch.Tensor,
-        visual_offset=0.35,
-        mesh_size=0.3,
-        is_shown=True,
-        iplanner: bool = False,
-        transform: bool = True,
-    ):
-        batch_size = len(waypoints)
-        if transform:
-            preds_ws = TrajCost.TransformPoints(odom, preds).tensor().cpu().detach().numpy()
-            wp_ws = TrajCost.TransformPoints(odom, waypoints).tensor().cpu().detach().numpy()
-            if goal.shape[-1] != 7:
-                pp_goal = pp.identity_SE3(batch_size, device=goal.device)
-                pp_goal.tensor()[:, 0:3] = goal
-                goal = pp_goal.tensor()
-            goal_ws = pp.SE3(odom) @ pp.SE3(goal)
-            # convert to positions
-            goal_ws = goal_ws.tensor()[:, 0:3].cpu().detach().numpy()
-        else:
-            preds_ws = preds.cpu().detach().numpy()
-            wp_ws = waypoints.cpu().detach().numpy()
-            goal_ws = goal.cpu().detach().numpy()
-
-        # adjust height
-        goal_ws[:, 2] = goal_ws[:, 2] - visual_offset
-
-        # set materia shader
-        mtl = o3d.visualization.rendering.MaterialRecord()
-        mtl.base_color = [1.0, 1.0, 1.0, 0.3]
-        mtl.shader = "defaultUnlit"
-        # set meshes
-        small_sphere = o3d.geometry.TriangleMesh.create_sphere(mesh_size / 10.0)  # trajectory points
-        small_sphere_fear = o3d.geometry.TriangleMesh.create_sphere(mesh_size / 10.0)  # trajectory points
-        mesh_sphere = o3d.geometry.TriangleMesh.create_sphere(mesh_size / 2.0)  # successful predict points
-        mesh_sphere_fear = o3d.geometry.TriangleMesh.create_sphere(mesh_size / 2.0)  # unsuccessful predict points
-        mesh_box = o3d.geometry.TriangleMesh.create_box(mesh_size, mesh_size, mesh_size * 2)  # end points
-        # set colors
-        if iplanner:
-            small_sphere.paint_uniform_color([0.0, 0.0, 1.0])  # blue
-            mesh_sphere.paint_uniform_color([1.0, 1.0, 0.0])
-        else:
-            small_sphere.paint_uniform_color([0.99, 0.2, 0.1])  # green
-            mesh_sphere.paint_uniform_color([0.4, 1.0, 0.1])
-
-        small_sphere_fear.paint_uniform_color([1.0, 0.4, 0.2])
-        mesh_sphere_fear.paint_uniform_color([1.0, 0.2, 0.1])
-
-        mesh_box.paint_uniform_color([1.0, 0.64, 0.1])
-
-        # init open3D render
-        render = rendering.OffscreenRenderer(self.camera.width, self.camera.height)
-        render.scene.set_background([0.0, 0.0, 0.0, 1.0])  # RGBA
-
-        # wp_start_idx = int(waypoints.shape[1] / preds.shape[1])
-        wp_start_idx = 1
-        cv_img_list = []
-
-        if is_shown:
-            fig, ax = plt.subplots()
-
-        for i in range(batch_size):
-            # add geometries
-            gp = goal_ws[i, :]
-            # add goal marker
-            goal_mesh = copy.deepcopy(mesh_box).translate(
-                (
-                    gp[0] - mesh_size / 2.0,
-                    gp[1] - mesh_size / 2.0,
-                    gp[2] - mesh_size / 2.0,
-                )
-            )
-            render.scene.add_geometry("goal_mesh", goal_mesh, mtl)
-            # add predictions
-            for j, kp in enumerate(preds_ws[i]):
-                if fear[i, :] > 0.5:
-                    kp_mesh = copy.deepcopy(mesh_sphere_fear).translate((kp[0], kp[1], kp[2] - visual_offset))
-                else:
-                    kp_mesh = copy.deepcopy(mesh_sphere).translate((kp[0], kp[1], kp[2] - visual_offset))
-                render.scene.add_geometry("keypose" + str(j), kp_mesh, mtl)
-            # add trajectory
-            for k, wp in enumerate(wp_ws[i]):
-                if k < wp_start_idx:
-                    continue
-                if fear[i, :] > 0.5:
-                    wp_mesh = copy.deepcopy(small_sphere_fear).translate((wp[0], wp[1], wp[2] - visual_offset))
-                else:
-                    wp_mesh = copy.deepcopy(small_sphere).translate((wp[0], wp[1], wp[2] - visual_offset))
-                render.scene.add_geometry("waypoint" + str(k), wp_mesh, mtl)
-            # set cameras
-            self.CameraLookAtPose(odom[i, :], render)
-            # project to image
-            img_o3d = np.asarray(render.render_to_image())
-            mask = (img_o3d < 10).all(axis=2)
-            # Attach image
-            c_img = images[i, :, :].expand(3, -1, -1)
-            c_img = c_img.cpu().detach().numpy()
-            c_img = np.moveaxis(c_img, 0, 2)
-            c_img = (c_img * 255 / np.max(c_img)).astype("uint8")
-            img_o3d[mask, :] = c_img[mask, :]
-            img_cv2 = cv2.cvtColor(img_o3d, cv2.COLOR_RGBA2BGRA)
-            cv_img_list.append(img_cv2)
-            if is_shown:
-                plt.imshow(img_cv2)
-                plt.draw()
-                plt.waitforbuttonpress(0)  # this will wait for indefinite time
-                plt.close(fig)
-            # clear render geometry
-            render.scene.clear_geometry()
-
-        return cv_img_list
-
-    def CameraLookAtPose(self, odom, render):
-        unit_vec = pp.identity_SE3(device=odom.device)
-        unit_vec.tensor()[0] = 1.0
-        tilt_vec = [0, 0, 0]
-        tilt_vec.extend(list(tf.Rotation.from_euler("y", self._camera_tilt, degrees=False).as_quat()))
-        tilt_vec = torch.tensor(tilt_vec, device=odom.device, dtype=odom.dtype)
-        target_pose = pp.SE3(odom) @ pp.SE3(tilt_vec) @ unit_vec
-        camera_up = [0, 0, 1]  # camera orientation
-        eye = pp.SE3(odom)
-        eye = eye.tensor()[0:3].cpu().detach().numpy()
-        target = target_pose.tensor()[0:3].cpu().detach().numpy()
-        render.scene.camera.look_at(target, eye, camera_up)
-        return
-
-
-# EoF
diff --git a/viplanner/utils/__init__.py b/viplanner/utils/__init__.py
deleted file mode 100644
index e22fc52..0000000
--- a/viplanner/utils/__init__.py
+++ /dev/null
@@ -1,5 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
diff --git a/viplanner/utils/dataset.py b/viplanner/utils/dataset.py
deleted file mode 100644
index cc32300..0000000
--- a/viplanner/utils/dataset.py
+++ /dev/null
@@ -1,1294 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import copy
-import math
-
-# python
-import os
-import random
-import shutil
-from pathlib import Path
-from random import sample
-from typing import Dict, List, Optional, Tuple
-
-import cv2
-import networkx as nx
-import numpy as np
-import open3d as o3d
-import PIL
-import pypose as pp
-import scipy.spatial.transform as tf
-import torch
-import torch.nn.functional as F
-import torchvision.transforms as transforms
-from PIL import Image
-from scipy.spatial.kdtree import KDTree
-from skimage.util import random_noise
-from torch.utils.data import Dataset
-from tqdm import tqdm
-
-# implerative-planner-learning
-from viplanner.config import DataCfg
-from viplanner.cost_maps import CostMapPCD
-
-# set default dtype to float32
-torch.set_default_dtype(torch.float32)
-
-
-class PlannerData(Dataset):
-    def __init__(
-        self,
-        cfg: DataCfg,
-        transform,
-        semantics: bool = False,
-        rgb: bool = False,
-        pixel_mean: Optional[np.ndarray] = None,
-        pixel_std: Optional[np.ndarray] = None,
-    ) -> None:
-        """_summary_
-
-        Args:
-            cfg (DataCfg): Dataset COnfiguration
-            transform (_type_): Compose torchvision transforms (resize and to tensor)
-            semantics (bool, optional): If semantics are used in the network input. Defaults to False.
-        """
-
-        self._cfg = cfg
-        self.transform = transform
-        self.semantics = semantics
-        self.rgb = rgb
-        assert not (semantics and rgb), "Semantics and RGB cannot be used at the same time"
-        self.pixel_mean = pixel_mean
-        self.pixel_std = pixel_std
-
-        # vertical flip transform
-        self.flip_transform = transforms.RandomHorizontalFlip(p=1.0)
-
-        # init buffers
-        self.depth_filename: List[str] = []
-        self.sem_rgb_filename: List[str] = []
-        self.depth_imgs: List[torch.Tensor] = []
-        self.sem_imgs: List[torch.Tensor] = []
-        self.odom: torch.Tensor = None
-        self.goal: torch.Tensor = None
-        self.pair_augment: np.ndarray = None
-        self.fov_angle: float = 0.0
-        self.load_ram: bool = False
-        return
-
-    def update_buffers(
-        self,
-        depth_filename: List[str],
-        sem_rgb_filename: List[str],
-        odom: torch.Tensor,
-        goal: torch.Tensor,
-        pair_augment: np.ndarray,
-    ) -> None:
-        self.depth_filename = depth_filename
-        self.sem_rgb_filename = sem_rgb_filename
-        self.odom = odom
-        self.goal = goal
-        self.pair_augment = pair_augment
-        return
-
-    def set_fov(self, fov_angle):
-        self.fov_angle = fov_angle
-        return
-
-    """Augment Images with black polygons"""
-
-    def _add_random_polygons(self, image, nb_polygons, max_size):
-        for i in range(nb_polygons):
-            num_corners = random.randint(10, 20)
-            polygon_points = np.random.randint(0, max_size, size=(num_corners, 2))
-            x_offset = np.random.randint(0, image.shape[0])
-            y_offset = np.random.randint(0, image.shape[1])
-            polygon_points[:, 0] += x_offset
-            polygon_points[:, 1] += y_offset
-
-            # Create a convex hull from the points
-            hull = cv2.convexHull(polygon_points)
-
-            # Draw the hull on the image
-            cv2.fillPoly(image, [hull], (0, 0, 0))
-        return image
-
-    """Load images"""
-
-    def load_data_in_memory(self) -> None:
-        """Load data into RAM to speed up training"""
-        for idx in tqdm(range(len(self.depth_filename)), desc="Load images into RAM"):
-            self.depth_imgs.append(self._load_depth_img(idx))
-            if self.semantics or self.rgb:
-                self.sem_imgs.append(self._load_sem_rgb_img(idx))
-        self.load_ram = True
-        return
-
-    def _load_depth_img(self, idx) -> torch.Tensor:
-        if self.depth_filename[idx].endswith(".png"):
-            depth_image = Image.open(self.depth_filename[idx])
-            if self._cfg.real_world_data:
-                depth_image = np.array(depth_image.transpose(PIL.Image.ROTATE_180))
-            else:
-                depth_image = np.array(depth_image)
-        else:
-            depth_image = np.load(self.depth_filename[idx])
-        depth_image[~np.isfinite(depth_image)] = 0.0
-        depth_image = (depth_image / 1000.0).astype("float32")
-        depth_image[depth_image > self._cfg.max_depth] = 0.0
-
-        # add noise to depth image
-        if self._cfg.depth_salt_pepper or self._cfg.depth_gaussian:
-            depth_norm = (depth_image - np.min(depth_image)) / (np.max(depth_image) - np.min(depth_image))
-            if self._cfg.depth_salt_pepper:
-                depth_norm = random_noise(
-                    depth_norm,
-                    mode="s&p",
-                    amount=self._cfg.depth_salt_pepper,
-                    clip=False,
-                )
-            if self._cfg.depth_gaussian:
-                depth_norm = random_noise(
-                    depth_norm,
-                    mode="gaussian",
-                    mean=0,
-                    var=self._cfg.depth_gaussian,
-                    clip=False,
-                )
-            depth_image = depth_norm * (np.max(depth_image) - np.min(depth_image)) + np.min(depth_image)
-        if self._cfg.depth_random_polygons_nb and self._cfg.depth_random_polygons_nb > 0:
-            depth_image = self._add_random_polygons(
-                depth_image,
-                self._cfg.depth_random_polygons_nb,
-                self._cfg.depth_random_polygon_size,
-            )
-
-        # transform depth image
-        depth_image = self.transform(depth_image).type(torch.float32)
-        if self.pair_augment[idx]:
-            depth_image = self.flip_transform.forward(depth_image)
-
-        return depth_image
-
-    def _load_sem_rgb_img(self, idx) -> torch.Tensor:
-        image = Image.open(self.sem_rgb_filename[idx])
-        if self._cfg.real_world_data:
-            image = np.array(image.transpose(PIL.Image.ROTATE_180))
-        else:
-            image = np.array(image)
-        # normalize image
-        if self.pixel_mean is not None and self.pixel_std is not None:
-            image = (image - self.pixel_mean) / self.pixel_std
-
-        # add noise to semantic image
-        if self._cfg.sem_rgb_black_img:
-            if random.randint(0, 99) < self._cfg.sem_rgb_black_img * 100:
-                image = np.zeros_like(image)
-        if self._cfg.sem_rgb_pepper:
-            image = random_noise(
-                image,
-                mode="pepper",
-                amount=self._cfg.depth_salt_pepper,
-                clip=False,
-            )
-        if self._cfg.sem_rgb_random_polygons_nb and self._cfg.sem_rgb_random_polygons_nb > 0:
-            image = self._add_random_polygons(
-                image,
-                self._cfg.sem_rgb_random_polygons_nb,
-                self._cfg.sem_rgb_random_polygon_size,
-            )
-
-        # transform semantic image
-        image = self.transform(image).type(torch.float32)
-        assert image.round(decimals=1).max() <= 1.0, (
-            f"Image '{self.sem_rgb_filename[idx]}' is not normalized with max" f" value {image.max().item()}"
-        )
-
-        if self.pair_augment[idx]:
-            image = self.flip_transform.forward(image)
-
-        return image
-
-    """Get image in training"""
-
-    def __len__(self):
-        return len(self.depth_filename)
-
-    def __getitem__(self, idx):
-        """
-        Get batch items
-
-        Returns:
-            - depth_image: depth image
-            - sem_rgb_image: semantic image
-            - odom: odometry of the start pose (point and rotation)
-            - goal: goal point in the camera frame
-            - pair_augment: bool if the pair is augmented (flipped at the y-axis of the image)
-        """
-
-        # get depth image
-        if self.load_ram:
-            depth_image = self.depth_imgs[idx]
-            if self.semantics or self.rgb:
-                sem_rgb_image = self.sem_imgs[idx]
-            else:
-                sem_rgb_image = 0
-        else:
-            depth_image = self._load_depth_img(idx)
-            if self.semantics or self.rgb:
-                sem_rgb_image = self._load_sem_rgb_img(idx)
-            else:
-                sem_rgb_image = 0
-
-        return (
-            depth_image,
-            sem_rgb_image,
-            self.odom[idx],
-            self.goal[idx],
-            self.pair_augment[idx],
-        )
-
-
-class DistanceSchemeIdx:
-    def __init__(self, distance: float) -> None:
-        self.distance: float = distance
-
-        self.odom_list: List[pp.LieTensor] = []
-        self.goal_list: List[pp.LieTensor] = []
-        self.pair_within_fov: List[bool] = []
-        self.pair_front_of_robot: List[bool] = []
-        self.pair_behind_robot: List[bool] = []
-        self.depth_img_list: List[str] = []
-        self.sem_rgb_img_list: List[str] = []
-
-        # flags
-        self.has_data: bool = False
-        return
-
-    def update_buffers(
-        self,
-        odom: pp.LieTensor,
-        goal: pp.LieTensor,
-        within_fov: bool = False,
-        front_of_robot: bool = False,
-        behind_robot: bool = False,
-        depth_filename: str = None,
-        sem_rgb_filename: str = None,
-    ) -> None:
-        self.odom_list.append(odom)
-        self.goal_list.append(goal)
-        self.pair_within_fov.append(within_fov)
-        self.pair_front_of_robot.append(front_of_robot)
-        self.pair_behind_robot.append(behind_robot)
-        self.depth_img_list.append(depth_filename)
-        self.sem_rgb_img_list.append(sem_rgb_filename)
-
-        self.has_data = len(self.odom_list) > 0
-        return
-
-    def get_data(
-        self,
-        nb_fov: int,
-        nb_front: int,
-        nb_back: int,
-        augment: bool = True,
-    ) -> Tuple[List[pp.LieTensor], List[pp.LieTensor], List[str], List[str], np.ndarray,]:
-        assert self.has_data, f"DistanceSchemeIdx for distance {self.distance} has no data"
-
-        # get all pairs that are within the fov
-        idx_fov = np.where(self.pair_within_fov)[0]
-        idx_front = np.where(self.pair_front_of_robot)[0]
-        idx_back = np.where(self.pair_behind_robot)[0]
-        idx_augment = []
-
-        # augment pairs if not enough
-        if len(idx_fov) == 0:
-            print(f"[WARNING] for distance {self.distance} no 'within_fov'" " samples")
-            idx_fov = np.array([], dtype=np.int64)
-        elif len(idx_fov) < nb_fov:
-            print(
-                f"[INFO] for distance {self.distance} not enough 'within_fov'"
-                f" samples ({len(idx_fov)} instead of {nb_fov})"
-            )
-            if augment:
-                idx_augment.append(
-                    np.random.choice(
-                        idx_fov,
-                        min(len(idx_fov), nb_fov - len(idx_fov)),
-                        replace=(nb_fov - len(idx_fov) > len(idx_fov)),
-                    )
-                )
-            else:
-                idx_fov = np.random.choice(idx_fov, len(idx_fov), replace=False)
-        else:
-            idx_fov = np.random.choice(idx_fov, nb_fov, replace=False)
-
-        if len(idx_front) == 0:
-            print(f"[WARNING] for distance {self.distance} no 'front_of_robot'" " samples")
-            idx_front = np.array([], dtype=np.int64)
-        elif len(idx_front) < nb_front:
-            print(
-                f"[INFO] for distance {self.distance} not enough"
-                f" 'front_of_robot' samples ({len(idx_front)} instead of"
-                f" {nb_front})"
-            )
-            if augment:
-                idx_augment.append(
-                    np.random.choice(
-                        idx_front,
-                        min(len(idx_front), nb_front - len(idx_front)),
-                        replace=(nb_front - len(idx_front) > len(idx_front)),
-                    )
-                )
-            else:
-                idx_front = np.random.choice(idx_front, len(idx_front), replace=False)
-        else:
-            idx_front = np.random.choice(idx_front, nb_front, replace=False)
-
-        if len(idx_back) == 0:
-            print(f"[WARNING] for distance {self.distance} no 'behind_robot'" " samples")
-            idx_back = np.array([], dtype=np.int64)
-        elif len(idx_back) < nb_back:
-            print(
-                f"[INFO] for distance {self.distance} not enough"
-                f" 'behind_robot' samples ({len(idx_back)} instead of"
-                f" {nb_back})"
-            )
-            if augment:
-                idx_augment.append(
-                    np.random.choice(
-                        idx_back,
-                        min(len(idx_back), nb_back - len(idx_back)),
-                        replace=(nb_back - len(idx_back) > len(idx_back)),
-                    )
-                )
-            else:
-                idx_back = np.random.choice(idx_back, len(idx_back), replace=False)
-        else:
-            idx_back = np.random.choice(idx_back, nb_back, replace=False)
-
-        idx = np.hstack([idx_fov, idx_front, idx_back])
-
-        # stack buffers
-        odom = torch.stack(self.odom_list)
-        goal = torch.stack(self.goal_list)
-
-        # get pairs
-        if idx_augment:
-            idx_augment = np.hstack(idx_augment)
-            odom = torch.vstack([odom[idx], odom[idx_augment]])
-            goal = torch.vstack(
-                [
-                    goal[idx],
-                    goal[idx_augment].tensor() * torch.tensor([[1, -1, 1, 1, 1, 1, 1]]),
-                ]
-            )
-            depth_img_list = [self.depth_img_list[j] for j in idx.tolist()] + [
-                self.depth_img_list[i] for i in idx_augment.tolist()
-            ]
-            sem_rgb_img_list = [self.sem_rgb_img_list[j] for j in idx.tolist()] + [
-                self.sem_rgb_img_list[i] for i in idx_augment.tolist()
-            ]
-            augment = np.hstack([np.zeros(len(idx)), np.ones(len(idx_augment))])
-            return odom, goal, depth_img_list, sem_rgb_img_list, augment
-        else:
-            return (
-                odom[idx],
-                goal[idx],
-                [self.depth_img_list[j] for j in idx.tolist()],
-                [self.sem_rgb_img_list[j] for j in idx.tolist()],
-                np.zeros(len(idx)),
-            )
-
-
-class PlannerDataGenerator(Dataset):
-    debug = False
-    mesh_size = 0.5
-
-    def __init__(
-        self,
-        cfg: DataCfg,
-        root: str,
-        semantics: bool = False,
-        rgb: bool = False,
-        cost_map: CostMapPCD = None,
-    ) -> None:
-        print(
-            f"[INFO] PlannerDataGenerator init with semantics={semantics},"
-            f" rgb={rgb} for ENV {os.path.split(root)[-1]}"
-        )
-        # super().__init__()
-        # set parameters
-        self._cfg = cfg
-        self.root = root
-        self.cost_map = cost_map
-        self.semantics = semantics
-        self.rgb = rgb
-        assert not (self.semantics and self.rgb), "semantics and rgb cannot be true at the same time"
-
-        # init list for final odom, goal and img mapping
-        self.depth_filename_list = []
-        self.sem_rgb_filename_list = []
-        self.odom_depth: torch.Tensor = None
-        self.goal: torch.Tensor = None
-        self.pair_outside: np.ndarray = None
-        self.pair_difficult: np.ndarray = None
-        self.pair_augment: np.ndarray = None
-        self.pair_within_fov: np.ndarray = None
-        self.pair_front_of_robot: np.ndarray = None
-        self.odom_array_sem_rgb: pp.LieTensor = None
-        self.odom_array_depth: pp.LieTensor = None
-
-        self.odom_used: int = 0
-        self.odom_no_suitable_goals: int = 0
-
-        # set parameters
-        self._device = "cuda:0" if torch.cuda.is_available() else "cpu"
-
-        # get odom data and filter
-        self.load_odom()
-        self.filter_obs_inflation()
-
-        # noise edges in depth image --> real world Realsense difficulties along edges
-        if self._cfg.noise_edges:
-            self.noise_edges()
-
-        # find odom-goal pairs
-        self.get_odom_goal_pairs()
-        return
-
-    """LOAD HELPER FUNCTIONS"""
-
-    def load_odom(self) -> None:
-        print("[INFO] Loading odom data...", end=" ")
-        # load odom of every image
-        odom_path = os.path.join(self.root, f"camera_extrinsic{self._cfg.depth_suffix}.txt")
-        odom_np = np.loadtxt(odom_path, delimiter=",")
-        self.odom_array_depth = pp.SE3(odom_np)
-
-        if self.semantics or self.rgb:
-            odom_path = os.path.join(self.root, f"camera_extrinsic{self._cfg.sem_suffix}.txt")
-            odom_np = np.loadtxt(odom_path, delimiter=",")
-            self.odom_array_sem_rgb = pp.SE3(odom_np)
-
-        if self.debug:
-            # plot odom
-            small_sphere = o3d.geometry.TriangleMesh.create_sphere(self.mesh_size / 3.0)  # successful trajectory points
-            small_sphere.paint_uniform_color([0.4, 1.0, 0.1])
-            odom_vis_list = []
-
-            for i in range(len(self.odom_array_depth)):
-                odom_vis_list.append(
-                    copy.deepcopy(small_sphere).translate(
-                        (
-                            self.odom_array_depth[i, 0],
-                            self.odom_array_depth[i, 1],
-                            self.odom_array_depth[i, 2],
-                        )
-                    )
-                )
-            odom_vis_list.append(self.cost_map.pcd_tsdf)
-
-            o3d.visualization.draw_geometries(odom_vis_list)
-        print("DONE!")
-        return
-
-    def load_images(self, root_path, domain: str = "depth"):
-        img_path = os.path.join(root_path, domain)
-        assert os.path.isdir(img_path), f"Image directory path '{img_path}' does not exist for domain" f" {domain}"
-        assert len(os.listdir(img_path)) > 0, f"Image directory '{img_path}' is empty for domain {domain}"
-
-        # use the more precise npy files if available
-        img_filename_list = [str(s) for s in Path(img_path).rglob("*.npy")]
-        if len(img_filename_list) == 0:
-            img_filename_list = [str(s) for s in Path(img_path).rglob("*.png")]
-
-        if domain == "depth":
-            img_filename_list.sort(key=lambda x: int(x.split("/")[-1][: -(4 + len(self._cfg.depth_suffix))]))
-        else:
-            img_filename_list.sort(key=lambda x: int(x.split("/")[-1][: -(4 + len(self._cfg.sem_suffix))]))
-        return img_filename_list
-
-    """FILTER HELPER FUNCTIONS"""
-
-    def filter_obs_inflation(self) -> None:
-        """
-        Filter odom points within the inflation range of the obstacles in the cost map.
-
-        Filtering only performed according to the position of the depth camera, due to the close position of depth and semantic camera.
-        """
-        print(
-            ("[INFO] Filter odom points within the inflation range of the" " obstacles in the cost map..."),
-            end="",
-        )
-
-        norm_inds, _ = self.cost_map.Pos2Ind(self.odom_array_depth[:, None, :3])
-        cost_grid = self.cost_map.cost_array.T.expand(self.odom_array_depth.shape[0], 1, -1, -1)
-        norm_inds = norm_inds.to(cost_grid.device)
-        oloss_M = (
-            F.grid_sample(
-                cost_grid,
-                norm_inds[:, None, :, :],
-                mode="bicubic",
-                padding_mode="border",
-                align_corners=False,
-            )
-            .squeeze(1)
-            .squeeze(1)
-        )
-        oloss_M = oloss_M.to(torch.float32).to("cpu")
-        if self.semantics or self.rgb:
-            points_free_space = oloss_M < self._cfg.obs_cost_height + abs(
-                self.cost_map.cfg.sem_cost_map.negative_reward
-            )
-        else:
-            points_free_space = oloss_M < self._cfg.obs_cost_height
-
-        if self._cfg.carla:
-            # for CARLA filter large open spaces
-            # Extract the x and y coordinates from the odom poses
-            x_coords = self.odom_array_depth.tensor()[:, 0]
-            y_coords = self.odom_array_depth.tensor()[:, 1]
-
-            # Filter the point cloud based on the square coordinates
-            mask_area_1 = (y_coords >= 100.5) & (y_coords <= 325.5) & (x_coords >= 208.9) & (x_coords <= 317.8)
-            mask_area_2 = (y_coords >= 12.7) & (y_coords <= 80.6) & (x_coords >= 190.3) & (x_coords <= 315.8)
-            mask_area_3 = (y_coords >= 10.0) & (y_coords <= 80.0) & (x_coords >= 123.56) & (x_coords <= 139.37)
-
-            combined_mask = mask_area_1 | mask_area_2 | mask_area_3 | ~points_free_space.squeeze(1)
-            points_free_space = (~combined_mask).unsqueeze(1)
-
-        if self.debug:
-            # plot odom
-            odom_vis_list = []
-            small_sphere = o3d.geometry.TriangleMesh.create_sphere(self.mesh_size / 3.0)  # successful trajectory points
-
-            for i in range(len(self.odom_array_depth)):
-                if round(oloss_M[i].item(), 3) == 0.0:
-                    small_sphere.paint_uniform_color([0.4, 0.1, 1.0])  # violette
-                elif points_free_space[i]:
-                    small_sphere.paint_uniform_color([0.4, 1.0, 0.1])  # green
-                else:
-                    small_sphere.paint_uniform_color([1.0, 0.4, 0.1])  # red
-                if self.semantics or self.rgb:
-                    z_height = self.odom_array_depth.tensor()[i, 2] + abs(
-                        self.cost_map.cfg.sem_cost_map.negative_reward
-                    )
-                else:
-                    z_height = self.odom_array_depth.tensor()[i, 2]
-
-                odom_vis_list.append(
-                    copy.deepcopy(small_sphere).translate(
-                        (
-                            self.odom_array_depth.tensor()[i, 0],
-                            self.odom_array_depth.tensor()[i, 1],
-                            z_height,
-                        )
-                    )
-                )
-
-            odom_vis_list.append(self.cost_map.pcd_tsdf)
-            o3d.visualization.draw_geometries(odom_vis_list)
-
-        nb_odom_point_prev = len(self.odom_array_depth)
-        self.odom_array_depth = self.odom_array_depth[points_free_space.squeeze()]
-        self.nb_odom_points = self.odom_array_depth.shape[0]
-
-        # load depth image files as name list
-        depth_filename_list = self.load_images(self.root, "depth")
-        self.depth_filename_list = [
-            depth_filename_list[i] for i in range(len(depth_filename_list)) if points_free_space[i]
-        ]
-
-        if self.semantics:
-            self.odom_array_sem_rgb = self.odom_array_sem_rgb[points_free_space.squeeze()]
-            sem_rgb_filename_list = self.load_images(self.root, "semantics")
-            self.sem_rgb_filename_list = [
-                sem_rgb_filename_list[i] for i in range(len(sem_rgb_filename_list)) if points_free_space[i]
-            ]
-        elif self.rgb:
-            self.odom_array_sem_rgb = self.odom_array_sem_rgb[points_free_space.squeeze()]
-            sem_rgb_filename_list = self.load_images(self.root, "rgb")
-            self.sem_rgb_filename_list = [
-                sem_rgb_filename_list[i] for i in range(len(sem_rgb_filename_list)) if points_free_space[i]
-            ]
-
-        assert len(self.depth_filename_list) != 0, "No depth images left after filtering"
-        print("DONE!")
-        print(
-            "[INFO] odom points outside obs inflation :"
-            f" \t{self.nb_odom_points} ({round(self.nb_odom_points/nb_odom_point_prev*100, 2)} %)"
-        )
-
-        return
-
-    """GENERATE SAMPLES"""
-
-    def get_odom_goal_pairs(self) -> None:
-        # get fov
-        self.get_intrinscs_and_fov()
-        # construct graph
-        self.get_graph()
-        # get pairs
-        self.get_pairs()
-
-        # free up memory
-        self.odom_array_depth = self.odom_array_sem_rgb = None
-        return
-
-    def compute_ratios(self) -> Tuple[float, float, float]:
-        # ratio of general samples distribution
-        num_within_fov = self.odom_depth[self.pair_within_fov].shape[0]
-        ratio_fov = num_within_fov / self.odom_depth.shape[0]
-        ratio_front = np.sum(self.pair_front_of_robot) / self.odom_depth.shape[0]
-        ratio_back = 1 - ratio_front - ratio_fov
-
-        # samples ratios within fov samples
-        num_easy = (
-            num_within_fov
-            - self.pair_difficult[self.pair_within_fov].sum().item()
-            - self.pair_outside[self.pair_within_fov].sum().item()
-        )
-        ratio_easy = num_easy / num_within_fov
-        ratio_hard = self.pair_difficult[self.pair_within_fov].sum().item() / num_within_fov
-        ratio_outside = self.pair_outside[self.pair_within_fov].sum().item() / num_within_fov
-        return (
-            ratio_fov,
-            ratio_front,
-            ratio_back,
-            ratio_easy,
-            ratio_hard,
-            ratio_outside,
-        )
-
-    def get_intrinscs_and_fov(self) -> None:
-        # load intrinsics
-        intrinsic_path = os.path.join(self.root, "intrinsics.txt")
-        P = np.loadtxt(intrinsic_path, delimiter=",")  # assumes ROS P matrix
-        self.K_depth = P[0].reshape(3, 4)[:3, :3]
-        self.K_sem_rgb = P[1].reshape(3, 4)[:3, :3]
-
-        self.alpha_fov = 2 * math.atan(self.K_depth[0, 0] / self.K_depth[0, 2])
-        return
-
-    def get_graph(self) -> None:
-        num_connections = 3
-        num_intermediate = 3
-
-        # get occpuancy map from tsdf map
-        cost_array = self.cost_map.tsdf_array.cpu().numpy()
-        if self.semantics or self.rgb:
-            occupancy_map = (
-                cost_array > self._cfg.obs_cost_height + abs(self.cost_map.cfg.sem_cost_map.negative_reward)
-            ).astype(np.uint8)
-        else:
-            occupancy_map = (cost_array > self._cfg.obs_cost_height).astype(np.uint8)
-        # construct kdtree to find nearest neighbors of points
-        odom_points = self.odom_array_depth.data[:, :2].data.cpu().numpy()
-        kdtree = KDTree(odom_points)
-        _, nearest_neighbors_idx = kdtree.query(odom_points, k=num_connections + 1, workers=-1)
-        # remove first neighbor as it is the point itself
-        nearest_neighbors_idx = nearest_neighbors_idx[:, 1:]
-
-        # define origin and neighbor points
-        origin_point = np.repeat(odom_points, repeats=num_connections, axis=0)
-        neighbor_points = odom_points[nearest_neighbors_idx, :].reshape(-1, 2)
-        # interpolate points between origin and neighbor points
-        x_interp = (
-            origin_point[:, None, 0]
-            + (neighbor_points[:, 0] - origin_point[:, 0])[:, None]
-            * np.linspace(0, 1, num=num_intermediate + 1, endpoint=False)[1:]
-        )
-        y_interp = (
-            origin_point[:, None, 1]
-            + (neighbor_points[:, 1] - origin_point[:, 1])[:, None]
-            * np.linspace(0, 1, num=num_intermediate + 1, endpoint=False)[1:]
-        )
-        inter_points = np.stack((x_interp.reshape(-1), y_interp.reshape(-1)), axis=1)
-        # get the indices of the interpolated points in the occupancy map
-        occupancy_idx = (
-            inter_points - np.array([self.cost_map.cfg.x_start, self.cost_map.cfg.y_start])
-        ) / self.cost_map.cfg.general.resolution
-
-        # check occupancy for collisions at the interpolated points
-        collision = occupancy_map[
-            occupancy_idx[:, 0].astype(np.int64),
-            occupancy_idx[:, 1].astype(np.int64),
-        ]
-        collision = np.any(collision.reshape(-1, num_intermediate), axis=1)
-
-        # get edge indices
-        idx_edge_start = np.repeat(np.arange(odom_points.shape[0]), repeats=num_connections, axis=0)
-        idx_edge_end = nearest_neighbors_idx.reshape(-1)
-
-        # filter collision edges
-        idx_edge_end = idx_edge_end[~collision]
-        idx_edge_start = idx_edge_start[~collision]
-
-        # init graph
-        self.graph = nx.Graph()
-        # add nodes with position attributes
-        self.graph.add_nodes_from(list(range(odom_points.shape[0])))
-        pos_attr = {i: {"pos": odom_points[i]} for i in range(odom_points.shape[0])}
-        nx.set_node_attributes(self.graph, pos_attr)
-        # add edges with distance attributes
-        self.graph.add_edges_from(list(map(tuple, np.stack((idx_edge_start, idx_edge_end), axis=1))))
-        distance_attr = {
-            (i, j): {"distance": np.linalg.norm(odom_points[i] - odom_points[j])}
-            for i, j in zip(idx_edge_start, idx_edge_end)
-        }
-        nx.set_edge_attributes(self.graph, distance_attr)
-
-        # DEBUG
-        if self.debug:
-            import matplotlib.pyplot as plt
-
-            nx.draw_networkx(
-                self.graph,
-                nx.get_node_attributes(self.graph, "pos"),
-                node_size=10,
-                with_labels=False,
-                node_color=[0.0, 1.0, 0.0],
-            )
-            plt.show()
-        return
-
-    def get_pairs(self):
-        # iterate over all odom points and find goal points
-        self.odom_no_suitable_goals = 0
-        self.odom_used = 0
-
-        # init semantic warp parameters
-        if self.semantics or self.rgb:
-            # compute pixel tensor
-            depth_filename = self.depth_filename_list[0]
-            depth_img = self._load_depth_image(depth_filename)
-            x_nums, y_nums = depth_img.shape
-            self.pix_depth_cam_frame = self.compute_pixel_tensor(x_nums, y_nums, self.K_depth)
-            # make dir
-            os.makedirs(os.path.join(self.root, "img_warp"), exist_ok=True)
-
-        # get distances between odom and goal points
-        odom_goal_distances = dict(
-            nx.all_pairs_dijkstra_path_length(
-                self.graph,
-                cutoff=self._cfg.max_goal_distance,
-                weight="distance",
-            )
-        )
-
-        # init dataclass for each entry in the distance scheme
-        self.category_scheme_pairs: Dict[float, DistanceSchemeIdx] = {
-            distance: DistanceSchemeIdx(distance=distance) for distance in self._cfg.distance_scheme.keys()
-        }
-
-        # iterate over all odom points
-        for odom_idx in tqdm(range(self.nb_odom_points), desc="Start-End Pairs Generation"):
-            odom = self.odom_array_depth[odom_idx]
-
-            # transform all odom points to current odom frame
-            goals = pp.Inv(odom) @ self.odom_array_depth
-            # categorize goals
-            (
-                within_fov,
-                front_of_robot,
-                behind_robot,
-            ) = self.get_goal_categories(
-                goals
-            )  # returns goals in odom frame
-
-            # filter odom if no suitable goals within the fov are found
-            if within_fov.sum() == 0:
-                self.odom_no_suitable_goals += 1
-                continue
-            self.odom_used += 1
-
-            if self.semantics or self.rgb:
-                # semantic warp
-                img_new_path = self._get_overlay_img(odom_idx)
-            else:
-                img_new_path = None
-
-            # get pair according to distance scheme for each category
-            self.reduce_pairs(
-                odom_idx,
-                goals,
-                within_fov,
-                odom_goal_distances[odom_idx],
-                img_new_path,
-                within_fov=True,
-            )
-            self.reduce_pairs(
-                odom_idx,
-                goals,
-                behind_robot,
-                odom_goal_distances[odom_idx],
-                img_new_path,
-                behind_robot=True,
-            )
-            self.reduce_pairs(
-                odom_idx,
-                goals,
-                front_of_robot,
-                odom_goal_distances[odom_idx],
-                img_new_path,
-                front_of_robot=True,
-            )
-
-            # DEBUG
-            if self.debug:
-                # plot odom
-                small_sphere = o3d.geometry.TriangleMesh.create_sphere(
-                    self.mesh_size / 3.0
-                )  # successful trajectory points
-                odom_vis_list = []
-                goal_odom = odom @ goals
-                hit_pcd = (goal_odom).cpu().numpy()[:, :3]
-                for idx, pts in enumerate(hit_pcd):
-                    if within_fov[idx]:
-                        small_sphere.paint_uniform_color([0.4, 1.0, 0.1])
-                    elif front_of_robot[idx]:
-                        small_sphere.paint_uniform_color([0.0, 0.5, 0.5])
-                    else:
-                        small_sphere.paint_uniform_color([0.0, 0.1, 1.0])
-                    odom_vis_list.append(copy.deepcopy(small_sphere).translate((pts[0], pts[1], pts[2])))
-
-                # viz cost map
-                odom_vis_list.append(self.cost_map.pcd_tsdf)
-
-                # field of view visualization
-                fov_vis_length = 0.75  # length of the fov visualization plane in meters
-                fov_vis_pt_right = odom @ pp.SE3(
-                    [
-                        fov_vis_length * np.cos(self.alpha_fov / 2),
-                        fov_vis_length * np.sin(self.alpha_fov / 2),
-                        0,
-                        0,
-                        0,
-                        0,
-                        1,
-                    ]
-                )
-                fov_vis_pt_left = odom @ pp.SE3(
-                    [
-                        fov_vis_length * np.cos(self.alpha_fov / 2),
-                        -fov_vis_length * np.sin(self.alpha_fov / 2),
-                        0,
-                        0,
-                        0,
-                        0,
-                        1,
-                    ]
-                )
-                fov_vis_pt_right = fov_vis_pt_right.numpy()[:3]
-                fov_vis_pt_left = fov_vis_pt_left.numpy()[:3]
-                fov_mesh = o3d.geometry.TriangleMesh(
-                    vertices=o3d.utility.Vector3dVector(
-                        np.array(
-                            [
-                                odom.data.cpu().numpy()[:3],
-                                fov_vis_pt_right,
-                                fov_vis_pt_left,
-                            ]
-                        )
-                    ),
-                    triangles=o3d.utility.Vector3iVector(np.array([[2, 1, 0]])),
-                )
-                fov_mesh.paint_uniform_color([1.0, 0.5, 0.0])
-                odom_vis_list.append(fov_mesh)
-
-                # odom viz
-                small_sphere.paint_uniform_color([1.0, 0.0, 0.0])
-                odom_vis_list.append(
-                    copy.deepcopy(small_sphere).translate(
-                        (
-                            odom.data[0].item(),
-                            odom.data[1].item(),
-                            odom.data[2].item(),
-                        )
-                    )
-                )
-
-                # plot goal
-                o3d.visualization.draw_geometries(odom_vis_list)
-
-        if self.debug:
-            small_sphere = o3d.geometry.TriangleMesh.create_sphere(self.mesh_size / 3.0)  # successful trajectory points
-            odom_vis_list = []
-
-            for distance in self._cfg.distance_scheme.keys():
-                odoms = torch.vstack(self.category_scheme_pairs[distance].odom_list)
-                odoms = odoms.tensor().cpu().numpy()[:, :3]
-                for idx, odom in enumerate(odoms):
-                    odom_vis_list.append(copy.deepcopy(small_sphere).translate((odom[0], odom[1], odom[2])))
-                    if idx > 10:
-                        break
-            # viz cost map
-            odom_vis_list.append(self.cost_map.pcd_tsdf)
-
-            # plot goal
-            o3d.visualization.draw_geometries(odom_vis_list)
-
-        return
-
-    def reduce_pairs(
-        self,
-        odom_idx: int,
-        goals: pp.LieTensor,
-        decision_tensor: torch.Tensor,
-        odom_distances: dict,
-        warp_img_path: Optional[str],
-        within_fov: bool = False,
-        behind_robot: bool = False,
-        front_of_robot: bool = False,
-    ):
-        # remove all goals depending on the decision tensor from the odom_distances dict
-        keep_distance_entries = decision_tensor[list(odom_distances.keys())]
-        distances = np.array(list(odom_distances.values()))[keep_distance_entries.numpy()]
-        goal_idx = np.array(list(odom_distances.keys()))[keep_distance_entries.numpy()]
-
-        # max distance enforced odom_distances, here enforce min distance
-        within_distance_idx = distances > self._cfg.min_goal_distance
-        goal_idx = goal_idx[within_distance_idx]
-        distances = distances[within_distance_idx]
-
-        # check if there are any goals left
-        if len(goal_idx) == 0:
-            return
-
-        # select the goal according to the distance_scheme
-        for distance in self._cfg.distance_scheme.keys():
-            # select nbr_samples from goals within distance
-            within_curr_distance_idx = distances < distance
-            if sum(within_curr_distance_idx) == 0:
-                continue
-            selected_idx = np.random.choice(
-                goal_idx[within_curr_distance_idx],
-                min(3, sum(within_curr_distance_idx)),
-                replace=False,
-            )
-            # remove the selected goals from the list for further selection
-            distances = distances[~within_curr_distance_idx]
-            goal_idx = goal_idx[~within_curr_distance_idx]
-
-            for idx in selected_idx:
-                self.category_scheme_pairs[distance].update_buffers(
-                    odom=self.odom_array_depth[odom_idx],
-                    goal=goals[idx],
-                    within_fov=within_fov,
-                    front_of_robot=front_of_robot,
-                    behind_robot=behind_robot,
-                    depth_filename=self.depth_filename_list[odom_idx],
-                    sem_rgb_filename=warp_img_path,
-                )
-
-    def get_goal_categories(self, goal_odom_frame: pp.LieTensor):
-        """
-        Decide which of the samples are within the fov, in front of the robot or behind the robot.
-        """
-        # get if odom-goal is within fov or outside the fov but still in front of the robot
-        goal_angle = abs(torch.atan2(goal_odom_frame.data[:, 1], goal_odom_frame.data[:, 0]))
-        within_fov = goal_angle < self.alpha_fov / 2 * self._cfg.fov_scale
-        front_of_robot = goal_angle < torch.pi / 2
-        front_of_robot[within_fov] = False
-
-        behind_robot = ~front_of_robot.clone()
-        behind_robot[within_fov] = False
-
-        return within_fov, front_of_robot, behind_robot
-
-    """SPLIT HELPER FUNCTIONS"""
-
-    def split_samples(
-        self,
-        test_dataset: PlannerData,
-        train_dataset: Optional[PlannerData] = None,
-        generate_split: bool = False,
-        ratio_fov_samples: Optional[float] = None,
-        ratio_front_samples: Optional[float] = None,
-        ratio_back_samples: Optional[float] = None,
-        allow_augmentation: bool = True,
-    ) -> None:
-        # check if ratios are given or defaults are used
-        ratio_fov_samples = ratio_fov_samples if ratio_fov_samples is not None else self._cfg.ratio_fov_samples
-        ratio_front_samples = ratio_front_samples if ratio_front_samples is not None else self._cfg.ratio_front_samples
-        ratio_back_samples = ratio_back_samples if ratio_back_samples is not None else self._cfg.ratio_back_samples
-        assert round(ratio_fov_samples + ratio_front_samples + ratio_back_samples, 2) == 1.0, (
-            "Sample ratios must sum up to 1.0, currently"
-            f" {ratio_back_samples + ratio_front_samples + ratio_fov_samples}"
-        )
-
-        # max sample number
-        if self._cfg.max_train_pairs:
-            max_sample_number = min(
-                int(self._cfg.max_train_pairs / self._cfg.ratio),
-                int(self.odom_used * self._cfg.pairs_per_image),
-            )
-        else:
-            max_sample_number = int(self.odom_used * self._cfg.pairs_per_image)
-
-        # init buffers
-        odom = torch.zeros((max_sample_number, 7), dtype=torch.float32)
-        goal = torch.zeros((max_sample_number, 7), dtype=torch.float32)
-        augment_samples = np.zeros((max_sample_number), dtype=bool)
-        depth_filename = []
-        sem_rgb_filename = []
-
-        current_idx = 0
-        for distance, distance_percentage in self._cfg.distance_scheme.items():
-            if not self.category_scheme_pairs[distance].has_data:
-                print(f"[WARN] No samples for distance {distance} in ENV" f" {os.path.split(self.root)[-1]}")
-                continue
-
-            # get number of samples
-            buffer_data = self.category_scheme_pairs[distance].get_data(
-                nb_fov=int(ratio_fov_samples * distance_percentage * max_sample_number),
-                nb_front=int(ratio_front_samples * distance_percentage * max_sample_number),
-                nb_back=int(ratio_back_samples * distance_percentage * max_sample_number),
-                augment=allow_augmentation,
-            )
-            nb_samples = buffer_data[0].shape[0]
-
-            # add to buffers
-            odom[current_idx : current_idx + nb_samples] = buffer_data[0]
-            goal[current_idx : current_idx + nb_samples] = buffer_data[1]
-            depth_filename += buffer_data[2]
-            sem_rgb_filename += buffer_data[3]
-            augment_samples[current_idx : current_idx + nb_samples] = buffer_data[4]
-
-            current_idx += nb_samples
-
-        # cut off unused space
-        odom = odom[:current_idx]
-        goal = goal[:current_idx]
-        augment_samples = augment_samples[:current_idx]
-
-        # print data mix
-        print(
-            f"[INFO] datamix containing {odom.shape[0]} suitable odom-goal"
-            " pairs: \n"
-            "\t fov               :"
-            f" \t{int(odom.shape[0] * ratio_fov_samples)  } ({round(ratio_fov_samples*100, 2)} %) \n"
-            "\t front of robot    :"
-            f" \t{int(odom.shape[0] * ratio_front_samples)} ({round(ratio_front_samples*100, 2)} %) \n"
-            "\t back of robot     :"
-            f" \t{int(odom.shape[0] * ratio_back_samples) } ({round(ratio_back_samples*100, 2)} %) \n"
-            "from"
-            f" {self.odom_used} ({round(self.odom_used/self.nb_odom_points*100, 2)} %)"
-            " different starting points where \n"
-            "\t non-suitable filter:"
-            f" {self.odom_no_suitable_goals} ({round(self.odom_no_suitable_goals/self.nb_odom_points*100, 2)} %)"
-        )
-
-        # generate split
-        idx = np.arange(odom.shape[0])
-        if generate_split:
-            train_index = sample(idx.tolist(), int(len(idx) * self._cfg.ratio))
-            idx = np.delete(idx, train_index)
-
-            train_dataset.update_buffers(
-                depth_filename=[depth_filename[i] for i in train_index],
-                sem_rgb_filename=([sem_rgb_filename[i] for i in train_index] if (self.semantics or self.rgb) else None),
-                odom=odom[train_index],
-                goal=goal[train_index],
-                pair_augment=augment_samples[train_index],
-            )
-            train_dataset.set_fov(self.alpha_fov)
-
-        test_dataset.update_buffers(
-            depth_filename=[depth_filename[i] for i in idx],
-            sem_rgb_filename=([sem_rgb_filename[i] for i in idx] if (self.semantics or self.rgb) else None),
-            odom=odom[idx],
-            goal=goal[idx],
-            pair_augment=augment_samples[idx],
-        )
-        test_dataset.set_fov(self.alpha_fov)
-
-        return
-
-    """ Warp semantic on depth image helper functions"""
-
-    @staticmethod
-    def compute_pixel_tensor(x_nums: int, y_nums: int, K_depth: np.ndarray) -> None:
-        # get image plane mesh grid
-        pix_u = np.arange(0, y_nums)
-        pix_v = np.arange(0, x_nums)
-        grid = np.meshgrid(pix_u, pix_v)
-        pixels = np.vstack(list(map(np.ravel, grid))).T
-        pixels = np.hstack([pixels, np.ones((len(pixels), 1))])  # add ones for 3D coordinates
-
-        # transform to camera frame
-        k_inv = np.linalg.inv(K_depth)
-        pix_cam_frame = np.matmul(k_inv, pixels.T)
-        # reorder to be in "robotics" axis order (x forward, y left, z up)
-        return pix_cam_frame[[2, 0, 1], :].T * np.array([1, -1, -1])
-
-    def _load_depth_image(self, depth_filename):
-        if depth_filename.endswith(".png"):
-            depth_image = Image.open(depth_filename)
-            if self._cfg.real_world_data:
-                depth_image = np.array(depth_image.transpose(PIL.Image.ROTATE_180))
-            else:
-                depth_image = np.array(depth_image)
-        else:
-            depth_image = np.load(depth_filename)
-
-        depth_image[~np.isfinite(depth_image)] = 0.0
-        depth_image = (depth_image / self._cfg.depth_scale).astype("float32")
-        depth_image[depth_image > self._cfg.max_depth] = 0.0
-        return depth_image
-
-    @staticmethod
-    def compute_overlay(
-        pose_dep,
-        pose_sem,
-        depth_img,
-        sem_rgb_image,
-        pix_depth_cam_frame,
-        K_sem_rgb,
-    ):
-        # get 3D points of depth image
-        rot = tf.Rotation.from_quat(pose_dep[3:]).as_matrix()
-        dep_im_reshaped = depth_img.reshape(
-            -1, 1
-        )  # flip s.t. start in lower left corner of image as (0,0) -> has to fit to the pixel tensor
-        points = dep_im_reshaped * (rot @ pix_depth_cam_frame.T).T + pose_dep[:3]
-
-        # transform points to semantic camera frame
-        points_sem_cam_frame = (tf.Rotation.from_quat(pose_sem[3:]).as_matrix().T @ (points - pose_sem[:3]).T).T
-        # normalize points
-        points_sem_cam_frame_norm = points_sem_cam_frame / points_sem_cam_frame[:, 0][:, np.newaxis]
-        # reorder points be camera convention (z-forward)
-        points_sem_cam_frame_norm = points_sem_cam_frame_norm[:, [1, 2, 0]] * np.array([-1, -1, 1])
-        # transform points to pixel coordinates
-        pixels = (K_sem_rgb @ points_sem_cam_frame_norm.T).T
-        # filter points outside of image
-        filter_idx = (
-            (pixels[:, 0] >= 0)
-            & (pixels[:, 0] < sem_rgb_image.shape[1])
-            & (pixels[:, 1] >= 0)
-            & (pixels[:, 1] < sem_rgb_image.shape[0])
-        )
-        # get semantic annotation
-        sem_annotation = np.zeros((pixels.shape[0], 3), dtype=np.uint8)
-        sem_annotation[filter_idx] = sem_rgb_image[
-            pixels[filter_idx, 1].astype(int),
-            pixels[filter_idx, 0].astype(int),
-        ]
-        # reshape to image
-
-        return sem_annotation.reshape(depth_img.shape[0], depth_img.shape[1], 3)
-
-    def _get_overlay_img(self, odom_idx):
-        # get corresponding filenames
-        depth_filename = self.depth_filename_list[odom_idx]
-        sem_rgb_filename = self.sem_rgb_filename_list[odom_idx]
-
-        # load semantic and depth image and get their poses
-        depth_img = self._load_depth_image(depth_filename)
-        sem_rgb_image = Image.open(sem_rgb_filename)
-        if self._cfg.real_world_data:
-            sem_rgb_image = np.array(sem_rgb_image.transpose(PIL.Image.ROTATE_180))
-        else:
-            sem_rgb_image = np.array(sem_rgb_image)
-        pose_dep = self.odom_array_depth[odom_idx].data.cpu().numpy()
-        pose_sem = self.odom_array_sem_rgb[odom_idx].data.cpu().numpy()
-
-        sem_rgb_image_warped = self.compute_overlay(
-            pose_dep,
-            pose_sem,
-            depth_img,
-            sem_rgb_image,
-            self.pix_depth_cam_frame,
-            self.K_sem_rgb,
-        )
-        assert sem_rgb_image_warped.dtype == np.uint8, "sem_rgb_image_warped has to be uint8"
-
-        # DEBUG
-        if self.debug:
-            import matplotlib.pyplot as plt
-
-            f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))
-            ax1.imshow(depth_img)
-            ax2.imshow(sem_rgb_image_warped / 255)
-            ax3.imshow(sem_rgb_image)
-            # ax3.imshow(depth_img)
-            # ax3.imshow(sem_rgb_image_warped / 255, alpha=0.5)
-            ax1.axis("off")
-            ax2.axis("off")
-            ax3.axis("off")
-            plt.show()
-
-        # save semantic image under the new path
-        sem_rgb_filename = os.path.split(sem_rgb_filename)[1]
-        sem_rgb_image_path = os.path.join(self.root, "img_warp", sem_rgb_filename)
-        sem_rgb_image_warped = cv2.cvtColor(sem_rgb_image_warped, cv2.COLOR_RGB2BGR)  # convert to BGR for cv2
-        assert cv2.imwrite(sem_rgb_image_path, sem_rgb_image_warped)
-
-        return sem_rgb_image_path
-
-    """Noise Edges helper functions"""
-
-    def noise_edges(self):
-        """
-        Along the edges in the depth image, set the values to 0.
-        Mimics the real-world behavior where RealSense depth cameras have difficulties along edges.
-        """
-        print("[INFO] Adding noise to edges in depth images ...", end=" ")
-        new_depth_filename_list = []
-        # create new directory
-        depth_noise_edge_dir = os.path.join(self.root, "depth_noise_edges")
-        os.makedirs(depth_noise_edge_dir, exist_ok=True)
-
-        for depth_filename in self.depth_filename_list:
-            depth_img = self._load_depth_image(depth_filename)
-            # Perform Canny edge detection
-            image = ((depth_img / depth_img.max()) * 255).astype(np.uint8)  # convert to CV_U8 format
-            edges = cv2.Canny(image, self._cfg.edge_threshold, self._cfg.edge_threshold * 3)
-            # Dilate the edges to extend their space
-            kernel = np.ones(self._cfg.extend_kernel_size, np.uint8)
-            dilated_edges = cv2.dilate(edges, kernel, iterations=1)
-            # Erode the edges to refine their shape
-            eroded_edges = cv2.erode(dilated_edges, kernel, iterations=1)
-            # modify depth image
-            depth_img[eroded_edges == 255] = 0.0
-            # save depth image
-            depth_img = (depth_img * self._cfg.depth_scale).astype("uint16")
-            if depth_filename.endswith(".png"):
-                assert cv2.imwrite(
-                    os.path.join(depth_noise_edge_dir, os.path.split(depth_filename)[1]),
-                    depth_img,
-                )
-            else:
-                np.save(
-                    os.path.join(depth_noise_edge_dir, os.path.split(depth_filename)[1]),
-                    depth_img,
-                )
-            new_depth_filename_list.append(os.path.join(depth_noise_edge_dir, os.path.split(depth_filename)[1]))
-
-        self.depth_filename_list = new_depth_filename_list
-        print("Done!")
-        return
-
-    """ Cleanup Script for files generated by this class"""
-
-    def cleanup(self):
-        print(
-            ("[INFO] Cleaning up for environment" f" {os.path.split(self.root)[1]} ..."),
-            end=" ",
-        )
-        # remove semantic_warp directory
-        if os.path.isdir(os.path.join(self.root, "img_warp")):
-            shutil.rmtree(os.path.join(self.root, "img_warp"))
-        # remove depth_noise_edges directory
-        if os.path.isdir(os.path.join(self.root, "depth_noise_edges")):
-            shutil.rmtree(os.path.join(self.root, "depth_noise_edges"))
-        print("Done!")
-        return
-
-
-# EoF
diff --git a/viplanner/utils/eval_utils.py b/viplanner/utils/eval_utils.py
deleted file mode 100644
index 83f3a19..0000000
--- a/viplanner/utils/eval_utils.py
+++ /dev/null
@@ -1,470 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# python
-import os
-from typing import List, Optional, Union
-
-import matplotlib.pyplot as plt
-import numpy as np
-import torch
-import yaml
-
-# viplanner
-from viplanner.config.learning_cfg import Loader as TrainCfgLoader
-from viplanner.traj_cost_opt import TrajCost
-
-
-class BaseEvaluator:
-    def __init__(
-        self,
-        distance_tolerance: float,
-        obs_loss_threshold: float,
-        cost_map_dir: Optional[str] = None,
-        cost_map_name: Optional[str] = None,
-    ) -> None:
-        # args
-        self.distance_tolerance = distance_tolerance
-        self.obs_loss_threshold = obs_loss_threshold
-        self.cost_map_dir = cost_map_dir
-        self.cost_map_name = cost_map_name
-
-        # parameters
-        self._nbr_paths: int = 0
-
-        # load cost_map
-        self._use_cost_map: bool = False
-        if all([self.cost_map_dir, self.cost_map_name]):
-            self._load_cost_map()
-        return
-
-    ##
-    # Properties
-    ##
-
-    @property
-    def nbr_paths(self) -> int:
-        return self._nbr_paths
-
-    def set_nbr_paths(self, nbr_paths: int) -> None:
-        self._nbr_paths = nbr_paths
-        return
-
-    ##
-    # Buffer
-    ##
-
-    def create_buffers(self) -> None:
-        self.length_goal: np.ndarray = np.zeros(self._nbr_paths)
-        self.length_path: np.ndarray = np.zeros(self._nbr_paths)
-        self.path_extension: np.ndarray = np.zeros(self._nbr_paths)
-        self.goal_distances: np.ndarray = np.zeros(self._nbr_paths)
-        if self._use_cost_map:
-            self.loss_obstacles: np.ndarray = np.zeros(self._nbr_paths)
-
-    ##
-    # Reset
-    ##
-
-    def reset(self) -> None:
-        self.create_buffers()
-        self.eval_stats = {}
-        return
-
-    ##
-    # Cost Map
-    ##
-
-    def _load_cost_map(self) -> None:
-        self._traj_cost: TrajCost = TrajCost(gpu_id=None)  # use cpu for evaluation
-        self._traj_cost.SetMap(self.cost_map_dir, self.cost_map_name)
-        self._use_cost_map = True
-        return
-
-    def _get_cost_map_loss(self, path: Union[torch.Tensor, np.ndarray]) -> float:
-        if isinstance(path, np.ndarray):
-            waypoints = torch.tensor(path, dtype=torch.float32)
-        else:
-            waypoints = path.to(dtype=torch.float32)
-
-        loss = self._traj_cost.cost_of_recorded_path(waypoints).numpy()
-        if self._traj_cost.cost_map.cfg.semantics:
-            loss -= self._traj_cost.cost_map.cfg.sem_cost_map.negative_reward
-        return loss
-
-    ##
-    # Eval Statistics
-    ##
-
-    def eval_statistics(self) -> None:
-        # Evaluate results
-        goal_reached = self.goal_distances < self.distance_tolerance
-        goal_reached_rate = sum(goal_reached) / len(goal_reached)
-        avg_distance_to_goal = sum(self.goal_distances) / len(self.goal_distances)
-        avg_distance_to_goal_reached = sum(self.goal_distances[goal_reached]) / sum(goal_reached)
-
-        print(
-            "All path segments been passed. Results: \nReached goal rate"
-            f" (thres: {self.distance_tolerance}):\t{goal_reached_rate} \nAvg"
-            f" goal-distance (all):    \t{avg_distance_to_goal} \nAvg"
-            f" goal-distance (reached):\t{avg_distance_to_goal_reached}"
-        )
-
-        self.eval_stats = {
-            "goal_reached_rate": goal_reached_rate,
-            "avg_distance_to_goal_all": avg_distance_to_goal,
-            "avg_distance_to_goal_reached": avg_distance_to_goal_reached,
-        }
-
-        if self._use_cost_map:
-            within_obs_threshold = np.sum(self.loss_obstacles < self.obs_loss_threshold) / len(self.loss_obstacles)
-            avg_obs_loss = sum(self.loss_obstacles) / len(self.loss_obstacles)
-            avg_obs_loss_reached = sum(self.loss_obstacles[goal_reached]) / sum(goal_reached)
-            max_obs_loss = max(self.loss_obstacles)
-            max_obs_loss_reached = max(self.loss_obstacles[goal_reached]) if sum(goal_reached) > 0 else np.inf
-
-            print(
-                "Within obs threshold"
-                f" ({self.obs_loss_threshold}):\t{within_obs_threshold} \nObstacle"
-                f" loss (all):        \t{avg_obs_loss} \nObstacle loss"
-                f" (reached):    \t{avg_obs_loss_reached} \nMax obstacle loss"
-                f" (all):    \t{max_obs_loss} \nMax obstacle loss"
-                f" (reached):\t{max_obs_loss_reached}"
-            )
-
-            self.eval_stats["avg_obs_loss_all"] = avg_obs_loss
-            self.eval_stats["avg_obs_loss_reached"] = avg_obs_loss_reached
-            self.eval_stats["max_obs_loss_all"] = max_obs_loss
-            self.eval_stats["max_obs_loss_reached"] = max_obs_loss_reached
-        return
-
-    def save_eval_results(self, model_dir: str, save_name: str) -> None:
-        # save eval results in model yaml
-        yaml_path = model_dir[:-3] + ".yaml"
-        if not os.path.exists(yaml_path):
-            return
-
-        with open(yaml_path) as file:
-            data: dict = yaml.load(file, Loader=TrainCfgLoader)
-        if "eval" not in data:
-            data["eval"] = {}
-
-        data["eval"][save_name] = self.eval_stats
-        with open(yaml_path, "w") as file:
-            yaml.dump(data, file)
-
-    ##
-    # Plotting
-    ##
-
-    def plt_single_model(self, eval_dir: str, show: bool = True) -> None:
-        # check if directory exists
-        os.makedirs(eval_dir, exist_ok=True)
-
-        # get unique goal lengths and init buffers
-        unique_goal_length = np.unique(np.round(self.length_goal, 1))
-        mean_path_extension = []
-        std_path_extension = []
-        mean_goal_distance = []
-        std_goal_distance = []
-        goal_counts = []
-        mean_obs_loss = []
-        std_obs_loss = []
-
-        for x in unique_goal_length:
-            # get subset of path predictions with goal length x
-            subset_idx = np.round(self.length_goal, 1) == x
-
-            mean_path_extension.append(np.mean(self.path_extension[subset_idx]))
-            std_path_extension.append(np.std(self.path_extension[subset_idx]))
-
-            mean_goal_distance.append(np.mean(self.goal_distances[subset_idx]))
-            std_goal_distance.append(np.std(self.goal_distances[subset_idx]))
-            goal_counts.append(len(self.goal_distances[subset_idx]))
-
-            if self._use_cost_map:
-                mean_obs_loss.append(np.mean(self.loss_obstacles[subset_idx]))
-                std_obs_loss.append(np.std(self.loss_obstacles[subset_idx]))
-
-        # plot with the distance to the goal depending on the length between goal and start
-        fig, ax = plt.subplots(figsize=(12, 10))
-        fig.suptitle("Path Length Increase", fontsize=20)
-        ax.plot(
-            unique_goal_length,
-            mean_path_extension,
-            color="blue",
-            label="Average path length",
-        )
-        ax.fill_between(
-            unique_goal_length,
-            np.array(mean_path_extension) - np.array(std_path_extension),
-            np.array(mean_path_extension) + np.array(std_path_extension),
-            color="blue",
-            alpha=0.2,
-            label="Uncertainty",
-        )
-        ax.set_xlabel("Start-Goal Distance", fontsize=16)
-        ax.set_ylabel("Path Length", fontsize=16)
-        ax.set_title(
-            (
-                "Avg increase of path length is"
-                f" {round(np.mean(self.path_extension), 5)*100:.2f}% for"
-                " successful paths with tolerance of"
-                f" {self.distance_tolerance}"
-            ),
-            fontsize=16,
-        )
-        ax.tick_params(axis="both", which="major", labelsize=14)
-        ax.legend()
-        fig.savefig(os.path.join(eval_dir, "path_length.png"))
-        if show:
-            plt.show()
-        else:
-            plt.close()
-
-        # plot to compare the increase in path length depending on the distance between goal and start
-        goal_success_mean = np.sum(self.goal_distances < self.distance_tolerance) / len(self.goal_distances)
-
-        # Create a figure and two axis objects, with the second one sharing the x-axis of the first
-        fig, ax1 = plt.subplots(figsize=(12, 10))
-        ax2 = ax1.twinx()
-        fig.subplots_adjust(hspace=0.4)  # Add some vertical spacing between the two plots
-
-        # Plot the goal distance data
-        ax1.plot(
-            unique_goal_length,
-            mean_goal_distance,
-            color="blue",
-            label="Average goal distance length",
-            zorder=2,
-        )
-        ax1.fill_between(
-            unique_goal_length,
-            np.array(mean_goal_distance) - np.array(std_goal_distance),
-            np.array(mean_goal_distance) + np.array(std_goal_distance),
-            color="blue",
-            alpha=0.2,
-            label="Uncertainty",
-            zorder=1,
-        )
-        ax1.set_xlabel("Start-Goal Distance", fontsize=16)
-        ax1.set_ylabel("Goal Distance", fontsize=16)
-        ax1.set_title(
-            (
-                f"With a tolerance of {self.distance_tolerance} are"
-                f" {round(goal_success_mean, 5)*100:.2f} % of goals reached"
-            ),
-            fontsize=16,
-        )
-        ax1.tick_params(axis="both", which="major", labelsize=14)
-
-        # Plot the goal counts data on the second axis
-        ax2.bar(
-            unique_goal_length,
-            goal_counts,
-            color="red",
-            alpha=0.5,
-            width=0.05,
-            label="Number of samples",
-            zorder=0,
-        )
-        ax2.set_ylabel("Sample count", fontsize=16)
-        ax2.tick_params(axis="both", which="major", labelsize=14)
-
-        # Combine the legends from both axes
-        lines, labels = ax1.get_legend_handles_labels()
-        bars, bar_labels = ax2.get_legend_handles_labels()
-        ax2.legend(lines + bars, labels + bar_labels, loc="upper center")
-
-        plt.suptitle("Goal Distance", fontsize=20)
-        fig.savefig(os.path.join(eval_dir, "goal_distance.png"))
-        if show:
-            plt.show()
-        else:
-            plt.close()
-
-        if self._use_cost_map:
-            # plot to compare the obs loss depending on the distance between goal and start
-            avg_obs_loss = np.mean(self.loss_obstacles)
-            obs_threshold_success_rate = np.sum(self.loss_obstacles < self.obs_loss_threshold) / len(
-                self.loss_obstacles
-            )
-
-            fig, ax = plt.subplots(figsize=(12, 10))
-            fig.suptitle("Obstacle Loss", fontsize=20)
-            ax.plot(
-                unique_goal_length,
-                mean_obs_loss,
-                color="blue",
-                label="Average obs loss",
-            )
-            ax.fill_between(
-                unique_goal_length,
-                np.array(mean_obs_loss) - np.array(std_obs_loss),
-                np.array(mean_obs_loss) + np.array(std_obs_loss),
-                color="blue",
-                alpha=0.2,
-                label="Uncertainty",
-            )
-            ax.set_xlabel("Start-Goal Distance", fontsize=16)
-            ax.set_ylabel("Obstacle Loss", fontsize=16)
-            ax.set_title(
-                (
-                    f"Avg obstacle loss {round(avg_obs_loss, 5):.5f} with"
-                    f" {obs_threshold_success_rate}% within obs thres"
-                    f" {self.obs_loss_threshold}"
-                ),
-                fontsize=16,
-            )
-            ax.tick_params(axis="both", which="major", labelsize=14)
-            ax.legend()
-            fig.savefig(os.path.join(eval_dir, "obs_cost.png"))
-            if show:
-                plt.show()
-            else:
-                plt.close()
-
-        return
-
-    def plt_comparison(
-        self,
-        length_goal_list: List[np.ndarray],
-        goal_distance_list: List[np.ndarray],
-        path_extension_list: List[np.ndarray],
-        model_dirs: List[str],
-        save_dir: str,
-        obs_loss_list: Optional[List[np.ndarray]] = None,
-        model_names: Optional[List[str]] = None,
-    ) -> None:
-        # path increase plot
-        fig_path, axs_path = plt.subplots(figsize=(12, 10))
-        fig_path.suptitle("Path Extension", fontsize=24)
-        axs_path.set_xlabel("Start-Goal Distance [m]", fontsize=20)
-        axs_path.set_ylabel("Path Extension [%]", fontsize=20)
-        axs_path.tick_params(axis="both", which="major", labelsize=16)
-
-        # goal distance plot
-        fig_goal, axs_goal = plt.subplots(figsize=(12, 10))
-        fig_goal.suptitle("Goal Distance", fontsize=24)
-        axs_goal.set_xlabel("Start-Goal Distance [m]", fontsize=20)
-        axs_goal.set_ylabel("Goal Distance [m]", fontsize=20)
-        axs_goal.tick_params(axis="both", which="major", labelsize=16)
-
-        if self._use_cost_map:
-            assert obs_loss_list is not None, "If cost map is used, obs_loss_list must be provided"
-            # obs loss plot
-            fig_obs, axs_obs = plt.subplots(figsize=(12, 10))
-            # fig_obs.suptitle("Mean Obstacle Loss Along Path", fontsize=24)
-            axs_obs.set_xlabel("Start-Goal Distance [m]", fontsize=20)
-            axs_obs.set_ylabel("Mean Obstacle Loss", fontsize=20)
-            axs_obs.tick_params(axis="both", which="major", labelsize=16)
-
-        bar_width = 0.8 / len(length_goal_list)
-
-        for idx in range(len(length_goal_list)):
-            if model_names is None:
-                model_name = os.path.split(model_dirs[idx])[1]
-            else:
-                model_name = model_names[idx]
-
-            goal_success_bool = goal_distance_list[idx] < self.distance_tolerance
-
-            unique_goal_length = np.unique(np.round(length_goal_list[idx], 0))
-            mean_path_extension = []
-            std_path_extension = []
-            mean_goal_distance = []
-            std_goal_distance = []
-            mean_obs_loss = []
-            std_obs_loss = []
-            goal_length_obs_exists = []
-            unqiue_goal_length_used = []
-
-            for x in unique_goal_length:
-                if x == 0:
-                    continue
-
-                # get subset of path predictions with goal length x
-                subset_idx = np.round(length_goal_list[idx], 0) == x
-
-                mean_path_extension.append(np.mean(path_extension_list[idx][subset_idx]))
-                std_path_extension.append(np.std(path_extension_list[idx][subset_idx]))
-
-                mean_goal_distance.append(np.mean(goal_distance_list[idx][subset_idx]))
-                std_goal_distance.append(np.std(goal_distance_list[idx][subset_idx]))
-
-                if self._use_cost_map:
-                    y_obs_subset = obs_loss_list[idx][subset_idx]
-                    if len(y_obs_subset) != 0:
-                        mean_obs_loss.append(np.mean(y_obs_subset))
-                        std_obs_loss.append(np.std(y_obs_subset))
-                        goal_length_obs_exists.append(x)
-                    else:
-                        print(f"Warning: No obs loss for {model_name} at goal" f" distance {x}")
-
-                unqiue_goal_length_used.append(x)
-
-            unique_goal_length = np.array(unqiue_goal_length_used)
-            goal_length_obs_exists = np.array(goal_length_obs_exists)
-
-            bar_pos = bar_width / 2 + idx * bar_width - 0.4
-            # plot to compare the increase in path length depending in on the distance between goal and start for the successful paths
-            avg_increase = np.mean(path_extension_list[idx])
-            axs_path.bar(
-                unique_goal_length + bar_pos,
-                mean_path_extension,
-                width=bar_width,
-                label=(f"{model_name} (avg {round(avg_increase, 5)*100:.2f} %))"),
-                alpha=0.8,
-            )  # yerr=std_path_extension,
-            # axs_path.plot(goal_length_path_exists, mean_path_extension, label=f'{model_name} ({round(avg_increase, 5)*100:.2f} %))')
-            # axs_path.fill_between(goal_length_path_exists, np.array(mean_path_extension) - np.array(std_path_extension), np.array(mean_path_extension) + np.array(std_path_extension), alpha=0.2)
-
-            # plot with the distance to the goal depending on the length between goal and start
-            goal_success = np.sum(goal_success_bool) / len(goal_distance_list[idx])
-            axs_goal.bar(
-                unique_goal_length + bar_pos,
-                mean_goal_distance,
-                width=bar_width,
-                label=(f"{model_name} (success rate" f" {round(goal_success, 5)*100:.2f} %)"),
-                alpha=0.8,
-            )  # yerr=std_goal_distance,
-            # axs_goal.plot(unique_goal_length, mean_goal_distance, label=f'{model_name} ({round(goal_success, 5)*100:.2f} %)')
-            # axs_goal.fill_between(unique_goal_length, np.array(mean_goal_distance) - np.array(std_goal_distance), np.array(mean_goal_distance) + np.array(std_goal_distance), alpha=0.2)
-
-            if self._use_cost_map:
-                # plot with the distance to the goal depending on the length between goal and start
-                avg_obs_loss = np.mean(obs_loss_list[idx])
-                axs_obs.bar(
-                    goal_length_obs_exists + bar_pos,
-                    mean_obs_loss,
-                    width=bar_width,
-                    label=f"{model_name} (avg {round(avg_obs_loss, 5):.3f})",
-                    alpha=0.8,
-                )  # yerr=std_obs_loss,
-                # axs_obs.plot(goal_length_obs_exists, mean_obs_loss, label=f'{model_name} ({round(avg_obs_loss, 5):.5f} %)')
-                # axs_obs.fill_between(goal_length_obs_exists, np.array(mean_obs_loss) - np.array(std_obs_loss), np.array(mean_obs_loss) + np.array(std_obs_loss), alpha=0.2)
-
-        # plot threshold for successful path
-        axs_goal.axhline(
-            y=self.distance_tolerance,
-            color="red",
-            linestyle="--",
-            label="threshold",
-        )
-
-        axs_path.legend(fontsize=20)
-        axs_goal.legend(fontsize=20)
-        fig_path.savefig(os.path.join(save_dir, "path_length_comp.png"))
-        fig_goal.savefig(os.path.join(save_dir, "goal_distance_comp.png"))
-        if self._use_cost_map:
-            axs_obs.legend(fontsize=20)
-            fig_obs.savefig(os.path.join(save_dir, "obs_loss_comp.png"))
-
-        plt.show()
-        return
-
-
-# EoF
diff --git a/viplanner/utils/torchutil.py b/viplanner/utils/torchutil.py
deleted file mode 100644
index dd8694c..0000000
--- a/viplanner/utils/torchutil.py
+++ /dev/null
@@ -1,75 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import torch
-import torch.fft
-
-
-class EarlyStopScheduler(torch.optim.lr_scheduler.ReduceLROnPlateau):
-    def __init__(
-        self,
-        optimizer,
-        mode="min",
-        factor=0.1,
-        patience=10,
-        verbose=False,
-        threshold=1e-4,
-        threshold_mode="rel",
-        cooldown=0,
-        min_lr=0,
-        eps=1e-8,
-    ):
-        super().__init__(
-            optimizer=optimizer,
-            mode=mode,
-            factor=factor,
-            patience=patience,
-            threshold=threshold,
-            threshold_mode=threshold_mode,
-            cooldown=cooldown,
-            min_lr=min_lr,
-            eps=eps,
-            verbose=verbose,
-        )
-        self.no_decrease = 0
-
-    def step(self, metrics, epoch=None):
-        # convert `metrics` to float, in case it's a zero-dim Tensor
-        current = float(metrics)
-        if epoch is None:
-            epoch = self.last_epoch = self.last_epoch + 1
-        self.last_epoch = epoch
-
-        if self.is_better(current, self.best):
-            self.best = current
-            self.num_bad_epochs = 0
-        else:
-            self.num_bad_epochs += 1
-
-        if self.in_cooldown:
-            self.cooldown_counter -= 1
-            self.num_bad_epochs = 0  # ignore any bad epochs in cooldown
-
-        if self.num_bad_epochs > self.patience:
-            self.cooldown_counter = self.cooldown
-            self.num_bad_epochs = 0
-            self._reduce_lr(epoch)
-
-    def _reduce_lr(self, epoch):
-        for i, param_group in enumerate(self.optimizer.param_groups):
-            old_lr = float(param_group["lr"])
-            new_lr = max(old_lr * self.factor, self.min_lrs[i])
-            if old_lr - new_lr > self.eps:
-                param_group["lr"] = new_lr
-                if self.verbose:
-                    print("Epoch {:5d}: reducing learning rate" " of group {} to {:.4e}.".format(epoch, i, new_lr))
-                return False
-            else:
-                return True
-
-
-def count_parameters(model):
-    return sum(p.numel() for p in model.parameters() if p.requires_grad)
diff --git a/viplanner/utils/trainer.py b/viplanner/utils/trainer.py
deleted file mode 100644
index 44c752f..0000000
--- a/viplanner/utils/trainer.py
+++ /dev/null
@@ -1,620 +0,0 @@
-# Copyright (c) 2023-2024, ETH Zurich (Robotics Systems Lab)
-# Author: Pascal Roth
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-import contextlib
-
-# python
-import os
-from typing import List, Optional, Tuple
-
-import matplotlib.pyplot as plt
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import torch.utils.data as Data
-import torchvision.transforms as transforms
-import tqdm
-import wandb  # logging
-import yaml
-
-# imperative-planning-learning
-from viplanner.config import TrainCfg
-from viplanner.plannernet import (
-    PRE_TRAIN_POSSIBLE,
-    AutoEncoder,
-    DualAutoEncoder,
-    get_m2f_cfg,
-)
-from viplanner.traj_cost_opt import TrajCost, TrajViz
-from viplanner.utils.torchutil import EarlyStopScheduler, count_parameters
-
-from .dataset import PlannerData, PlannerDataGenerator
-
-torch.set_default_dtype(torch.float32)
-
-
-class Trainer:
-    """
-    VIPlanner Trainer
-    """
-
-    def __init__(self, cfg: TrainCfg) -> None:
-        self._cfg = cfg
-
-        # set model save/load path
-        os.makedirs(self._cfg.curr_model_dir, exist_ok=True)
-        self.model_path = os.path.join(self._cfg.curr_model_dir, "model.pt")
-        if self._cfg.hierarchical:
-            self.model_dir_hierarch = os.path.join(self._cfg.curr_model_dir, "hierarchical")
-            os.makedirs(self.model_dir_hierarch, exist_ok=True)
-            self.hierach_losses = {}
-
-        # image transforms
-        self.transform = transforms.Compose(
-            [
-                transforms.ToTensor(),
-                transforms.Resize((self._cfg.img_input_size), antialias=True),
-            ]
-        )
-
-        # init buffers DATA
-        self.data_generators: List[PlannerDataGenerator] = []
-        self.data_traj_cost: List[TrajCost] = []
-        self.data_traj_viz: List[TrajViz] = []
-        self.fov_ratio: float = None
-        self.front_ratio: float = None
-        self.back_ratio: float = None
-        self.pixel_mean: np.ndarray = None
-        self.pixel_std: np.ndarray = None
-
-        # inti buffers MODEL
-        self.best_loss = float("inf")
-        self.test_loss = float("inf")
-        self.net: nn.Module = None
-        self.optimizer: optim.Optimizer = None
-        self.scheduler: EarlyStopScheduler = None
-
-        print("[INFO] Trainer initialized")
-        return
-
-    """PUBLIC METHODS"""
-
-    def train(self) -> None:
-        print("[INFO] Start Training")
-        # init logging
-        self._init_logging()
-        # load model and prepare model for training
-        self._load_model(self._cfg.resume)
-        self._configure_optimizer()
-
-        # get dataloader for training
-        self._load_data(train=True)
-        if self._cfg.hierarchical:
-            step_counter = 0
-            train_loader_list, val_loader_list = self._get_dataloader(step=step_counter)
-        else:
-            train_loader_list, val_loader_list = self._get_dataloader()
-
-        try:
-            wandb.watch(self.net)
-        except:  # noqa: E722
-            print("[WARNING] Wandb model watch failed")
-
-        for epoch in range(self._cfg.epochs):
-            train_loss = 0
-            val_loss = 0
-            for i in range(len(train_loader_list)):
-                train_loss += self._train_epoch(train_loader_list[i], epoch, env_id=i)
-                val_loss += self._test_epoch(val_loader_list[i], env_id=i, epoch=epoch)
-
-            train_loss /= len(train_loader_list)
-            val_loss /= len(train_loader_list)
-
-            try:
-                wandb.log(
-                    {
-                        "train_loss": train_loss,
-                        "val_loss": val_loss,
-                        "epoch": epoch,
-                    }
-                )
-            except:  # noqa: E722
-                print("[WARNING] Wandb logging failed")
-
-            # if val_loss < best_loss:
-            if val_loss < self.best_loss:
-                print("[INFO] Save model of epoch %d" % (epoch))
-                torch.save((self.net.state_dict(), val_loss), self.model_path)
-                self.best_loss = val_loss
-                print("[INFO] Current val loss: %.4f" % (self.best_loss))
-
-            if self.scheduler.step(val_loss):
-                print("[INFO] Early Stopping!")
-                break
-
-            if self._cfg.hierarchical and (epoch + 1) % self._cfg.hierarchical_step == 0:
-                torch.save(
-                    (self.net.state_dict(), self.best_loss),
-                    os.path.join(
-                        self.model_dir_hierarch,
-                        (
-                            f"model_ep{epoch}_fov{round(self.fov_ratio, 3)}_"
-                            f"front{round(self.front_ratio, 3)}_"
-                            f"back{round(self.back_ratio, 3)}.pt"
-                        ),
-                    ),
-                )
-                step_counter += 1
-                train_loader_list, val_loader_list = self._get_dataloader(step=step_counter)
-                self.hierach_losses[epoch] = self.best_loss
-
-        torch.cuda.empty_cache()
-
-        # cleanup data
-        for generator in self.data_generators:
-            generator.cleanup()
-
-        # empty buffers
-        self.data_generators = []
-        self.data_traj_cost = []
-        self.data_traj_viz = []
-        return
-
-    def test(self, step: Optional[int] = None) -> None:
-        print("[INFO] Start Training")
-        # set random seed for reproducibility
-        torch.manual_seed(self._cfg.seed)
-
-        # define step
-        if step is None and self._cfg.hierarchical:
-            step = int(self._cfg.epochs / self._cfg.hierarchical_step)
-
-        # load model
-        self._load_model(resume=True)
-        # get dataloader for training
-        self._load_data(train=False)
-        _, test_loader = self._get_dataloader(train=False, step=step)
-
-        self.test_loss = self._test_epoch(
-            test_loader[0],
-            env_id=0,
-            is_visual=not os.getenv("EXPERIMENT_DIRECTORY"),
-            fov_angle=self.data_generators[0].alpha_fov,
-            dataset="test",
-        )
-
-        # cleanup data
-        for generator in self.data_generators:
-            generator.cleanup()
-
-    def save_config(self) -> None:
-        print(f"[INFO] val_loss: {self.best_loss:.2f}, test_loss," f"{self.test_loss:.4f}")
-        """ Save config and loss to file"""
-        path, _ = os.path.splitext(self.model_path)
-        yaml_path = path + ".yaml"
-        print(f"[INFO] Save config and loss to {yaml_path} file")
-
-        loss_dict = {"val_loss": self.best_loss, "test_loss": self.test_loss}
-        save_dict = {"config": vars(self._cfg), "loss": loss_dict}
-
-        # dump yaml
-        with open(yaml_path, "w+") as file:
-            yaml.dump(save_dict, file, allow_unicode=True, default_flow_style=False)
-
-        # logging
-        with contextlib.suppress(Exception):
-            wandb.finish()
-
-        # plot hierarchical losses
-        if self._cfg.hierarchical:
-            plt.figure(figsize=(10, 10))
-            plt.plot(
-                list(self.hierach_losses.keys()),
-                list(self.hierach_losses.values()),
-            )
-            plt.xlabel("Epoch")
-            plt.ylabel("Validation Loss")
-            plt.title("Hierarchical Losses")
-            plt.savefig(os.path.join(self.model_dir_hierarch, "hierarchical_losses.png"))
-            plt.close()
-
-        return
-
-    """PRIVATE METHODS"""
-
-    # Helper function DATA
-    def _load_data(self, train: bool = True) -> None:
-        if not isinstance(self._cfg.data_cfg, list):
-            self._cfg.data_cfg = [self._cfg.data_cfg] * len(self._cfg.env_list)
-        assert len(self._cfg.data_cfg) == len(self._cfg.env_list), (
-            "Either single DataCfg or number matching number of environments" "must be provided"
-        )
-
-        for idx, env_name in enumerate(self._cfg.env_list):
-            if (train and idx == self._cfg.test_env_id) or (not train and idx != self._cfg.test_env_id):
-                continue
-
-            data_path = os.path.join(self._cfg.data_dir, env_name)
-
-            # get trajectory cost map
-            traj_cost = TrajCost(
-                self._cfg.gpu_id,
-                log_data=train,
-                w_obs=self._cfg.w_obs,
-                w_height=self._cfg.w_height,
-                w_goal=self._cfg.w_goal,
-                w_motion=self._cfg.w_motion,
-                obstalce_thread=self._cfg.obstacle_thread,
-            )
-            traj_cost.SetMap(
-                data_path,
-                self._cfg.cost_map_name,
-            )
-
-            generator = PlannerDataGenerator(
-                cfg=self._cfg.data_cfg[idx],
-                root=data_path,
-                semantics=self._cfg.sem,
-                rgb=self._cfg.rgb,
-                cost_map=traj_cost.cost_map,  # trajectory cost class
-            )
-
-            traj_viz = TrajViz(
-                intrinsics=generator.K_depth,
-                cam_resolution=self._cfg.img_input_size,
-                camera_tilt=self._cfg.camera_tilt,
-                cost_map=traj_cost.cost_map,
-            )
-
-            self.data_generators.append(generator)
-            self.data_traj_cost.append(traj_cost)
-            self.data_traj_viz.append(traj_viz)
-            print(f"LOADED DATA FOR ENVIRONMENT: {env_name}")
-
-        print("[INFO] LOADED ALL DATA")
-        return
-
-    # Helper function TRAINING
-    def _init_logging(self) -> None:
-        # logging
-        os.environ["WANDB_API_KEY"] = self._cfg.wb_api_key
-        os.environ["WANDB_MODE"] = "online"
-        os.makedirs(self._cfg.log_dir, exist_ok=True)
-
-        try:
-            wandb.init(
-                project=self._cfg.wb_project,
-                entity=self._cfg.wb_entity,
-                name=self._cfg.get_model_save(),
-                config=self._cfg.__dict__,
-                dir=self._cfg.log_dir,
-            )
-        except:  # noqa: E722
-            print("[WARNING: Wandb not available")
-        return
-
-    def _load_model(self, resume: bool = False) -> None:
-        if self._cfg.sem or self._cfg.rgb:
-            if self._cfg.rgb and self._cfg.pre_train_sem:
-                assert PRE_TRAIN_POSSIBLE, (
-                    "Pretrained model not available since either detectron2"
-                    " not installed or mask2former not found in thrid_party"
-                    " folder"
-                )
-                pre_train_cfg = os.path.join(self._cfg.all_model_dir, self._cfg.pre_train_cfg)
-                pre_train_weights = (
-                    os.path.join(self._cfg.all_model_dir, self._cfg.pre_train_weights)
-                    if self._cfg.pre_train_weights
-                    else None
-                )
-                m2f_cfg = get_m2f_cfg(pre_train_cfg)
-                self.pixel_mean = m2f_cfg.MODEL.PIXEL_MEAN
-                self.pixel_std = m2f_cfg.MODEL.PIXEL_STD
-            else:
-                m2f_cfg = None
-                pre_train_weights = None
-
-            self.net = DualAutoEncoder(self._cfg, m2f_cfg=m2f_cfg, weight_path=pre_train_weights)
-        else:
-            self.net = AutoEncoder(self._cfg.in_channel, self._cfg.knodes)
-
-        assert torch.cuda.is_available(), "Code requires GPU"
-        print(f"Available GPU list: {list(range(torch.cuda.device_count()))}")
-        print(f"Running on GPU: {self._cfg.gpu_id}")
-        self.net = self.net.cuda(self._cfg.gpu_id)
-        print(f"[INFO] MODEL LOADED ({count_parameters(self.net)} parameters)")
-
-        if resume:
-            model_state_dict, self.best_loss = torch.load(self.model_path)
-            self.net.load_state_dict(model_state_dict)
-            print(f"Resume train from {self.model_path} with loss " f"{self.best_loss}")
-
-        return
-
-    def _configure_optimizer(self) -> None:
-        if self._cfg.optimizer == "adam":
-            self.optimizer = optim.Adam(
-                self.net.parameters(),
-                lr=self._cfg.lr,
-                weight_decay=self._cfg.w_decay,
-            )
-        elif self._cfg.optimizer == "sgd":
-            self.optimizer = optim.SGD(
-                self.net.parameters(),
-                lr=self._cfg.lr,
-                momentum=self._cfg.momentum,
-                weight_decay=self._cfg.w_decay,
-            )
-        else:
-            raise KeyError(f"Optimizer {self._cfg.optimizer} not supported")
-        self.scheduler = EarlyStopScheduler(
-            self.optimizer,
-            factor=self._cfg.factor,
-            verbose=True,
-            min_lr=self._cfg.min_lr,
-            patience=self._cfg.patience,
-        )
-        print("[INFO] OPTIMIZER AND SCHEDULER CONFIGURED")
-        return
-
-    def _get_dataloader(
-        self,
-        train: bool = True,
-        step: Optional[int] = None,
-        allow_augmentation: bool = True,
-    ) -> None:
-        train_loader_list: List[Data.DataLoader] = []
-        val_loader_list: List[Data.DataLoader] = []
-
-        if step is not None:
-            self.fov_ratio = (
-                1.0 - (self._cfg.hierarchical_front_step_ratio + self._cfg.hierarchical_back_step_ratio) * step
-            )
-            self.front_ratio = self._cfg.hierarchical_front_step_ratio * step
-            self.back_ratio = self._cfg.hierarchical_back_step_ratio * step
-
-        for generator in self.data_generators:
-            # init data classes
-
-            val_data = PlannerData(
-                cfg=generator._cfg,
-                transform=self.transform,
-                semantics=self._cfg.sem,
-                rgb=self._cfg.rgb,
-                pixel_mean=self.pixel_mean,
-                pixel_std=self.pixel_std,
-            )
-
-            if train:
-                train_data = PlannerData(
-                    cfg=generator._cfg,
-                    transform=self.transform,
-                    semantics=self._cfg.sem,
-                    rgb=self._cfg.rgb,
-                    pixel_mean=self.pixel_mean,
-                    pixel_std=self.pixel_std,
-                )
-            else:
-                train_data = None
-
-            # split data in train and validation with given sample ratios
-            if train:
-                generator.split_samples(
-                    train_dataset=train_data,
-                    test_dataset=val_data,
-                    generate_split=train,
-                    ratio_back_samples=self.back_ratio,
-                    ratio_front_samples=self.front_ratio,
-                    ratio_fov_samples=self.fov_ratio,
-                    allow_augmentation=allow_augmentation,
-                )
-            else:
-                generator.split_samples(
-                    train_dataset=train_data,
-                    test_dataset=val_data,
-                    generate_split=train,
-                    ratio_back_samples=self.back_ratio,
-                    ratio_front_samples=self.front_ratio,
-                    ratio_fov_samples=self.fov_ratio,
-                    allow_augmentation=allow_augmentation,
-                )
-
-            if self._cfg.load_in_ram:
-                if train:
-                    train_data.load_data_in_memory()
-                val_data.load_data_in_memory()
-
-            if train:
-                train_loader = Data.DataLoader(
-                    dataset=train_data,
-                    batch_size=self._cfg.batch_size,
-                    shuffle=True,
-                    pin_memory=True,
-                    num_workers=self._cfg.num_workers,
-                )
-            val_loader = Data.DataLoader(
-                dataset=val_data,
-                batch_size=self._cfg.batch_size,
-                shuffle=True,
-                pin_memory=True,
-                num_workers=self._cfg.num_workers,
-            )
-
-            if train:
-                train_loader_list.append(train_loader)
-            val_loader_list.append(val_loader)
-
-        return train_loader_list, val_loader_list
-
-    def _train_epoch(
-        self,
-        loader: Data.DataLoader,
-        epoch: int,
-        env_id: int,
-    ) -> float:
-        train_loss, batches = 0, len(loader)
-        enumerater = tqdm.tqdm(enumerate(loader))
-
-        for batch_idx, inputs in enumerater:
-            odom = inputs[2].cuda(self._cfg.gpu_id)
-            goal = inputs[3].cuda(self._cfg.gpu_id)
-            self.optimizer.zero_grad()
-
-            if self._cfg.sem or self._cfg.rgb:
-                depth_image = inputs[0].cuda(self._cfg.gpu_id)
-                sem_rgb_image = inputs[1].cuda(self._cfg.gpu_id)
-                preds, fear = self.net(depth_image, sem_rgb_image, goal)
-            else:
-                image = inputs[0].cuda(self._cfg.gpu_id)
-                preds, fear = self.net(image, goal)
-
-            # flip y axis for augmented samples  (clone necessary due to
-            # inplace operation that otherwise leads to error in backprop)
-            preds_flip = torch.clone(preds)
-            preds_flip[inputs[4], :, 1] = preds_flip[inputs[4], :, 1] * -1
-            goal_flip = torch.clone(goal)
-            goal_flip[inputs[4], 1] = goal_flip[inputs[4], 1] * -1
-
-            log_step = batch_idx + epoch * batches
-            loss, _ = self._loss(
-                preds_flip,
-                fear,
-                self.data_traj_cost[env_id],
-                odom,
-                goal_flip,
-                log_step=log_step,
-            )
-            wandb.log({"train_loss_step": loss}, step=log_step)
-
-            loss.backward()
-            self.optimizer.step()
-            train_loss += loss.item()
-            enumerater.set_description(
-                f"Epoch: {epoch} in Env: "
-                f"({env_id+1}/{len(self._cfg.env_list)-1}) "
-                f"- train loss:{round(train_loss/(batch_idx+1), 4)} on"
-                f" {batch_idx}/{batches}"
-            )
-        return train_loss / (batch_idx + 1)
-
-    def _test_epoch(
-        self,
-        loader,
-        env_id: int,
-        epoch: int = 0,
-        is_visual=False,
-        fov_angle: float = 90.0,
-        dataset: str = "val",
-    ) -> float:
-        test_loss = 0
-        num_batches = len(loader)
-        preds_viz = []
-        wp_viz = []
-        image_viz = []
-
-        with torch.no_grad():
-            for batch_idx, inputs in enumerate(loader):
-                odom = inputs[2].cuda(self._cfg.gpu_id)
-                goal = inputs[3].cuda(self._cfg.gpu_id)
-
-                if self._cfg.sem or self._cfg.rgb:
-                    image = inputs[0].cuda(self._cfg.gpu_id)  # depth
-                    sem_rgb_image = inputs[1].cuda(self._cfg.gpu_id)  # sem
-                    preds, fear = self.net(image, sem_rgb_image, goal)
-                else:
-                    image = inputs[0].cuda(self._cfg.gpu_id)
-                    preds, fear = self.net(image, goal)
-
-                # flip y axis for augmented samples
-                preds[inputs[4], :, 1] = preds[inputs[4], :, 1] * -1
-                goal[inputs[4], 1] = goal[inputs[4], 1] * -1
-
-                log_step = epoch * num_batches + batch_idx
-                loss, waypoints = self._loss(
-                    preds,
-                    fear,
-                    self.data_traj_cost[env_id],
-                    odom,
-                    goal,
-                    log_step=log_step,
-                    dataset=dataset,
-                )
-
-                if dataset == "val":
-                    wandb.log({f"{dataset}_loss_step": loss}, step=log_step)
-
-                test_loss += loss.item()
-
-                if is_visual and len(preds_viz) * batch_idx < self._cfg.n_visualize:
-                    if batch_idx == 0:
-                        odom_viz = odom.cpu()
-                        goal_viz = goal.cpu()
-                        fear_viz = fear.cpu()
-                        augment_viz = inputs[4].cpu()
-                    else:
-                        odom_viz = torch.cat((odom_viz, odom.cpu()), dim=0)
-                        goal_viz = torch.cat((goal_viz, goal.cpu()), dim=0)
-                        fear_viz = torch.cat((fear_viz, fear.cpu()), dim=0)
-                        augment_viz = torch.cat((augment_viz, inputs[4].cpu()), dim=0)
-                    preds_viz.append(preds.cpu())
-                    wp_viz.append(waypoints.cpu())
-                    image_viz.append(image.cpu())
-
-            if is_visual:
-                preds_viz = torch.vstack(preds_viz)
-                wp_viz = torch.vstack(wp_viz)
-                image_viz = torch.vstack(image_viz)
-
-                # limit again to number of visualizations since before
-                # added as multiple of batch size
-                preds_viz = preds_viz[: self._cfg.n_visualize]
-                wp_viz = wp_viz[: self._cfg.n_visualize]
-                image_viz = image_viz[: self._cfg.n_visualize]
-                odom_viz = odom_viz[: self._cfg.n_visualize]
-                goal_viz = goal_viz[: self._cfg.n_visualize]
-                fear_viz = fear_viz[: self._cfg.n_visualize]
-                augment_viz = augment_viz[: self._cfg.n_visualize]
-
-                # visual trajectory and images
-                self.data_traj_viz[env_id].VizTrajectory(
-                    preds_viz,
-                    wp_viz,
-                    odom_viz,
-                    goal_viz,
-                    fear_viz,
-                    fov_angle=fov_angle,
-                    augment_viz=augment_viz,
-                )
-                self.data_traj_viz[env_id].VizImages(preds_viz, wp_viz, odom_viz, goal_viz, fear_viz, image_viz)
-        return test_loss / (batch_idx + 1)
-
-    def _loss(
-        self,
-        preds: torch.Tensor,
-        fear: torch.Tensor,
-        traj_cost: TrajCost,
-        odom: torch.Tensor,
-        goal: torch.Tensor,
-        log_step: int,
-        step: float = 0.1,
-        dataset: str = "train",
-    ) -> Tuple[torch.Tensor, torch.Tensor]:
-        waypoints = traj_cost.opt.TrajGeneratorFromPFreeRot(preds, step=step)
-        loss = traj_cost.CostofTraj(
-            waypoints,
-            odom,
-            goal,
-            fear,
-            log_step,
-            ahead_dist=self._cfg.fear_ahead_dist,
-            dataset=dataset,
-        )
-
-        return loss, waypoints
-
-
-# EoF